{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'imp': {'datasets': 5, 'iterations': 3, 'mmc': 0, 'mmf': 'mean_match_default', 'tune': True}, 'trf': {'act_equiv': 'passthrough', 'apdc_day': 'passthrough', 'appl_day': 'passthrough', 'birth_day': 'passthrough', 'coll_code': 'passthrough', 'distance': 'passthrough', 'gap_score': 'passthrough', 'gender': 'passthrough', 'hs_qrtl': 'passthrough', 'international': 'passthrough', 'lgcy': 'passthrough', 'math': 'passthrough', 'oriented': 'passthrough', 'race_american_indian': 'passthrough', 'race_asian': 'passthrough', 'race_black': 'passthrough', 'race_hispanic': 'passthrough', 'race_pacific': 'passthrough', 'race_white': 'passthrough', 'reading': 'passthrough', 'remote': 'passthrough', 'resd': 'passthrough', 'schlship_app': 'passthrough', 'ssb': 'passthrough', 'waiver': 'passthrough', 'writing': 'passthrough'}}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>train_term</th>\n",
       "      <th>202008</th>\n",
       "      <th>202108</th>\n",
       "      <th>202208</th>\n",
       "      <th>202308</th>\n",
       "      <th>abs_mean</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>crse</th>\n",
       "      <th>styp_code</th>\n",
       "      <th>pred_term</th>\n",
       "      <th>err_pct</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">_total</th>\n",
       "      <th rowspan=\"4\" valign=\"top\">n</th>\n",
       "      <th>202008</th>\n",
       "      <th>50%</th>\n",
       "      <td>0.331675</td>\n",
       "      <td>8.015478</td>\n",
       "      <td>3.92482</td>\n",
       "      <td>-5.583195</td>\n",
       "      <td>4.463792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202108</th>\n",
       "      <th>50%</th>\n",
       "      <td>5.055487</td>\n",
       "      <td>1.109741</td>\n",
       "      <td>-0.924784</td>\n",
       "      <td>-20.345253</td>\n",
       "      <td>6.858816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202208</th>\n",
       "      <th>50%</th>\n",
       "      <td>7.135307</td>\n",
       "      <td>-3.065539</td>\n",
       "      <td>0.105708</td>\n",
       "      <td>-19.133192</td>\n",
       "      <td>7.359937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202308</th>\n",
       "      <th>50%</th>\n",
       "      <td>10.778986</td>\n",
       "      <td>5.842391</td>\n",
       "      <td>-2.038043</td>\n",
       "      <td>0.588768</td>\n",
       "      <td>4.812047</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'imp': {'datasets': 5, 'iterations': 3, 'mmc': 5, 'mmf': 'mean_match_default', 'tune': True}, 'trf': {'act_equiv': 'passthrough', 'apdc_day': 'passthrough', 'appl_day': 'passthrough', 'birth_day': 'passthrough', 'coll_code': 'passthrough', 'distance': 'passthrough', 'gap_score': 'passthrough', 'gender': 'passthrough', 'hs_qrtl': 'passthrough', 'international': 'passthrough', 'lgcy': 'passthrough', 'math': 'passthrough', 'oriented': 'passthrough', 'race_american_indian': 'passthrough', 'race_asian': 'passthrough', 'race_black': 'passthrough', 'race_hispanic': 'passthrough', 'race_pacific': 'passthrough', 'race_white': 'passthrough', 'reading': 'passthrough', 'remote': 'passthrough', 'resd': 'passthrough', 'schlship_app': 'passthrough', 'ssb': 'passthrough', 'waiver': 'passthrough', 'writing': 'passthrough'}}\n"
     ]
    }
   ],
   "source": [
    "from LiveAMP import *\n",
    "import miceforest as mf\n",
    "# from sklearn.base import clone, BaseEstimator\n",
    "from sklearn.compose import make_column_selector, ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, PowerTransformer, KBinsDiscretizer\n",
    "# from sklearn.experimental import enable_iterative_imputer\n",
    "# from sklearn.impute import IterativeImputer\n",
    "# from sklearn.linear_model import BayesianRidge, Ridge\n",
    "# from sklearn.kernel_approximation import Nystroem\n",
    "# from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, GradientBoostingClassifier, GradientBoostingRegressor, HistGradientBoostingClassifier, HistGradientBoostingRegressor\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn import set_config\n",
    "set_config(transform_output=\"pandas\")\n",
    "\n",
    "def feature_importance_df(self, dataset=0, iteration=None, normalize=True):\n",
    "    targ = [self._get_var_name_from_scalar(int(i)) for i in np.sort(self.imputation_order)]\n",
    "    feat = [self._get_var_name_from_scalar(int(i)) for i in np.sort(self.predictor_vars)]\n",
    "    I = pd.DataFrame(self.get_feature_importance(dataset, iteration), index=targ, columns=feat).T\n",
    "    return I / I.sum() * 100 if normalize else I\n",
    "mf.ImputationKernel.feature_importance_df = feature_importance_df\n",
    "\n",
    "def inspect(self, **kwargs):\n",
    "    self.plot_imputed_distributions(wspace=0.3,hspace=0.3)\n",
    "    plt.show()\n",
    "    self.plot_mean_convergence(wspace=0.3, hspace=0.4)\n",
    "    plt.show()\n",
    "    I = self.feature_importance_df(**kwargs)\n",
    "    I.disp(100)\n",
    "    return I\n",
    "mf.ImputationKernel.inspect = inspect\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class AMP(MyBaseClass):\n",
    "    cycle_day: int\n",
    "    term_codes: typing.List\n",
    "    infer_term: int\n",
    "    crse: typing.List\n",
    "    attr: typing.List\n",
    "    fill: typing.Dict = None\n",
    "    trf_grid: typing.Dict = None\n",
    "    imp_grid: typing.Dict = None\n",
    "    overwrite: typing.Dict = None\n",
    "    show: typing.Dict = None\n",
    "    inspect: bool = False\n",
    "\n",
    "    def dump(self):\n",
    "        return write(self.rslt, self, overwrite=True)\n",
    "\n",
    "    def __post_init__(self):\n",
    "        D = {'adm':False, 'reg':False, 'flg':False, 'raw':False, 'term':False, 'raw_df':False, 'reg_df':False, 'X':False, 'Y':False, 'pred':False}\n",
    "        for x in ['overwrite','show']:\n",
    "            self[x] = D.copy() if self[x] is None else D.copy() | self[x]\n",
    "        self.overwrite['raw'] |= self.overwrite['reg'] | self.overwrite['adm'] | self.overwrite['flg']\n",
    "        self.overwrite['term'] |= self.overwrite['raw']\n",
    "        self.overwrite['raw_df'] |= self.overwrite['term']\n",
    "        self.overwrite['reg_df'] |= self.overwrite['term']\n",
    "        self.overwrite['X'] |= self.overwrite['raw_df']\n",
    "        self.overwrite['Y'] |= self.overwrite['reg_df'] | self.overwrite['X']\n",
    "        self.overwrite['pred'] |= self.overwrite['Y']\n",
    "        self.path = root_path / f\"resources/rslt/{rjust(self.cycle_day,3,0)}\"\n",
    "        self.rslt = self.path / f\"rslt.pkl\"\n",
    "        self.tune = self.path / f\"tune.pkl\"\n",
    "        try:\n",
    "            self.__dict__ = read(self.rslt).__dict__ | self.__dict__\n",
    "        except:\n",
    "            pass\n",
    "        for k, v in self.overwrite.items():\n",
    "            if v and k in self:\n",
    "                del self[k]\n",
    "\n",
    "        for k in ['fill','term','pred','trf_grid','imp_grid']:\n",
    "            self[k] = self[k] if k in self else dict()\n",
    "\n",
    "        self.term_codes = uniquify([*listify(self.term_codes), self.infer_term])\n",
    "        self.crse = uniquify(['_total', *listify(self.crse)])\n",
    "        self.mlt_grp = ['crse','levl_code','styp_code','term_code']\n",
    "        self.trf_list = cartesian({k: sorted(setify(v), key=str) for k,v in self.trf_grid.items()})\n",
    "        self.trf_list = [mysort({k:v for k,v in t.items() if v not in ['drop',None,'']}) for t in self.trf_list]\n",
    "        imp_default = {'iterations':3, 'mmc':0, 'mmf':'mean_match_default', 'datasets':5, 'tune':True}\n",
    "        self.imp_list = cartesian(self.imp_grid)\n",
    "        self.imp_list = [mysort(imp_default | v) for v in self.imp_list]\n",
    "        self.params_list = [mysort({'imp':imp, 'trf':trf}) for trf, imp in it.product(self.trf_list,self.imp_list)]\n",
    "\n",
    "        opts = {x:self[x] for x in ['cycle_day','overwrite','show']}\n",
    "        for nm in self.term_codes:\n",
    "            if nm not in self.term:\n",
    "                print(f'get {nm}')\n",
    "                self.term[nm] = TERM(term_code=nm, **opts).get_raw()\n",
    "        return self.dump()\n",
    "\n",
    "\n",
    "    def preprocess(self):\n",
    "        def get(nm):\n",
    "            if nm in self:\n",
    "                return False\n",
    "            print(f'get {nm}')\n",
    "            return True\n",
    "\n",
    "        if get('raw_df'):\n",
    "            self.raw_df = pd.concat([term.raw for term in self.term.values()], ignore_index=True).dropna(axis=1, how='all').prep()\n",
    "\n",
    "        if get('reg_df'):\n",
    "            with warnings.catch_warnings(action='ignore'):\n",
    "                self.reg_df = {k: pd.concat([term.reg[k].query(\"crse in @self.crse\") for term in self.term.values()]) for k in ['cur','end']}\n",
    "\n",
    "        where = lambda x: x.query(\"levl_code == 'ug' and styp_code in ('n','r','t')\").copy()\n",
    "        if get('X'):\n",
    "            R = self.raw_df.copy()\n",
    "            repl = {'ae':0, 'n1':1, 'n2':2, 'n3':3, 'n4':4, 'r1':1, 'r2':2, 'r3':3, 'r4':4}\n",
    "            R['hs_qrtl'] = pd.cut(R['hs_pctl'], bins=[-1,25,50,75,90,101], labels=[4,3,2,1,0], right=False).combine_first(R['apdc_code'].map(repl))\n",
    "            R['remote'] = R['camp_code'] != 's'\n",
    "            R['resd'] = R['resd_code'] == 'r'\n",
    "            R['lgcy'] = ~R['lgcy_code'].isin(['n','o'])\n",
    "            R['majr_code'] = R['majr_code'].replace({'0000':'und', 'eled':'eted', 'agri':'unda'})\n",
    "            R['coll_code'] = R['coll_code'].replace({'ae':'an', 'eh':'ed', 'hs':'hl', 'st':'sm', '00':pd.NA})\n",
    "            R['coll_desc'] = R['coll_desc'].replace({\n",
    "                'ag & environmental sciences':'ag & natural resources',\n",
    "                'education & human development':'education',\n",
    "                'health science & human service':'health sciences',\n",
    "                'science & technology':'science & mathematics'})\n",
    "            majr = ['majr_desc','dept_code','dept_desc','coll_code','coll_desc']\n",
    "            S = R.sort_values('cycle_date').drop_duplicates(subset='majr_code', keep='last')[['majr_code',*majr]]\n",
    "            X = where(R.drop(columns=majr).merge(S, on='majr_code', how='left')).prep().binarize()\n",
    "\n",
    "            checks = [\n",
    "                'cycle_day >= 0',\n",
    "                'apdc_day >= cycle_day',\n",
    "                'appl_day >= apdc_day',\n",
    "                'birth_day >= appl_day',\n",
    "                'birth_day >= 5000',\n",
    "                'distance >= 0',\n",
    "                'hs_pctl >=0',\n",
    "                'hs_pctl <= 100',\n",
    "                'hs_qrtl >= 0',\n",
    "                'hs_qrtl <= 4',\n",
    "                'act_equiv >= 1',\n",
    "                'act_equiv <= 36',\n",
    "                'gap_score >= 0',\n",
    "                'gap_score <= 100',\n",
    "            ]\n",
    "            for check in checks:\n",
    "                mask = X.eval(check)\n",
    "                assert mask.all(), [check,X[~mask].disp(5)]\n",
    "\n",
    "            for k, v in self.fill.items():\n",
    "                X[k] = X.impute(k, *listify(v))\n",
    "\n",
    "            self.X = X.prep().binarize().set_index(self.attr, drop=False).rename(columns=lambda x:'__'+x)\n",
    "            self.X.missing().disp(100)\n",
    "\n",
    "        if get('Y'):\n",
    "            self.Y = {k: self.X[[]].join(y.set_index(['pidm','term_code','crse'])['credit_hr']) for k, y in self.reg_df.items()}\n",
    "            agg = lambda y: where(y).groupby(self.mlt_grp)['credit_hr'].agg(lambda x: (x>0).sum())\n",
    "            A = agg(self.reg_df['end'])\n",
    "            B = agg(self.Y['end'])\n",
    "            M = (A / B).replace(np.inf, pd.NA).rename('mlt').reset_index().query(f\"term_code != {self.infer_term}\").prep()\n",
    "            N = M.assign(term_code=self.infer_term)\n",
    "            self.mlt = pd.concat([M, N], axis=0).set_index(self.mlt_grp)\n",
    "            Y = {k: y.squeeze().unstack().dropna(how='all', axis=1).fillna(0) for k, y in self.Y.items()}\n",
    "            self.Y = Y['cur'].rename(columns=lambda x:x+'_cur').join(Y['end']>0).prep()\n",
    "        return self.dump()\n",
    "\n",
    "\n",
    "    def analyze(self, params):\n",
    "        def pivot(df, val):\n",
    "            Y = (\n",
    "                df\n",
    "                .reset_index()\n",
    "                .pivot_table(columns='train_term', index=['crse','styp_code','pred_term'], values=val, aggfunc=['count',pctl(0),pctl(25),pctl(50),pctl(75),pctl(100)])\n",
    "                .rename_axis(columns=[val,'train_term'])\n",
    "                .stack(0, future_stack=True)\n",
    "                .assign(abs_mean = lambda x: x.abs().mean(axis=1))\n",
    "            )\n",
    "            return Y\n",
    "        v = self.pred[params]\n",
    "        df = v['summary']\n",
    "        mask = df.eval(f\"pred_term!={self.infer_term}\")\n",
    "        v['rslt'] = {stat: pivot(df[mask], stat) for stat in [\"pred\",\"err\",\"err_pct\",\"mse_pct\",\"f1_inv_pct\"]} | {'pred': pivot(df[~mask], \"pred\")}\n",
    "        v['rslt']['err_pct'].query(\"err_pct in [' 50%']\").disp(200)\n",
    "        # v['rslt']['err_pct'].query(\"err_pct in ['count',' 50%']\").disp(200)\n",
    "        return self.dump()\n",
    "\n",
    "\n",
    "    def main(self):\n",
    "        self = self.preprocess()\n",
    "        g = lambda Y: Y | {k: pd.concat([y[k] for y in Y.values() if isinstance(y, dict) and k in y.keys()]).sort_index() for k in ['details','summary']}\n",
    "        for params in self.params_list:\n",
    "            print(str(params))\n",
    "            self.pred.setdefault(str(params), dict())\n",
    "            # print(str(params))\n",
    "            for crse in self.crse:\n",
    "                self.pred[str(params)].setdefault(crse, dict())\n",
    "                for train_term in self.term_codes:\n",
    "                    self.pred[str(params)][crse].setdefault(train_term, dict())\n",
    "                    for styp_code in ['n']:#,'r','t']:\n",
    "                        self.pred[str(params)][crse][train_term][styp_code] = self.predict(copy.deepcopy(params), crse, train_term, styp_code)\n",
    "                        self.dump()\n",
    "                    self.pred[str(params)][crse][train_term] = g(self.pred[str(params)][crse][train_term])\n",
    "                self.pred[str(params)][crse] = g(self.pred[str(params)][crse])\n",
    "            self.pred[str(params)] = g(self.pred[str(params)])\n",
    "            self.analyze(str(params))\n",
    "            self.dump()\n",
    "        return self\n",
    "    \n",
    "    \n",
    "    def predict(self, params, crse, train_term, styp_code='all'):\n",
    "        for p, P in self.pred.items():\n",
    "            if p == str(params):\n",
    "                for c, C in P.items():\n",
    "                    if c == crse:\n",
    "                        for t, T in C.items():\n",
    "                            if t == train_term:\n",
    "                                for s, S in T.items():\n",
    "                                    if s == styp_code:\n",
    "                                        # print(ljust(crse,8), train_term, styp_code, 'reusing')\n",
    "                                        return S\n",
    "\n",
    "        print(ljust(crse,8), train_term, styp_code, 'creating')\n",
    "        X = self.X.copy()\n",
    "        if styp_code != 'all':\n",
    "            X = X.query(f\"styp_code==@styp_code\")\n",
    "        trf = ColumnTransformer([(c,t,[\"__\"+c]) for c,t in params['trf'].items()], remainder='drop', verbose_feature_names_out=False)\n",
    "        cols = uniquify(['_total_cur',crse+'_cur',crse])\n",
    "        Z = trf.fit_transform(X).join(self.Y[cols]).prep().categorize().sort_index()\n",
    "        y = Z[crse].copy().rename('true').to_frame()\n",
    "        Z.loc[Z.eval(\"term_code!=@train_term\"), crse] = pd.NA\n",
    "\n",
    "        iterations = params['imp'].pop('iterations')\n",
    "        datasets = params['imp'].pop('datasets')\n",
    "        tune = params['imp'].pop('tune')\n",
    "        mmc = params['imp'].pop('mmc')\n",
    "        mmf = params['imp'].pop('mmf')\n",
    "        if mmc > 0 and mmf is not None:\n",
    "            params['imp']['mean_match_scheme'] = getattr(mf, mmf).copy()\n",
    "            params['imp']['mean_match_scheme'].set_mean_match_candidates(mmc)\n",
    "        \n",
    "        if tune:\n",
    "            # print('tuning')\n",
    "            imp = mf.ImputationKernel(Z, datasets=1, **params['imp'])\n",
    "            imp.mice(iterations=1)\n",
    "            optimal_parameters, losses = imp.tune_parameters(dataset=0, optimization_steps=5)\n",
    "        else:\n",
    "            # print('not tuning')\n",
    "            optimal_parameters = None\n",
    "        imp = mf.ImputationKernel(Z, datasets=datasets, **params['imp'])\n",
    "        imp.mice(iterations=iterations, variable_parameters=optimal_parameters)\n",
    "\n",
    "        if self.inspect:\n",
    "            imp.inspect()\n",
    "        Z.loc[:, crse] = pd.NA\n",
    "        P = imp.impute_new_data(Z)\n",
    "        details = pd.concat([y\n",
    "                .assign(pred=P.complete_data(k)[crse], train_term=train_term, crse=crse, sim=k)\n",
    "                .set_index(['train_term','crse','sim'], append=True)\n",
    "            for k in range(P.dataset_count())]).binarize()\n",
    "        agg = lambda x: pd.Series({\n",
    "            'pred': x['pred'].sum(min_count=1),\n",
    "            'true': x['true'].sum(min_count=1),\n",
    "            'mse_pct': ((1*x['pred'] - x['true'])**2).mean()*100,\n",
    "            'f1_inv_pct': (1-f1_score(x.dropna()['true'], x.dropna()['pred'], zero_division=np.nan))*100,\n",
    "        })\n",
    "        summary = details.groupby([*self.mlt_grp,'train_term','sim']).apply(agg).join(self.mlt).rename_axis(index={'term_code':'pred_term'})\n",
    "        for x in ['pred','true']:\n",
    "            summary[x] = summary[x] * summary['mlt']\n",
    "        summary.insert(2, 'err', summary['pred'] - summary['true'])\n",
    "        summary.insert(3, 'err_pct', (summary['err'] / summary['true']).clip(-1, 1) * 100)\n",
    "        S = {'details':details, 'summary':summary.drop(columns='mlt').prep(), 'trf':trf, 'imp':imp}\n",
    "        # S['summary'].disp(5)\n",
    "        return S\n",
    "\n",
    "\n",
    "code_desc = lambda x: [x+'_code', x+'_desc']\n",
    "passthru = ['passthrough']\n",
    "passdrop = passthru\n",
    "# passdrop = ['passthrough', 'drop']\n",
    "bintrf = lambda n_bins: KBinsDiscretizer(n_bins=n_bins, encode='ordinal', strategy='uniform', subsample=None)\n",
    "pwrtrf = make_pipeline(StandardScaler(), PowerTransformer())\n",
    "kwargs = {\n",
    "    'term_codes': np.arange(2020,2025)*100+8,\n",
    "    'infer_term': 202408,\n",
    "    'show': {\n",
    "        # 'reg':True,\n",
    "        # 'adm':True,\n",
    "    },\n",
    "    'fill': {\n",
    "        'birth_day': ['median',['term_code','styp_code']],\n",
    "        'remote': False,\n",
    "        'international': False,\n",
    "        **{f'race_{r}': False for r in ['american_indian','asian','black','pacific','white','hispanic']},\n",
    "        'lgcy': False,\n",
    "        'resd': False,\n",
    "        'waiver': False,\n",
    "        'fafsa_app': False,\n",
    "        'schlship_app': False,\n",
    "        'finaid_accepted': False,\n",
    "        'ssb': False,\n",
    "        'math': False,\n",
    "        'reading': False,\n",
    "        'writing': False,\n",
    "        'gap_score': 0,\n",
    "        'oriented': 'n',\n",
    "    },\n",
    "    'attr': [\n",
    "        # 'index',\n",
    "        'pidm',\n",
    "        *code_desc('term'),\n",
    "        *code_desc('apdc'),\n",
    "        *code_desc('levl'),\n",
    "        *code_desc('styp'),\n",
    "        *code_desc('admt'),\n",
    "        *code_desc('camp'),\n",
    "        *code_desc('coll'),\n",
    "        *code_desc('dept'),\n",
    "        *code_desc('majr'),\n",
    "        *code_desc('cnty'),\n",
    "        *code_desc('stat'),\n",
    "        *code_desc('natn'),\n",
    "        *code_desc('resd'),\n",
    "        *code_desc('lgcy'),\n",
    "        'international',\n",
    "        'gender',\n",
    "        *[f'race_{r}' for r in ['american_indian','asian','black','pacific','white','hispanic']],\n",
    "        'waiver',\n",
    "        'birth_day',\n",
    "        'distance',\n",
    "        'hs_qrtl',\n",
    "    ],\n",
    "    'cycle_day': (TERM(term_code=202408).cycle_date-pd.Timestamp.now()).days+1,\n",
    "    # 'cycle_day': 186,\n",
    "    'crse': [\n",
    "        # 'engl1301',\n",
    "        # 'biol1406',\n",
    "        # 'biol2401',\n",
    "        # 'math1314',\n",
    "        # 'math2412',\n",
    "        # 'agri1419',\n",
    "        # 'psyc2301',\n",
    "        # 'ansc1319',\n",
    "        # 'comm1311',\n",
    "        # 'hist1301',\n",
    "        # 'govt2306',\n",
    "        # 'math1324',\n",
    "        # 'chem1411',\n",
    "        # 'univ0301',\n",
    "        # 'univ0204',\n",
    "        # 'univ0304',\n",
    "        # 'agri1100',\n",
    "        # 'comm1315',\n",
    "        # 'agec2317',\n",
    "        # 'govt2305',\n",
    "        # 'busi1301',\n",
    "        # 'arts1301',\n",
    "        # 'math1342',\n",
    "        # 'math2413',\n",
    "        ],\n",
    "    'trf_grid': {\n",
    "        'appl_day': passthru,\n",
    "        'apdc_day': passthru,\n",
    "        'birth_day': [*passthru],#, pwrtrf],#, bintrf(5)],\n",
    "        # 'levl_code': passthru,\n",
    "        # 'styp_code': passthru,\n",
    "        # 'admt_code': passdrop,\n",
    "        # 'camp_code': passdrop,\n",
    "        'remote': passdrop,\n",
    "        'coll_code': passdrop,\n",
    "        'international': passdrop,\n",
    "        **{f'race_{r}': passthru for r in ['american_indian','asian','black','pacific','white','hispanic']},\n",
    "        'gender': passthru,\n",
    "        'lgcy': passthru,\n",
    "        'resd': passthru,\n",
    "        'waiver': passthru,\n",
    "        # 'fafsa_app': passthru,\n",
    "        'schlship_app': passthru,\n",
    "        # 'finaid_accepted': passthru,\n",
    "        'ssb': passthru,\n",
    "        'math': passthru,\n",
    "        'reading': passthru,\n",
    "        'writing': passthru,\n",
    "        'gap_score': passthru,\n",
    "        'oriented': passthru,\n",
    "        'hs_qrtl': passthru,\n",
    "        'act_equiv': passthru,\n",
    "        'distance': [*passthru],#, pwrtrf],#, bintrf(5)],\n",
    "        },\n",
    "    'imp_grid': {\n",
    "        # 'datasets': [1, 5],\n",
    "        'datasets': 5,\n",
    "        'mmc': np.arange(0,41, 5),\n",
    "        # 'mmf': ['mean_match_default'],#, 'mean_match_shap'],\n",
    "        # 'iterations': 3,\n",
    "        # 'tune': [False, True],\n",
    "    },\n",
    "    'overwrite': {\n",
    "        # 'reg':True,\n",
    "        # 'adm':True,\n",
    "        # 'flg':True,\n",
    "        # 'raw':True,\n",
    "        # 'term': True,\n",
    "        # 'raw_df': True,\n",
    "        # 'reg_df': True,\n",
    "        # 'X': True,\n",
    "        # 'Y': True,\n",
    "        # 'pred': True,\n",
    "    },\n",
    "    # 'inspect': True,\n",
    "}\n",
    "\n",
    "# FLAGS().run()\n",
    "self = AMP(**kwargs)\n",
    "self = self.preprocess()\n",
    "self.term_codes.remove(self.infer_term)\n",
    "P = self.main()\n",
    "# print(self.params_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'imp': {'datasets': 5, 'iterations': 3, 'mmc': 0, 'mmf': 'mean_match_default', 'tune': True}, 'trf': {'act_equiv': 'passthrough', 'apdc_day': 'passthrough', 'appl_day': 'passthrough', 'birth_day': 'passthrough', 'coll_code': 'passthrough', 'distance': 'passthrough', 'gap_score': 'passthrough', 'gender': 'passthrough', 'hs_qrtl': 'passthrough', 'international': 'passthrough', 'lgcy': 'passthrough', 'math': 'passthrough', 'oriented': 'passthrough', 'race_american_indian': 'passthrough', 'race_asian': 'passthrough', 'race_black': 'passthrough', 'race_hispanic': 'passthrough', 'race_pacific': 'passthrough', 'race_white': 'passthrough', 'reading': 'passthrough', 'remote': 'passthrough', 'resd': 'passthrough', 'schlship_app': 'passthrough', 'ssb': 'passthrough', 'waiver': 'passthrough', 'writing': 'passthrough'}}\n",
      "\n",
      "{'imp': {'datasets': 5, 'iterations': 3, 'mmc': 5, 'mmf': 'mean_match_default', 'tune': True}, 'trf': {'act_equiv': 'passthrough', 'apdc_day': 'passthrough', 'appl_day': 'passthrough', 'birth_day': 'passthrough', 'coll_code': 'passthrough', 'distance': 'passthrough', 'gap_score': 'passthrough', 'gender': 'passthrough', 'hs_qrtl': 'passthrough', 'international': 'passthrough', 'lgcy': 'passthrough', 'math': 'passthrough', 'oriented': 'passthrough', 'race_american_indian': 'passthrough', 'race_asian': 'passthrough', 'race_black': 'passthrough', 'race_hispanic': 'passthrough', 'race_pacific': 'passthrough', 'race_white': 'passthrough', 'reading': 'passthrough', 'remote': 'passthrough', 'resd': 'passthrough', 'schlship_app': 'passthrough', 'ssb': 'passthrough', 'waiver': 'passthrough', 'writing': 'passthrough'}}\n",
      "\n",
      "{'imp': {'datasets': 5, 'iterations': 3, 'mmc': 10, 'mmf': 'mean_match_default', 'tune': True}, 'trf': {'act_equiv': 'passthrough', 'apdc_day': 'passthrough', 'appl_day': 'passthrough', 'birth_day': 'passthrough', 'coll_code': 'passthrough', 'distance': 'passthrough', 'gap_score': 'passthrough', 'gender': 'passthrough', 'hs_qrtl': 'passthrough', 'international': 'passthrough', 'lgcy': 'passthrough', 'math': 'passthrough', 'oriented': 'passthrough', 'race_american_indian': 'passthrough', 'race_asian': 'passthrough', 'race_black': 'passthrough', 'race_hispanic': 'passthrough', 'race_pacific': 'passthrough', 'race_white': 'passthrough', 'reading': 'passthrough', 'remote': 'passthrough', 'resd': 'passthrough', 'schlship_app': 'passthrough', 'ssb': 'passthrough', 'waiver': 'passthrough', 'writing': 'passthrough'}}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for x in self.params_list:\n",
    "    print(x)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__hs_qrtl\n",
       "2    13058\n",
       "1    10497\n",
       "0     4942\n",
       "3     3601\n",
       "4      140\n",
       "Name: count, dtype: Int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self.X['__hs_qrtl'].value_counts(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>train_term</th>\n",
       "      <th>202008</th>\n",
       "      <th>202108</th>\n",
       "      <th>202208</th>\n",
       "      <th>202308</th>\n",
       "      <th>abs_mean</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>crse</th>\n",
       "      <th>styp_code</th>\n",
       "      <th>pred_term</th>\n",
       "      <th>err_pct</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">_total</th>\n",
       "      <th rowspan=\"4\" valign=\"top\">n</th>\n",
       "      <th>202008</th>\n",
       "      <th>50%</th>\n",
       "      <td>-1.105583</td>\n",
       "      <td>11.608624</td>\n",
       "      <td>5.085683</td>\n",
       "      <td>-5.362078</td>\n",
       "      <td>5.790492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202108</th>\n",
       "      <th>50%</th>\n",
       "      <td>4.438964</td>\n",
       "      <td>-0.863132</td>\n",
       "      <td>-2.404439</td>\n",
       "      <td>-25.524044</td>\n",
       "      <td>8.307645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202208</th>\n",
       "      <th>50%</th>\n",
       "      <td>5.443975</td>\n",
       "      <td>-4.016913</td>\n",
       "      <td>-0.89852</td>\n",
       "      <td>-25.211416</td>\n",
       "      <td>8.892706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202308</th>\n",
       "      <th>50%</th>\n",
       "      <td>9.556159</td>\n",
       "      <td>4.483696</td>\n",
       "      <td>1.539855</td>\n",
       "      <td>-0.452899</td>\n",
       "      <td>4.008152</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "train_term                            202008     202108    202208     202308  \\\n",
       "crse   styp_code pred_term err_pct                                             \n",
       "_total n         202008    50%     -1.105583  11.608624  5.085683  -5.362078   \n",
       "                 202108    50%      4.438964  -0.863132 -2.404439 -25.524044   \n",
       "                 202208    50%      5.443975  -4.016913  -0.89852 -25.211416   \n",
       "                 202308    50%      9.556159   4.483696  1.539855  -0.452899   \n",
       "\n",
       "train_term                          abs_mean  \n",
       "crse   styp_code pred_term err_pct            \n",
       "_total n         202008    50%      5.790492  \n",
       "                 202108    50%      8.307645  \n",
       "                 202208    50%      8.892706  \n",
       "                 202308    50%      4.008152  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = list(self.pred.values())[0]['rslt']['err_pct'].query(\"err_pct==' 50%'\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>train_term</th>\n",
       "      <th>202008</th>\n",
       "      <th>202108</th>\n",
       "      <th>202208</th>\n",
       "      <th>202308</th>\n",
       "      <th>abs_mean</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>crse</th>\n",
       "      <th>styp_code</th>\n",
       "      <th>pred_term</th>\n",
       "      <th>err_pct</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"30\" valign=\"top\">_total</th>\n",
       "      <th rowspan=\"30\" valign=\"top\">n</th>\n",
       "      <th rowspan=\"6\" valign=\"top\">202008</th>\n",
       "      <th>count</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0%</th>\n",
       "      <td>0.497512</td>\n",
       "      <td>9.950249</td>\n",
       "      <td>11.553344</td>\n",
       "      <td>-5.306799</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.497512</td>\n",
       "      <td>9.950249</td>\n",
       "      <td>11.553344</td>\n",
       "      <td>-5.306799</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.497512</td>\n",
       "      <td>9.950249</td>\n",
       "      <td>11.553344</td>\n",
       "      <td>-5.306799</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.497512</td>\n",
       "      <td>9.950249</td>\n",
       "      <td>11.553344</td>\n",
       "      <td>-5.306799</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100%</th>\n",
       "      <td>0.497512</td>\n",
       "      <td>9.950249</td>\n",
       "      <td>11.553344</td>\n",
       "      <td>-5.306799</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"6\" valign=\"top\">202108</th>\n",
       "      <th>count</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0%</th>\n",
       "      <td>7.583231</td>\n",
       "      <td>0.061652</td>\n",
       "      <td>4.747226</td>\n",
       "      <td>-21.331689</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>7.583231</td>\n",
       "      <td>0.061652</td>\n",
       "      <td>4.747226</td>\n",
       "      <td>-21.331689</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>7.583231</td>\n",
       "      <td>0.061652</td>\n",
       "      <td>4.747226</td>\n",
       "      <td>-21.331689</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>7.583231</td>\n",
       "      <td>0.061652</td>\n",
       "      <td>4.747226</td>\n",
       "      <td>-21.331689</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100%</th>\n",
       "      <td>7.583231</td>\n",
       "      <td>0.061652</td>\n",
       "      <td>4.747226</td>\n",
       "      <td>-21.331689</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"6\" valign=\"top\">202208</th>\n",
       "      <th>count</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0%</th>\n",
       "      <td>6.712474</td>\n",
       "      <td>-2.114165</td>\n",
       "      <td>0.739958</td>\n",
       "      <td>-22.621564</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>6.712474</td>\n",
       "      <td>-2.114165</td>\n",
       "      <td>0.739958</td>\n",
       "      <td>-22.621564</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>6.712474</td>\n",
       "      <td>-2.114165</td>\n",
       "      <td>0.739958</td>\n",
       "      <td>-22.621564</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.712474</td>\n",
       "      <td>-2.114165</td>\n",
       "      <td>0.739958</td>\n",
       "      <td>-22.621564</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100%</th>\n",
       "      <td>6.712474</td>\n",
       "      <td>-2.114165</td>\n",
       "      <td>0.739958</td>\n",
       "      <td>-22.621564</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"6\" valign=\"top\">202308</th>\n",
       "      <th>count</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0%</th>\n",
       "      <td>8.288043</td>\n",
       "      <td>2.853261</td>\n",
       "      <td>11.413043</td>\n",
       "      <td>0.317029</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>8.288043</td>\n",
       "      <td>2.853261</td>\n",
       "      <td>11.413043</td>\n",
       "      <td>0.317029</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>8.288043</td>\n",
       "      <td>2.853261</td>\n",
       "      <td>11.413043</td>\n",
       "      <td>0.317029</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>8.288043</td>\n",
       "      <td>2.853261</td>\n",
       "      <td>11.413043</td>\n",
       "      <td>0.317029</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100%</th>\n",
       "      <td>8.288043</td>\n",
       "      <td>2.853261</td>\n",
       "      <td>11.413043</td>\n",
       "      <td>0.317029</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"6\" valign=\"top\">202408</th>\n",
       "      <th>count</th>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0%</th>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100%</th>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "train_term                            202008    202108     202208     202308  \\\n",
       "crse   styp_code pred_term err_pct                                             \n",
       "_total n         202008    count         1.0       1.0        1.0        1.0   \n",
       "                             0%     0.497512  9.950249  11.553344  -5.306799   \n",
       "                            25%     0.497512  9.950249  11.553344  -5.306799   \n",
       "                            50%     0.497512  9.950249  11.553344  -5.306799   \n",
       "                            75%     0.497512  9.950249  11.553344  -5.306799   \n",
       "                           100%     0.497512  9.950249  11.553344  -5.306799   \n",
       "                 202108    count         1.0       1.0        1.0        1.0   \n",
       "                             0%     7.583231  0.061652   4.747226 -21.331689   \n",
       "                            25%     7.583231  0.061652   4.747226 -21.331689   \n",
       "                            50%     7.583231  0.061652   4.747226 -21.331689   \n",
       "                            75%     7.583231  0.061652   4.747226 -21.331689   \n",
       "                           100%     7.583231  0.061652   4.747226 -21.331689   \n",
       "                 202208    count         1.0       1.0        1.0        1.0   \n",
       "                             0%     6.712474 -2.114165   0.739958 -22.621564   \n",
       "                            25%     6.712474 -2.114165   0.739958 -22.621564   \n",
       "                            50%     6.712474 -2.114165   0.739958 -22.621564   \n",
       "                            75%     6.712474 -2.114165   0.739958 -22.621564   \n",
       "                           100%     6.712474 -2.114165   0.739958 -22.621564   \n",
       "                 202308    count         1.0       1.0        1.0        1.0   \n",
       "                             0%     8.288043  2.853261  11.413043   0.317029   \n",
       "                            25%     8.288043  2.853261  11.413043   0.317029   \n",
       "                            50%     8.288043  2.853261  11.413043   0.317029   \n",
       "                            75%     8.288043  2.853261  11.413043   0.317029   \n",
       "                           100%     8.288043  2.853261  11.413043   0.317029   \n",
       "                 202408    count         4.0       4.0        4.0        4.0   \n",
       "                             0%        100.0     100.0      100.0      100.0   \n",
       "                            25%        100.0     100.0      100.0      100.0   \n",
       "                            50%        100.0     100.0      100.0      100.0   \n",
       "                            75%        100.0     100.0      100.0      100.0   \n",
       "                           100%        100.0     100.0      100.0      100.0   \n",
       "\n",
       "train_term                          abs_mean  \n",
       "crse   styp_code pred_term err_pct            \n",
       "_total n         202008    count        <NA>  \n",
       "                             0%         <NA>  \n",
       "                            25%         <NA>  \n",
       "                            50%         <NA>  \n",
       "                            75%         <NA>  \n",
       "                           100%         <NA>  \n",
       "                 202108    count        <NA>  \n",
       "                             0%         <NA>  \n",
       "                            25%         <NA>  \n",
       "                            50%         <NA>  \n",
       "                            75%         <NA>  \n",
       "                           100%         <NA>  \n",
       "                 202208    count        <NA>  \n",
       "                             0%         <NA>  \n",
       "                            25%         <NA>  \n",
       "                            50%         <NA>  \n",
       "                            75%         <NA>  \n",
       "                           100%         <NA>  \n",
       "                 202308    count        <NA>  \n",
       "                             0%         <NA>  \n",
       "                            25%         <NA>  \n",
       "                            50%         <NA>  \n",
       "                            75%         <NA>  \n",
       "                           100%         <NA>  \n",
       "                 202408    count        <NA>  \n",
       "                             0%         <NA>  \n",
       "                            25%         <NA>  \n",
       "                            50%         <NA>  \n",
       "                            75%         <NA>  \n",
       "                           100%         <NA>  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pivot(df, val):\n",
    "    idx = ['crse','styp_code','pred_term']\n",
    "    am = lambda x: x.abs().mean()#axis=1)\n",
    "    Y = (\n",
    "        df\n",
    "        .reset_index()\n",
    "        .pivot_table(columns='train_term', index=idx, values=val, aggfunc=['count',pctl(0),pctl(25),pctl(50),pctl(75),pctl(100)])\n",
    "        .rename_axis(columns=[val,'train_term'])\n",
    "        # .append(lambda x: x.abs().mean(axis=0))\n",
    "        # .T\n",
    "        # .assign(abs_mean = am)\n",
    "        # .T\n",
    "        .stack(0, future_stack=True)\n",
    "        .assign(abs_mean = am)\n",
    "    )\n",
    "    # Y = pd.concat([Y, Y.abs().mean().to_frame(0)])\n",
    "    # A.disp(1000)\n",
    "    # Z = Y.groupby(['crse','styp_code',val]).agg(lambda x:x.abs().mean()).reset_index().assign(pred_term='abs_mean').set_index(Y.index.names)\n",
    "    \n",
    "    # Z.disp(100)\n",
    "    # Y = pd.concat([Y,Z])\n",
    "    # am = lambda x: x.abs().mean()\n",
    "    # am2 = lambda x: am(am(x))\n",
    "    # Y = Y.groupby(['crse','styp_code']).transform(am)\n",
    "    # Y = Y.assign(abs_mean = lambda x: x.abs().mean(axis=1)).T\n",
    "    # Y = Y.assign(abs_mean = lambda x: x.abs().mean(axis=1)).T\n",
    "    return Y\n",
    "    # return pd.concat([Y,Z])\n",
    "\n",
    "df = list(self.pred.values())[0]['summary']\n",
    "val = 'err_pct'\n",
    "pivot(df, val)\n",
    "# A.groupby(['crse','styp_code',val]).agg(lambda x:x.abs().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.2000000000000002"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "def pctl(p):\n",
    "    p = round(p if p >= 1 else p*100)\n",
    "    f = lambda x: x.quantile(p/100)\n",
    "    f.__name__ = f'{p}%'.rjust(4)\n",
    "    return f\n",
    "\n",
    "# def pctl(ser, p):\n",
    "\n",
    "class M(pd.DataFrame):\n",
    "    def pctl(self, p, col):\n",
    "        p = round(p if p >= 1 else p*100)\n",
    "        \n",
    "        f = lambda self: self.quantile(p/100)\n",
    "        f.__name__ = f'{p}%'.rjust(4)\n",
    "\n",
    "\n",
    "\n",
    "# df = pd.DataFrame()\n",
    "# # df['g'] = ['a','b','a']\n",
    "# df['g'] = [1,2,3]\n",
    "# df['x'] = [2,3,-4.4]\n",
    "# pctl(25)(df['x'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.pctl.<locals>.f(x)>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "def pd_ext(func):\n",
    "    def wrapper(X, *args, **kwargs):\n",
    "        try:\n",
    "            Y = func(X, *args, **kwargs)\n",
    "        except:\n",
    "            Y = pd.DataFrame(X)\n",
    "            try:\n",
    "                Y = func(Y, *args, **kwargs)\n",
    "            except:\n",
    "                Y = Y.apply(func, *args, **kwargs)\n",
    "        if isinstance(X, pd.Series):\n",
    "            try:\n",
    "                Y = Y.squeeze()\n",
    "            except:\n",
    "                pass\n",
    "        return Y\n",
    "    wrapper.__name__ = func.__name__\n",
    "    for cls in [pd.DataFrame, pd.Series]:\n",
    "        setattr(cls, wrapper.__name__, wrapper)\n",
    "    return wrapper\n",
    "\n",
    "def rename(name):\n",
    "    def wrapper(f):\n",
    "        f.__name__ = name\n",
    "        return f\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "# @pd_ext\n",
    "# def pctl(ser, p):\n",
    "#     assert isinstance(ser, pd.Series)\n",
    "#     p = round(p if p >= 1 else p*100)\n",
    "#     return ser.quantile(p/100)\n",
    "\n",
    "\n",
    "\n",
    "# @rename('a')\n",
    "@pd_ext\n",
    "def rnd(ser, digits=0):\n",
    "    assert isinstance(ser, pd.Series)\n",
    "    return ser.round(digits)\n",
    "\n",
    "# @pd_ext\n",
    "# def pctl(ser, p):\n",
    "#     @rename(f'{p}%'.rjust(4))\n",
    "#     def f(p):\n",
    "#         assert isinstance(ser, pd.Series)\n",
    "#         p = round(p if p >= 1 else p*100)\n",
    "#         return ser.quantile(p/100)\n",
    "#     return f\n",
    "#     # f = lambda x: x.quantile(p/100)\n",
    "#     # f.__name__ = f'{p}%'.rjust(4)\n",
    "#     # return f\n",
    "\n",
    "def pctl(p):\n",
    "    p = round(p if p >= 1 else p*100)\n",
    "    def f(x):\n",
    "        return lambda x: x.quantile(p/100)\n",
    "    f.__name__ = f'{p}%'.rjust(4)\n",
    "    return f\n",
    "\n",
    "# def pctl(p):\n",
    "#     @rename(str(p))\n",
    "#     def f(x, p):\n",
    "#         p = round(p if p >= 1 else p*100)\n",
    "#         return x.quantile(p/100)\n",
    "#     return f\n",
    "\n",
    "# @pd_ext\n",
    "# def pctl_meth(ser, p):\n",
    "#     return pctl(p)(ser)\n",
    "\n",
    "# # pctl(25)\n",
    "df = pd.DataFrame()\n",
    "# df['g'] = ['a','b','a']\n",
    "df['g'] = [1,2,3]\n",
    "df['x'] = [2,3,-4.4]\n",
    "pctl(25)#(df['x'])\n",
    "\n",
    "# df.pctl_meth(25)\n",
    "# df.pctl_meth.__name__\n",
    "# pctl(50)#.__name__\n",
    "# df['x'].rnd()\n",
    "# df.rnd.__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "()\n",
      "{}\n",
      "new name\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def rename(newname):\n",
    "    def decorator(f):\n",
    "        f.__name__ = newname\n",
    "        return f\n",
    "    return decorator\n",
    "\n",
    "@rename('new name')\n",
    "def g(x, y):\n",
    "    return 2*x + 3*y\n",
    "print(g.__name__)\n",
    "g(y=1, x=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "pd_ext() missing 1 required positional argument: 'func'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 30\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ser, pd\u001b[38;5;241m.\u001b[39mSeries)\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ser\u001b[38;5;241m.\u001b[39mround(digits)\n\u001b[0;32m---> 30\u001b[0m \u001b[38;5;129m@pd_ext\u001b[39m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhi\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mabsmean\u001b[39m(ser, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ser, pd\u001b[38;5;241m.\u001b[39mSeries)\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ser\u001b[38;5;241m.\u001b[39mabs()\u001b[38;5;241m.\u001b[39mmean(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;31mTypeError\u001b[0m: pd_ext() missing 1 required positional argument: 'func'"
     ]
    }
   ],
   "source": [
    "# from LiveAMP import *\n",
    "import pandas as pd\n",
    "\n",
    "def pd_ext(name=None):\n",
    "    def wrapper(func, X, *args, **kwargs):\n",
    "        try:\n",
    "            Y = func(X, *args, **kwargs)\n",
    "        except:\n",
    "            Y = pd.DataFrame(X)\n",
    "            try:\n",
    "                Y = func(Y, *args, **kwargs)\n",
    "            except:\n",
    "                Y = Y.apply(func, *args, **kwargs)\n",
    "        if isinstance(X, pd.Series):\n",
    "            try:\n",
    "                Y = Y.squeeze()\n",
    "            except:\n",
    "                pass\n",
    "        return Y\n",
    "    wrapper.__name__ = func.__name__ if name is None else name\n",
    "    for cls in [pd.DataFrame, pd.Series]:\n",
    "        setattr(cls, wrapper.__name__, wrapper)\n",
    "    return wrapper\n",
    "\n",
    "@pd_ext\n",
    "def rnd(ser, digits=0):\n",
    "    assert isinstance(ser, pd.Series)\n",
    "    return ser.round(digits)\n",
    "\n",
    "@pd_ext(name='hi')\n",
    "def absmean(ser, **kwargs):\n",
    "    assert isinstance(ser, pd.Series)\n",
    "    return ser.abs().mean(**kwargs)\n",
    "\n",
    "# @pd_ext('hi')\n",
    "def absmedian(ser, **kwargs):\n",
    "    assert isinstance(ser, pd.Series)\n",
    "    return ser.abs().median(**kwargs)\n",
    "\n",
    "# @pd_ext\n",
    "# def pctl(ser, p):\n",
    "#     assert isinstance(ser, pd.Series)\n",
    "#     p = round(p if p >= 1 else p*100)\n",
    "#     f = lambda x: x.quantile(p/100)\n",
    "#     f.__name__ = f'{p}%'.rjust(4)\n",
    "#     return f\n",
    "\n",
    "# for func in [rnd]:\n",
    "#     for cls in [pd.DataFrame, pd.Series]:\n",
    "#         setattr(cls, func.__name__, func)\n",
    "\n",
    "# def pctl_func(p, **kwargs):\n",
    "#     p = round(p if p >= 1 else p*100)\n",
    "#     f = lambda x: x.quantile(p/100, **kwargs)\n",
    "#     f.__name__ = f'{p}%'.rjust(4)\n",
    "#     return f\n",
    "\n",
    "# def pctl(self, p, **kwargs):\n",
    "#     return pctl_func(p, **kwargs)(self)\n",
    "\n",
    "# for func in [pctl]:\n",
    "#     for cls in [pd.DataFrame, pd.Series]:\n",
    "#         setattr(cls, func.__name__, func)\n",
    "def pctl(p):\n",
    "    p = round(p if p >= 1 else p*100)\n",
    "    f = lambda x: x.quantile(p/100)\n",
    "    f.__name__ = f'{p}%'.rjust(4)\n",
    "    return f\n",
    "\n",
    "\n",
    "df = pd.DataFrame()\n",
    "# df['g'] = ['a','b','a']\n",
    "df['g'] = [1,2,3]\n",
    "df['x'] = [2,3,-4.4]\n",
    "df.absmean.__name__\n",
    "# df.rnd.__name__\n",
    "# # df.groupby('g').quantile(0.25)\n",
    "# df.groupby('g').agg(pctl(0.25))\n",
    "# pctl(25)(df['x'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'to_numeric'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def to_numeric(ser, dtype_backend='numpy_nullable', errors='ignore'):\n",
    "    assert isinstance(ser, pd.Series)\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(action='ignore', category=(FutureWarning,UserWarning))\n",
    "        dt = str(ser.dtype).lower()\n",
    "        if 'geometry' not in dt and 'bool' not in dt and 'category' not in dt:\n",
    "            ser = pd.to_numeric(ser.astype('string').str.lower().str.strip().replace('',pd.NA), downcast='integer', errors=errors)\n",
    "            if pd.api.types.is_string_dtype(ser):\n",
    "                ser = pd.to_datetime(ser, errors=errors)\n",
    "            elif pd.api.types.is_integer_dtype(ser):\n",
    "                ser = ser.astype('Int64', errors=errors)\n",
    "        return ser.convert_dtypes(dtype_backend)\n",
    "to_numeric.__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{'imp': {'datasets': 1, 'iterations': 1}, 'trf': {'act_equiv': 'passthrough', 'apdc_day': 'passthrough', 'appl_day': 'passthrough', 'birth_day': 'passthrough', 'coll_code': 'passthrough', 'distance': 'passthrough', 'gap_score': 'passthrough', 'gender': 'passthrough', 'hs_qrtl': 'passthrough', 'international': 'passthrough', 'lgcy': 'passthrough', 'math': 'passthrough', 'oriented': 'passthrough', 'race_american_indian': 'passthrough', 'race_asian': 'passthrough', 'race_black': 'passthrough', 'race_hispanic': 'passthrough', 'race_pacific': 'passthrough', 'race_white': 'passthrough', 'reading': 'passthrough', 'remote': 'passthrough', 'resd': 'passthrough', 'schlship_app': 'passthrough', 'ssb': 'passthrough', 'waiver': 'passthrough', 'writing': 'passthrough'}}\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(self.params_list[0])\n",
    "# list(self.pred.keys())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'details'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpred\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdetails\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;66;03m#['_total']#[202208]\u001b[39;00m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'details'"
     ]
    }
   ],
   "source": [
    "list(self.pred.values())[1]['details']#['_total']#[202208]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>ColumnTransformer(transformers=[(&#x27;act_equiv&#x27;, &#x27;passthrough&#x27;, [&#x27;__act_equiv&#x27;]),\n",
       "                                (&#x27;apdc_day&#x27;, &#x27;passthrough&#x27;, [&#x27;__apdc_day&#x27;]),\n",
       "                                (&#x27;appl_day&#x27;, &#x27;passthrough&#x27;, [&#x27;__appl_day&#x27;]),\n",
       "                                (&#x27;birth_day&#x27;,\n",
       "                                 Pipeline(steps=[(&#x27;standardscaler&#x27;,\n",
       "                                                  StandardScaler()),\n",
       "                                                 (&#x27;powertransformer&#x27;,\n",
       "                                                  PowerTransformer())]),\n",
       "                                 [&#x27;__birth_day&#x27;]),\n",
       "                                (&#x27;coll_code&#x27;, &#x27;drop&#x27;, [&#x27;__coll_code&#x27;]),\n",
       "                                (&#x27;distance&#x27;,\n",
       "                                 Pipeline(steps=[(...\n",
       "                                 [&#x27;__race_pacific&#x27;]),\n",
       "                                (&#x27;race_white&#x27;, &#x27;passthrough&#x27;, [&#x27;__race_white&#x27;]),\n",
       "                                (&#x27;reading&#x27;, &#x27;passthrough&#x27;, [&#x27;__reading&#x27;]),\n",
       "                                (&#x27;remote&#x27;, &#x27;drop&#x27;, [&#x27;__remote&#x27;]),\n",
       "                                (&#x27;resd&#x27;, &#x27;passthrough&#x27;, [&#x27;__resd&#x27;]),\n",
       "                                (&#x27;schlship_app&#x27;, &#x27;passthrough&#x27;,\n",
       "                                 [&#x27;__schlship_app&#x27;]),\n",
       "                                (&#x27;ssb&#x27;, &#x27;passthrough&#x27;, [&#x27;__ssb&#x27;]),\n",
       "                                (&#x27;waiver&#x27;, &#x27;passthrough&#x27;, [&#x27;__waiver&#x27;]),\n",
       "                                (&#x27;writing&#x27;, &#x27;passthrough&#x27;, [&#x27;__writing&#x27;])],\n",
       "                  verbose_feature_names_out=False)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;ColumnTransformer<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.compose.ColumnTransformer.html\">?<span>Documentation for ColumnTransformer</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>ColumnTransformer(transformers=[(&#x27;act_equiv&#x27;, &#x27;passthrough&#x27;, [&#x27;__act_equiv&#x27;]),\n",
       "                                (&#x27;apdc_day&#x27;, &#x27;passthrough&#x27;, [&#x27;__apdc_day&#x27;]),\n",
       "                                (&#x27;appl_day&#x27;, &#x27;passthrough&#x27;, [&#x27;__appl_day&#x27;]),\n",
       "                                (&#x27;birth_day&#x27;,\n",
       "                                 Pipeline(steps=[(&#x27;standardscaler&#x27;,\n",
       "                                                  StandardScaler()),\n",
       "                                                 (&#x27;powertransformer&#x27;,\n",
       "                                                  PowerTransformer())]),\n",
       "                                 [&#x27;__birth_day&#x27;]),\n",
       "                                (&#x27;coll_code&#x27;, &#x27;drop&#x27;, [&#x27;__coll_code&#x27;]),\n",
       "                                (&#x27;distance&#x27;,\n",
       "                                 Pipeline(steps=[(...\n",
       "                                 [&#x27;__race_pacific&#x27;]),\n",
       "                                (&#x27;race_white&#x27;, &#x27;passthrough&#x27;, [&#x27;__race_white&#x27;]),\n",
       "                                (&#x27;reading&#x27;, &#x27;passthrough&#x27;, [&#x27;__reading&#x27;]),\n",
       "                                (&#x27;remote&#x27;, &#x27;drop&#x27;, [&#x27;__remote&#x27;]),\n",
       "                                (&#x27;resd&#x27;, &#x27;passthrough&#x27;, [&#x27;__resd&#x27;]),\n",
       "                                (&#x27;schlship_app&#x27;, &#x27;passthrough&#x27;,\n",
       "                                 [&#x27;__schlship_app&#x27;]),\n",
       "                                (&#x27;ssb&#x27;, &#x27;passthrough&#x27;, [&#x27;__ssb&#x27;]),\n",
       "                                (&#x27;waiver&#x27;, &#x27;passthrough&#x27;, [&#x27;__waiver&#x27;]),\n",
       "                                (&#x27;writing&#x27;, &#x27;passthrough&#x27;, [&#x27;__writing&#x27;])],\n",
       "                  verbose_feature_names_out=False)</pre></div> </div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">act_equiv</label><div class=\"sk-toggleable__content fitted\"><pre>[&#x27;__act_equiv&#x27;]</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">passthrough</label><div class=\"sk-toggleable__content fitted\"><pre>passthrough</pre></div> </div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">apdc_day</label><div class=\"sk-toggleable__content fitted\"><pre>[&#x27;__apdc_day&#x27;]</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">passthrough</label><div class=\"sk-toggleable__content fitted\"><pre>passthrough</pre></div> </div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">appl_day</label><div class=\"sk-toggleable__content fitted\"><pre>[&#x27;__appl_day&#x27;]</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">passthrough</label><div class=\"sk-toggleable__content fitted\"><pre>passthrough</pre></div> </div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">birth_day</label><div class=\"sk-toggleable__content fitted\"><pre>[&#x27;__birth_day&#x27;]</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" ><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;StandardScaler<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.preprocessing.StandardScaler.html\">?<span>Documentation for StandardScaler</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>StandardScaler()</pre></div> </div></div><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" ><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;PowerTransformer<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.preprocessing.PowerTransformer.html\">?<span>Documentation for PowerTransformer</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>PowerTransformer()</pre></div> </div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-11\" type=\"checkbox\" ><label for=\"sk-estimator-id-11\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">coll_code</label><div class=\"sk-toggleable__content fitted\"><pre>[&#x27;__coll_code&#x27;]</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-12\" type=\"checkbox\" ><label for=\"sk-estimator-id-12\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">drop</label><div class=\"sk-toggleable__content fitted\"><pre>drop</pre></div> </div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-13\" type=\"checkbox\" ><label for=\"sk-estimator-id-13\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">distance</label><div class=\"sk-toggleable__content fitted\"><pre>[&#x27;__distance&#x27;]</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-14\" type=\"checkbox\" ><label for=\"sk-estimator-id-14\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;StandardScaler<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.preprocessing.StandardScaler.html\">?<span>Documentation for StandardScaler</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>StandardScaler()</pre></div> </div></div><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-15\" type=\"checkbox\" ><label for=\"sk-estimator-id-15\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;PowerTransformer<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.preprocessing.PowerTransformer.html\">?<span>Documentation for PowerTransformer</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>PowerTransformer()</pre></div> </div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-16\" type=\"checkbox\" ><label for=\"sk-estimator-id-16\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">gap_score</label><div class=\"sk-toggleable__content fitted\"><pre>[&#x27;__gap_score&#x27;]</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-17\" type=\"checkbox\" ><label for=\"sk-estimator-id-17\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">passthrough</label><div class=\"sk-toggleable__content fitted\"><pre>passthrough</pre></div> </div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-18\" type=\"checkbox\" ><label for=\"sk-estimator-id-18\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">gender</label><div class=\"sk-toggleable__content fitted\"><pre>[&#x27;__gender&#x27;]</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-19\" type=\"checkbox\" ><label for=\"sk-estimator-id-19\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">passthrough</label><div class=\"sk-toggleable__content fitted\"><pre>passthrough</pre></div> </div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-20\" type=\"checkbox\" ><label for=\"sk-estimator-id-20\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">hs_qrtl</label><div class=\"sk-toggleable__content fitted\"><pre>[&#x27;__hs_qrtl&#x27;]</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-21\" type=\"checkbox\" ><label for=\"sk-estimator-id-21\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">passthrough</label><div class=\"sk-toggleable__content fitted\"><pre>passthrough</pre></div> </div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-22\" type=\"checkbox\" ><label for=\"sk-estimator-id-22\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">international</label><div class=\"sk-toggleable__content fitted\"><pre>[&#x27;__international&#x27;]</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-23\" type=\"checkbox\" ><label for=\"sk-estimator-id-23\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">drop</label><div class=\"sk-toggleable__content fitted\"><pre>drop</pre></div> </div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-24\" type=\"checkbox\" ><label for=\"sk-estimator-id-24\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">lgcy</label><div class=\"sk-toggleable__content fitted\"><pre>[&#x27;__lgcy&#x27;]</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-25\" type=\"checkbox\" ><label for=\"sk-estimator-id-25\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">passthrough</label><div class=\"sk-toggleable__content fitted\"><pre>passthrough</pre></div> </div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-26\" type=\"checkbox\" ><label for=\"sk-estimator-id-26\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">math</label><div class=\"sk-toggleable__content fitted\"><pre>[&#x27;__math&#x27;]</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-27\" type=\"checkbox\" ><label for=\"sk-estimator-id-27\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">passthrough</label><div class=\"sk-toggleable__content fitted\"><pre>passthrough</pre></div> </div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-28\" type=\"checkbox\" ><label for=\"sk-estimator-id-28\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">oriented</label><div class=\"sk-toggleable__content fitted\"><pre>[&#x27;__oriented&#x27;]</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-29\" type=\"checkbox\" ><label for=\"sk-estimator-id-29\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">passthrough</label><div class=\"sk-toggleable__content fitted\"><pre>passthrough</pre></div> </div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-30\" type=\"checkbox\" ><label for=\"sk-estimator-id-30\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">race_american_indian</label><div class=\"sk-toggleable__content fitted\"><pre>[&#x27;__race_american_indian&#x27;]</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-31\" type=\"checkbox\" ><label for=\"sk-estimator-id-31\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">passthrough</label><div class=\"sk-toggleable__content fitted\"><pre>passthrough</pre></div> </div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-32\" type=\"checkbox\" ><label for=\"sk-estimator-id-32\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">race_asian</label><div class=\"sk-toggleable__content fitted\"><pre>[&#x27;__race_asian&#x27;]</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-33\" type=\"checkbox\" ><label for=\"sk-estimator-id-33\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">passthrough</label><div class=\"sk-toggleable__content fitted\"><pre>passthrough</pre></div> </div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-34\" type=\"checkbox\" ><label for=\"sk-estimator-id-34\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">race_black</label><div class=\"sk-toggleable__content fitted\"><pre>[&#x27;__race_black&#x27;]</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-35\" type=\"checkbox\" ><label for=\"sk-estimator-id-35\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">passthrough</label><div class=\"sk-toggleable__content fitted\"><pre>passthrough</pre></div> </div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-36\" type=\"checkbox\" ><label for=\"sk-estimator-id-36\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">race_hispanic</label><div class=\"sk-toggleable__content fitted\"><pre>[&#x27;__race_hispanic&#x27;]</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-37\" type=\"checkbox\" ><label for=\"sk-estimator-id-37\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">passthrough</label><div class=\"sk-toggleable__content fitted\"><pre>passthrough</pre></div> </div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-38\" type=\"checkbox\" ><label for=\"sk-estimator-id-38\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">race_pacific</label><div class=\"sk-toggleable__content fitted\"><pre>[&#x27;__race_pacific&#x27;]</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-39\" type=\"checkbox\" ><label for=\"sk-estimator-id-39\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">passthrough</label><div class=\"sk-toggleable__content fitted\"><pre>passthrough</pre></div> </div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-40\" type=\"checkbox\" ><label for=\"sk-estimator-id-40\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">race_white</label><div class=\"sk-toggleable__content fitted\"><pre>[&#x27;__race_white&#x27;]</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-41\" type=\"checkbox\" ><label for=\"sk-estimator-id-41\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">passthrough</label><div class=\"sk-toggleable__content fitted\"><pre>passthrough</pre></div> </div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-42\" type=\"checkbox\" ><label for=\"sk-estimator-id-42\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">reading</label><div class=\"sk-toggleable__content fitted\"><pre>[&#x27;__reading&#x27;]</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-43\" type=\"checkbox\" ><label for=\"sk-estimator-id-43\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">passthrough</label><div class=\"sk-toggleable__content fitted\"><pre>passthrough</pre></div> </div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-44\" type=\"checkbox\" ><label for=\"sk-estimator-id-44\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">remote</label><div class=\"sk-toggleable__content fitted\"><pre>[&#x27;__remote&#x27;]</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-45\" type=\"checkbox\" ><label for=\"sk-estimator-id-45\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">drop</label><div class=\"sk-toggleable__content fitted\"><pre>drop</pre></div> </div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-46\" type=\"checkbox\" ><label for=\"sk-estimator-id-46\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">resd</label><div class=\"sk-toggleable__content fitted\"><pre>[&#x27;__resd&#x27;]</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-47\" type=\"checkbox\" ><label for=\"sk-estimator-id-47\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">passthrough</label><div class=\"sk-toggleable__content fitted\"><pre>passthrough</pre></div> </div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-48\" type=\"checkbox\" ><label for=\"sk-estimator-id-48\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">schlship_app</label><div class=\"sk-toggleable__content fitted\"><pre>[&#x27;__schlship_app&#x27;]</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-49\" type=\"checkbox\" ><label for=\"sk-estimator-id-49\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">passthrough</label><div class=\"sk-toggleable__content fitted\"><pre>passthrough</pre></div> </div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-50\" type=\"checkbox\" ><label for=\"sk-estimator-id-50\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">ssb</label><div class=\"sk-toggleable__content fitted\"><pre>[&#x27;__ssb&#x27;]</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-51\" type=\"checkbox\" ><label for=\"sk-estimator-id-51\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">passthrough</label><div class=\"sk-toggleable__content fitted\"><pre>passthrough</pre></div> </div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-52\" type=\"checkbox\" ><label for=\"sk-estimator-id-52\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">waiver</label><div class=\"sk-toggleable__content fitted\"><pre>[&#x27;__waiver&#x27;]</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-53\" type=\"checkbox\" ><label for=\"sk-estimator-id-53\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">passthrough</label><div class=\"sk-toggleable__content fitted\"><pre>passthrough</pre></div> </div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-54\" type=\"checkbox\" ><label for=\"sk-estimator-id-54\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">writing</label><div class=\"sk-toggleable__content fitted\"><pre>[&#x27;__writing&#x27;]</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-55\" type=\"checkbox\" ><label for=\"sk-estimator-id-55\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">passthrough</label><div class=\"sk-toggleable__content fitted\"><pre>passthrough</pre></div> </div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "ColumnTransformer(transformers=[('act_equiv', 'passthrough', ['__act_equiv']),\n",
       "                                ('apdc_day', 'passthrough', ['__apdc_day']),\n",
       "                                ('appl_day', 'passthrough', ['__appl_day']),\n",
       "                                ('birth_day',\n",
       "                                 Pipeline(steps=[('standardscaler',\n",
       "                                                  StandardScaler()),\n",
       "                                                 ('powertransformer',\n",
       "                                                  PowerTransformer())]),\n",
       "                                 ['__birth_day']),\n",
       "                                ('coll_code', 'drop', ['__coll_code']),\n",
       "                                ('distance',\n",
       "                                 Pipeline(steps=[(...\n",
       "                                 ['__race_pacific']),\n",
       "                                ('race_white', 'passthrough', ['__race_white']),\n",
       "                                ('reading', 'passthrough', ['__reading']),\n",
       "                                ('remote', 'drop', ['__remote']),\n",
       "                                ('resd', 'passthrough', ['__resd']),\n",
       "                                ('schlship_app', 'passthrough',\n",
       "                                 ['__schlship_app']),\n",
       "                                ('ssb', 'passthrough', ['__ssb']),\n",
       "                                ('waiver', 'passthrough', ['__waiver']),\n",
       "                                ('writing', 'passthrough', ['__writing'])],\n",
       "                  verbose_feature_names_out=False)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P['trf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>_total</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pidm</th>\n",
       "      <th>term_code</th>\n",
       "      <th>term_desc</th>\n",
       "      <th>apdc_code</th>\n",
       "      <th>apdc_desc</th>\n",
       "      <th>levl_code</th>\n",
       "      <th>levl_desc</th>\n",
       "      <th>styp_code</th>\n",
       "      <th>styp_desc</th>\n",
       "      <th>admt_code</th>\n",
       "      <th>admt_desc</th>\n",
       "      <th>camp_code</th>\n",
       "      <th>camp_desc</th>\n",
       "      <th>coll_code</th>\n",
       "      <th>coll_desc</th>\n",
       "      <th>dept_code</th>\n",
       "      <th>dept_desc</th>\n",
       "      <th>majr_code</th>\n",
       "      <th>majr_desc</th>\n",
       "      <th>cnty_code</th>\n",
       "      <th>cnty_desc</th>\n",
       "      <th>stat_code</th>\n",
       "      <th>stat_desc</th>\n",
       "      <th>natn_code</th>\n",
       "      <th>natn_desc</th>\n",
       "      <th>resd_code</th>\n",
       "      <th>resd_desc</th>\n",
       "      <th>lgcy_code</th>\n",
       "      <th>lgcy_desc</th>\n",
       "      <th>international</th>\n",
       "      <th>gender</th>\n",
       "      <th>race_american_indian</th>\n",
       "      <th>race_asian</th>\n",
       "      <th>race_black</th>\n",
       "      <th>race_pacific</th>\n",
       "      <th>race_white</th>\n",
       "      <th>race_hispanic</th>\n",
       "      <th>waiver</th>\n",
       "      <th>birth_day</th>\n",
       "      <th>distance</th>\n",
       "      <th>hs_qrtl</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>88089</th>\n",
       "      <th>202008</th>\n",
       "      <th>fall 2020</th>\n",
       "      <th>r2</th>\n",
       "      <th>admitted (rp2)</th>\n",
       "      <th>ug</th>\n",
       "      <th>undergraduate</th>\n",
       "      <th>n</th>\n",
       "      <th>new first time</th>\n",
       "      <th>ea</th>\n",
       "      <th>early admission (6th sem)</th>\n",
       "      <th>s</th>\n",
       "      <th>stephenville</th>\n",
       "      <th>hl</th>\n",
       "      <th>health sciences</th>\n",
       "      <th>hhpf</th>\n",
       "      <th>health &amp; human performance</th>\n",
       "      <th>kins</th>\n",
       "      <th>kinesiology</th>\n",
       "      <th>68</th>\n",
       "      <th>ector</th>\n",
       "      <th>tx</th>\n",
       "      <th>texas</th>\n",
       "      <th>&lt;NA&gt;</th>\n",
       "      <th>&lt;NA&gt;</th>\n",
       "      <th>r</th>\n",
       "      <th>texas resident</th>\n",
       "      <th>&lt;NA&gt;</th>\n",
       "      <th>&lt;NA&gt;</th>\n",
       "      <th>False</th>\n",
       "      <th>m</th>\n",
       "      <th>False</th>\n",
       "      <th>False</th>\n",
       "      <th>False</th>\n",
       "      <th>False</th>\n",
       "      <th>False</th>\n",
       "      <th>True</th>\n",
       "      <th>False</th>\n",
       "      <th>6695</th>\n",
       "      <th>16028.63</th>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "P.complete_data(0)[crse].disp(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__act_equiv                Float64\n",
       "__apdc_day                   Int64\n",
       "__appl_day                   Int64\n",
       "__birth_day                Float64\n",
       "__distance                 Float64\n",
       "__gap_score                  Int64\n",
       "__gender                  category\n",
       "__hs_qrtl                    Int64\n",
       "__lgcy                     boolean\n",
       "__math                     boolean\n",
       "__oriented                category\n",
       "__race_american_indian     boolean\n",
       "__race_asian               boolean\n",
       "__race_black               boolean\n",
       "__race_hispanic            boolean\n",
       "__race_pacific             boolean\n",
       "__race_white               boolean\n",
       "__reading                  boolean\n",
       "__resd                     boolean\n",
       "__schlship_app             boolean\n",
       "__ssb                      boolean\n",
       "__waiver                   boolean\n",
       "__writing                  boolean\n",
       "_total_cur                   Int64\n",
       "_total                     boolean\n",
       "train_term                   int64\n",
       "sim                          int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crse = '_total'\n",
    "train_term = 202208\n",
    "pred = pd.concat([Z.complete_data(k).assign(train_term=train_term, sim=k) for k in range(Z.dataset_count())])\n",
    "pred.dtypes\n",
    "# # T = self.Y[crse]\n",
    "# # # pd.concat([P, T], axis=1)\n",
    "# # # P.shape\n",
    "# # # P['a'].describe()\n",
    "# details = pd.concat([(\n",
    "#     self.Y[crse]\n",
    "#     .rename('true')\n",
    "#     .to_frame()\n",
    "#     # .join(P.complete_data(k)[crse])\n",
    "#     .assign(pred=P.complete_data(k)[crse], train_term=train_term, crse=crse, sim=k)\n",
    "#     .set_index(['train_term','crse','sim'], append=True)\n",
    "# ) for k in range(P.dataset_count())])\n",
    "\n",
    "# #     .binarize()\n",
    "# details.disp(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mf.ImputationKernel?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 'passthrough'\n",
    "pd = ['passthrough', 'drop']\n",
    "bin = lambda n_bins: KBinsDiscretizer(n_bins=n_bins, encode='ordinal', strategy='uniform')\n",
    "pwt = make_pipeline(StandardScaler(), PowerTransformer())\n",
    "trf_grid = {\n",
    "    'appl_day': p,\n",
    "    'apdc_day': p,\n",
    "    'birth_day': [p, bin(5), pwt],\n",
    "    # 'levl_code': p,\n",
    "    # 'styp_code': p,\n",
    "    # 'admt_code': pd,\n",
    "    # 'camp_code': pd,\n",
    "    'remote': pd,\n",
    "    'coll_code': pd,\n",
    "    'international': pd,\n",
    "    **{f'race_{r}': p for r in ['american_indian','asian','black','pacific','white','hispanic']},\n",
    "    'gender': p,\n",
    "    'lgcy': p,\n",
    "    'resd': p,\n",
    "    'waiver': p,\n",
    "    # 'fafsa_app': p,\n",
    "    'schlship_app': p,\n",
    "    # 'finaid_accepted': p,\n",
    "    'ssb': p,\n",
    "    'math': p,\n",
    "    'reading': p,\n",
    "    'writing': p,\n",
    "    'gap_score': p,\n",
    "    'oriented': p,\n",
    "    'hs_qrtl': p,\n",
    "    'act_equiv': p,\n",
    "    'distance': [p, bin(5), pwt],\n",
    "    }\n",
    "trf_list = cartesian({k: sorted(setify(v), key=str) for k,v in trf_grid.items()})\n",
    "imp_grid = {\n",
    "    'datasets': 5,\n",
    "    'mmc': [0, 5],\n",
    "    'mmf': ['mean_match_default', 'mean_match_shap'],\n",
    "    'iterations':4,\n",
    "}\n",
    "imp_list = cartesian(imp_grid)\n",
    "\n",
    "params_list = [{'trf':trf, 'imp':imp} for trf, imp in it.product(trf_list,imp_list)]\n",
    "\n",
    "\n",
    "def predict(self, params, crse, train_term, styp_code='all'):\n",
    "        for p, P in self.pred.items():\n",
    "            if p == str(params):\n",
    "                for c, C in P.items():\n",
    "                    if c == crse:\n",
    "                        for t, T in C.items():\n",
    "                            if t == train_term:\n",
    "                                for s, S in T.items():\n",
    "                                    if s == styp_code:\n",
    "                                        return S\n",
    "\n",
    "    print('creating', ljust(crse,8), train_term, styp_code)\n",
    "    cols = uniquify(['_total_cur',crse+'_cur',crse])\n",
    "    S = dict()\n",
    "\n",
    "    X = self.X.copy()\n",
    "    if styp_code != 'all':\n",
    "        X = X.query(f\"styp_code==@styp_code\")\n",
    "    trf = ColumnTransformer([(c,t,[\"__\"+c]) for c,t in params['trf'].items()], remainder='drop', verbose_feature_names_out=False)\n",
    "    Z = trf.fit_transform(X).categorize().join(self.Y[cols]).sort_index()\n",
    "\n",
    "\n",
    "    model = c[0](T, datasets=1, **c[1])\n",
    "    model.mice(1)\n",
    "    optimal_parameters, losses = model.tune_parameters(dataset=0, optimization_steps=5)\n",
    "    model.mice(iterations, variable_parameters=optimal_parameters)\n",
    "    model.inspect()\n",
    "    T.loc[:, crse] = pd.NA\n",
    "    pred = model.impute_new_data(T).complete_data(0).pop(crse)\n",
    "\n",
    "\n",
    "    y['true'] = Z[crse].copy()\n",
    "    Z.loc[Z.eval(\"term_code!=@train_term\"), crse] = pd.NA\n",
    "    iterations = params['imp'].pop('iterations') if 'iterations' in params['imp'] else 3\n",
    "    params['imp']['mean_match_scheme'] = getattr(mf, params['imp'].pop('mmf')).copy()\n",
    "    params['imp']['mean_match_scheme'].set_mean_match_candidates(params['imp'].pop('mmc'))\n",
    "    imp = mf.ImputationKernel(Z, **params['imp'])\n",
    "    imp.mice(1)\n",
    "    optimal_parameters, losses = imp.tune_parameters(dataset=0, optimization_steps=5)\n",
    "    imp.mice(iterations, variable_parameters=optimal_parameters)\n",
    "    imp.inspect()\n",
    "    Z.loc[:, crse] = pd.NA\n",
    "    P = imp.impute_new_data(Z)\n",
    "    P = pd.concat([P.complete])\n",
    "    P = imp.impute_new_data(Z).complete_data(0).pop(crse)\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "# imp = cartesian(imp)\n",
    "# imp\n",
    "# def g(d):\n",
    "#     d['mean_match_scheme'] = getattr(mf, d.pop('mmf')).copy()\n",
    "#     d['mean_match_scheme'].set_mean_match_candidates(d.pop('mmc'))\n",
    "#     return d\n",
    "\n",
    "# imp = [g(d) for d in cartesian(imp)]\n",
    "# # imp\n",
    "# # # imp = \n",
    "# # str(imp[0])\n",
    "\n",
    "# str(imp[0]['mean_match_scheme'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from miceforest import mean_match_default\n",
    "import miceforest as mf\n",
    "getattr(mf, 'mean_match_default')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# params_grid = {\n",
    "#     'enc': {\n",
    "#         OneHotEncoder: {\n",
    "#             'min_frequency': [0.1],\n",
    "#         },\n",
    "#     },\n",
    "#     'imp': {\n",
    "#         ImputationKernel: {\n",
    "#             'datasets': 5,\n",
    "#             # 'save_all_iterations': False,\n",
    "#             # 'mean_match_candidates': [29],\n",
    "#             # 'mean_match_function': mean_match_kdtree_classification,\n",
    "#             'mean_match_scheme': [mmc(25)],\n",
    "#             'iterations':4,\n",
    "#         },\n",
    "#     },\n",
    "#     'clf': {\n",
    "#         # RandomForestClassifier: {\n",
    "#         #     'max_depth': 3,\n",
    "#         # },\n",
    "#         HistGradientBoostingClassifier: {\n",
    "#             'max_depth': 3,\n",
    "#         },\n",
    "#         # ImputationKernel: {\n",
    "#         #     'save_all_iterations': False,\n",
    "#         #     # 'mean_match_candidates': [29],\n",
    "#         #     # 'mean_match_function': mean_match_kdtree_classification,\n",
    "#         #     'iterations':3,\n",
    "#         # }\n",
    "#     },\n",
    "# }\n",
    "# params_list = cartesian({step: [[alg, values] for alg, grid in D.items() for values in cartesian(grid)] for step, D in params_grid.items()})\n",
    "# def stringify(P):\n",
    "#     def f(x):\n",
    "#         try:\n",
    "#             return x.__name__\n",
    "#         except:\n",
    "#             return x\n",
    "#     return str({f(step): [f(D[0]), {f(k): f(v) for k, v in D[1].items()}] for step, D in P.items()})\n",
    "\n",
    "# self.main(params_list);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# StandardScaler().\n",
    "HistGradientBoostingClassifier(random_state=45).__str__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trf = sorted([\n",
    "trf = mysort({\n",
    "    'appl_day': 'passthrough',\n",
    "    'apdc_day': 'drop',\n",
    "    # 'birth_day': 'passthrough',\n",
    "    'birth_day': make_pipeline(StandardScaler(), PowerTransformer()),\n",
    "    # 'levl_code': 'passthrough',\n",
    "    # 'styp_code': 'passthrough',\n",
    "    # 'admt_code': 'passthrough',\n",
    "    # 'camp_code': 'passthrough',\n",
    "    'remote': 'passthrough',\n",
    "    'coll_code': 'passthrough',\n",
    "    'international': 'passthrough',\n",
    "    **{f'race_{r}': 'passthrough' for r in ['american_indian','asian','black','pacific','white','hispanic']},\n",
    "    'gender': 'passthrough',\n",
    "    'lgcy': 'passthrough',\n",
    "    'resd': 'passthrough',\n",
    "    'waiver': 'passthrough',\n",
    "    # 'fafsa_app': 'passthrough',\n",
    "    'schlship_app': 'passthrough',\n",
    "    # 'finaid_accepted': 'passthrough',\n",
    "    'ssb': 'passthrough',\n",
    "    'math': 'passthrough',\n",
    "    'reading': 'passthrough',\n",
    "    'writing': 'passthrough',\n",
    "    'gap_score': 'passthrough',\n",
    "    'oriented': 'passthrough',\n",
    "    'hs_qrtl': 'passthrough',\n",
    "    'act_equiv': 'passthrough',\n",
    "    'lgcy': 'passthrough',\n",
    "    # 'distance': StandardScaler()\n",
    "    # 'distance': 'passthrough'\n",
    "    'distance': make_pipeline(StandardScaler(), PowerTransformer()),\n",
    "    })\n",
    "ct = ColumnTransformer([(c,t,[\"__\"+c]) for c,t in trf.items()], remainder='drop', verbose_feature_names_out=False)\n",
    "#         # ('lgcy', 'passthrough', '__lgcy'),\n",
    "#         # ('distance', StandardScaler(), '__distance'),\n",
    "#     # ]\n",
    "str(trf)\n",
    "\n",
    "# from sklearn.compose import ColumnTransformer\n",
    "# ct = ColumnTransformer([\n",
    "#         ('lgcy', 'passthrough', '__lgcy'),\n",
    "#         ('distance', StandardScaler(), '__distance'),\n",
    "#     ], remainder='drop', verbose_feature_names_out=False)\n",
    "# str(ct)\n",
    "# def stringify(self):\n",
    "#     # def f(x):\n",
    "#     #     try:\n",
    "#     #         return str(x).split('(')[0]\n",
    "#     #     except:\n",
    "#     #         return str(x)\n",
    "#     # return ', '.join([f\"({a}, {b}, {c})\" for a,b,c in sorted(self.transformers)])\n",
    "#     return join(f\"({join(t)})\" for t in sorted(self.transformers))\n",
    "# stringify(ct)\n",
    "ct.fit_transform(self.X).columns\n",
    "# # str(ct.transformers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = mean_match_default#.copy()\n",
    "x = mean_match_shap\n",
    "x.__repr__()\n",
    "mean_match_default??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from LiveAMP import *\n",
    "from miceforest import ImputationKernel, mean_match_default, mean_match_fast_cat, mean_match_shap\n",
    "# from sklearn.base import clone, BaseEstimator\n",
    "# from sklearn.compose import make_column_selector, ColumnTransformer\n",
    "# from sklearn.preprocessing import OneHotEncoder, StandardScaler, PowerTransformer\n",
    "# from sklearn.experimental import enable_iterative_imputer\n",
    "# from sklearn.impute import IterativeImputer\n",
    "# from sklearn.linear_model import BayesianRidge, Ridge\n",
    "# from sklearn.kernel_approximation import Nystroem\n",
    "# from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, GradientBoostingClassifier, GradientBoostingRegressor, HistGradientBoostingClassifier, HistGradientBoostingRegressor\n",
    "# from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn import set_config\n",
    "set_config(transform_output=\"pandas\")\n",
    "\n",
    "def feature_importance_df(self, dataset=0, iteration=None, normalize=True):\n",
    "    targ = [self._get_var_name_from_scalar(int(i)) for i in np.sort(self.imputation_order)]\n",
    "    feat = [self._get_var_name_from_scalar(int(i)) for i in np.sort(self.predictor_vars)]\n",
    "    I = pd.DataFrame(self.get_feature_importance(dataset, iteration), index=targ, columns=feat).T\n",
    "    return I / I.sum() * 100 if normalize else I\n",
    "ImputationKernel.feature_importance_df = feature_importance_df\n",
    "\n",
    "def inspect(self, **kwargs):\n",
    "    self.plot_imputed_distributions(wspace=0.3,hspace=0.3)\n",
    "    self.plot_mean_convergence(wspace=0.3, hspace=0.4)\n",
    "    I = self.feature_importance_df(**kwargs)\n",
    "    I.disp(100)\n",
    "    return I\n",
    "ImputationKernel.inspect = inspect\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class AMP(MyBaseClass):\n",
    "    cycle_day: int\n",
    "    term_codes: typing.List\n",
    "    infer_term: int\n",
    "    crse: typing.List\n",
    "    attr: typing.List\n",
    "    # feat: typing.Dict\n",
    "    fill: typing.Dict = None\n",
    "    # trf: typing.Dict = None\n",
    "    overwrite: typing.Dict = None\n",
    "    show: typing.Dict = None\n",
    "\n",
    "    def dump(self):\n",
    "        return write(self.rslt, self, overwrite=True)\n",
    "\n",
    "    def __post_init__(self):\n",
    "        D = {'adm':False, 'reg':False, 'flg':False, 'raw':False, 'term':False, 'X':False, 'Y':False, 'Z':False, 'pred':False}\n",
    "        for x in ['overwrite','show']:\n",
    "            self[x] = D.copy() if self[x] is None else D.copy() | self[x]\n",
    "        self.overwrite['Z'] |= self.overwrite['X'] | self.overwrite['Y']\n",
    "        self.overwrite['raw'] |= self.overwrite['reg'] | self.overwrite['adm'] | self.overwrite['flg']\n",
    "        self.overwrite['term'] |= self.overwrite['raw']\n",
    "        self.path = root_path / f\"resources/rslt/{rjust(self.cycle_day,3,0)}\"\n",
    "        self.rslt = self.path / f\"rslt.pkl\"\n",
    "        self.tune = self.path / f\"tune.pkl\"\n",
    "        try:\n",
    "            self.__dict__ = read(self.rslt).__dict__ | self.__dict__\n",
    "        except:\n",
    "            pass\n",
    "        for k, v in self.overwrite.items():\n",
    "            if v and k in self:\n",
    "                del self[k]\n",
    "\n",
    "        for k in ['term','pred','imputed']:\n",
    "            self[k] = self[k] if k in self else dict()\n",
    "\n",
    "        self.term_codes = uniquify([*listify(self.term_codes), self.infer_term])\n",
    "        self.crse = uniquify(['_total', *listify(self.crse)])\n",
    "        self.mlt_grp = ['crse','levl_code','styp_code','term_code']\n",
    "        \n",
    "\n",
    "        # def g(X):\n",
    "        #     X = [listify(x) for x in X]\n",
    "        #     return {x.pop(0): x if x else [np.nan] for x in X}\n",
    "        # L = [g(self.attr), g(self.feat)]\n",
    "        # self.imp = L[0] | L[1]\n",
    "        # self.attr, self.feat = [list(x.keys()) for x in L]\n",
    "\n",
    "        opts = {x:self[x] for x in ['cycle_day','overwrite','show']}\n",
    "        for nm in self.term_codes:\n",
    "            if nm not in self.term:\n",
    "                print(f'get {nm}')\n",
    "                self.term[nm] = TERM(term_code=nm, **opts).get_raw()\n",
    "        return self.dump()\n",
    "\n",
    "    def preprocess(self):\n",
    "        def get(nm):\n",
    "            if nm in self:\n",
    "                return False\n",
    "            print(f'get {nm}')\n",
    "            return True\n",
    "\n",
    "        if get('raw_df'):\n",
    "            self.raw_df = pd.concat([term.raw for term in self.term.values()], ignore_index=True).dropna(axis=1, how='all').prep()\n",
    "        \n",
    "        where = lambda x: x.query(\"levl_code == 'ug' and styp_code in ('n','r','t')\").copy()\n",
    "        \n",
    "        if get('X'):\n",
    "            R = self.raw_df.copy()\n",
    "            repl = {'ae':0, 'n1':1, 'n2':2, 'n3':3, 'n4':4, 'r1':1, 'r2':2, 'r3':3, 'r4':4}\n",
    "            R['hs_qrtl'] = pd.cut(R['hs_pctl'], bins=[-1,25,50,75,90,101], labels=[4,3,2,1,0], right=False).combine_first(R['apdc_code'].map(repl))\n",
    "            R['remote'] = R['camp_code'] != 's'\n",
    "            R['resd'] = R['resd_code'] == 'r'\n",
    "            R['lgcy'] = ~R['lgcy_code'].isin(['n','o'])\n",
    "            R['majr_code'] = R['majr_code'].replace({'0000':'und', 'eled':'eted', 'agri':'unda'})\n",
    "            R['coll_code'] = R['coll_code'].replace({'ae':'an', 'eh':'ed', 'hs':'hl', 'st':'sm', '00':pd.NA})\n",
    "            R['coll_desc'] = R['coll_desc'].replace({\n",
    "                'ag & environmental sciences':'ag & natural resources',\n",
    "                'education & human development':'education',\n",
    "                'health science & human service':'health sciences',\n",
    "                'science & technology':'science & mathematics'})\n",
    "            majr = ['majr_desc','dept_code','dept_desc','coll_code','coll_desc']\n",
    "            S = R.sort_values('cycle_date').drop_duplicates(subset='majr_code', keep='last')[['majr_code',*majr]]\n",
    "            X = where(R.drop(columns=majr).merge(S, on='majr_code', how='left')).prep().binarize()\n",
    "\n",
    "            checks = [\n",
    "                'cycle_day >= 0',\n",
    "                'apdc_day >= cycle_day',\n",
    "                'appl_day >= apdc_day',\n",
    "                'birth_day >= appl_day',\n",
    "                'birth_day >= 5000',\n",
    "                'distance >= 0',\n",
    "                'hs_pctl >=0',\n",
    "                'hs_pctl <= 100',\n",
    "                'hs_qrtl >= 0',\n",
    "                'hs_qrtl <= 4',\n",
    "                'act_equiv >= 1',\n",
    "                'act_equiv <= 36',\n",
    "                'gap_score >= 0',\n",
    "                'gap_score <= 100',\n",
    "            ]\n",
    "            for check in checks:\n",
    "                mask = X.eval(check)\n",
    "                assert mask.all(), [check,X[~mask].disp(5)]\n",
    "\n",
    "            for k, v in self.feat.items():\n",
    "                if 'fill' in v:\n",
    "                    X[k] = X.impute(k, *v['fill'])\n",
    "\n",
    "            # ren = lambda x:'__'+x\n",
    "            self.X = X.prep().binarize().set_index(self.attr, drop=False).rename(columns=lambda x:'__'+x)\n",
    "            # self.feat = {ren(k): v for k, v in self.feat.items()}\n",
    "            # for k, v in self.feat.items():\n",
    "            #     if 'fill' in v:\n",
    "            #         X[k] = X.impute(k, *v['fill'])\n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "            # self.feat = {ren(k): v for k, v in self.feat.items()}\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "            # for k, v in self.trf.items():\n",
    "            #     X[k] = instantiate(v).fit_transform(X[[k]]).squeeze()\n",
    "            \n",
    "            # for k, v in self.imp.items():\n",
    "            #     X[k] = X.impute(k, *v)\n",
    "\n",
    "            # self.X = (X\n",
    "            #     .prep()\n",
    "            #     .binarize()\n",
    "            #     # .reset_index()\n",
    "            #     # .rename(columns={'index':'idx'})\n",
    "            #     # .set_index(['idx',*self.attr], drop=False)\n",
    "            #     .set_index(self.attr, drop=False)\n",
    "            #     .rename(columns=feat_renamer)\n",
    "            #     [self.feat.keys()]\n",
    "                \n",
    "            # )\n",
    "\n",
    "            # .categorize()\n",
    "\n",
    "            self.X.missing().disp(100)\n",
    "\n",
    "        if get('reg_df'):\n",
    "            with warnings.catch_warnings(action='ignore'):\n",
    "                self.reg_df = {k: pd.concat([term.reg[k].query(\"crse in @self.crse\") for term in self.term.values()]) for k in ['cur','end']}\n",
    "        \n",
    "        if get('Y'):\n",
    "            print(self.X.shape)\n",
    "            self.Y = {k: self.X[[]].join(y.set_index(['pidm','term_code','crse'])['credit_hr']) for k, y in self.reg_df.items()}\n",
    "            print(self.Y['end'].shape)\n",
    "            print(self.Y['cur'].shape)\n",
    "            agg = lambda y: where(y).groupby(self.mlt_grp)['credit_hr'].agg(lambda x: (x>0).sum())\n",
    "            A = agg(self.reg_df['end'])\n",
    "            B = agg(self.Y['end'])\n",
    "            M = (A / B).replace(np.inf, pd.NA).rename('mlt').reset_index().query(f\"term_code != {self.infer_term}\").prep()\n",
    "            N = M.assign(term_code=self.infer_term)\n",
    "            self.mlt = pd.concat([M, N], axis=0).set_index(self.mlt_grp)\n",
    "            Y = {k: y.squeeze().unstack().dropna(how='all', axis=1).fillna(0) for k, y in self.Y.items()}\n",
    "            self.Y = Y['cur'].rename(columns=lambda x:x+'_cur').join(Y['end']>0).prep()\n",
    "            print(self.Y.shape)\n",
    "            self.Y.missing().disp(100)\n",
    "\n",
    "        # if get('Z'):\n",
    "        #     self.Z = self.X.join(self.Y['cur'].rename(columns=lambda x:x+'_cur')).join(self.Y['end']>0).prep()\n",
    "        return self.dump()\n",
    "\n",
    "   \n",
    "    def predict(self, params, crse, train_term, styp_code='all'):\n",
    "        for step in ['imp','enc','clf']:\n",
    "            params.setdefault(step, dict())\n",
    "        for p, P in self.pred.items():\n",
    "            if p == stringify(params):\n",
    "                for c, C in P.items():\n",
    "                    if c == crse:\n",
    "                        for t, T in C.items():\n",
    "                            if t == train_term:\n",
    "                                for s, S in T.items():\n",
    "                                    if s == styp_code:\n",
    "                                        # print('reusing')\n",
    "                                        return S\n",
    "\n",
    "        X = self.X.copy()\n",
    "        if styp_code != 'all':\n",
    "            X = X.query(f\"styp_code==@styp_code\")\n",
    "\n",
    "        imp_str = stringify({'imp':params['imp']})\n",
    "        X_imputed = None\n",
    "        for i, I in self.imputed.items():\n",
    "            if i == imp_str:\n",
    "                for s, S in I.items():\n",
    "                    if s == styp_code:\n",
    "                        # print('reusing imputed data')\n",
    "                        X_imputed = S\n",
    "\n",
    "        if X_imputed is None:\n",
    "            print('imputing')\n",
    "            p = params['imp'].copy()\n",
    "            # p = params.pop('imp')\n",
    "            iterations = p[1].pop('iterations') if 'iterations' in p[1] else 5\n",
    "            print(iterations, p[1])\n",
    "            imp = p[0](X, **p[1])\n",
    "            imp.mice(iterations)\n",
    "            X_imputed = [imp.complete_data(k) for k in range(imp.dataset_count())]\n",
    "            self.imputed.setdefault(imp_str, dict()).update({styp_code: X_imputed})\n",
    "            self.dump()\n",
    "            imp.inspect()\n",
    "\n",
    "        print('creating', ljust(crse,8), train_term, styp_code)#, stringify(params))\n",
    "        cols = uniquify(['_total_cur',crse+'_cur',crse])\n",
    "        S = dict()\n",
    "        for k, X in enumerate(X_imputed):\n",
    "            Z = X.join(self.Y[cols]).sort_index()\n",
    "            Z.missing().disp(100)\n",
    "            c = params['clf'].copy()\n",
    "            if c[0].__name__ != 'ImputationKernel':\n",
    "                enc = ColumnTransformer([('cat', OneHotEncoder(sparse_output=False, handle_unknown='infrequent_if_exist', **params['enc'][1]), make_column_selector(dtype_include=['string','category']))], remainder='passthrough', verbose_feature_names_out=False)\n",
    "                T = enc.fit_transform(Z)\n",
    "                T.missing().disp(100)\n",
    "                X = {'test': T, 'train': T.query(f\"term_code==@train_term\")}\n",
    "                y = {k: x.pop(crse) for k, x in X.items()}\n",
    "                model = c[0](**c[1]).fit(X['train'], y['train'])\n",
    "                pred = model.predict(X['test'])\n",
    "            else:\n",
    "                iterations = c[1].pop('iterations') if 'iterations' in c[1] else 5\n",
    "                T = Z.copy()\n",
    "                T.loc[T.eval(\"term_code!=@train_term\"), crse] = pd.NA\n",
    "                model = c[0](T, datasets=1, **c[1])\n",
    "                model.mice(1)\n",
    "                optimal_parameters, losses = model.tune_parameters(dataset=0, optimization_steps=5)\n",
    "                model.mice(iterations, variable_parameters=optimal_parameters)\n",
    "                model.inspect()\n",
    "                T.loc[:, crse] = pd.NA\n",
    "                pred = model.impute_new_data(T).complete_data(0).pop(crse)\n",
    "                \n",
    "            details = (\n",
    "                Z[crse]\n",
    "                .rename('true')\n",
    "                .to_frame()\n",
    "                .assign(pred=pred, train_term=train_term, crse=crse, sim=k)\n",
    "                .set_index(['train_term','crse','sim'], append=True)\n",
    "                .binarize()\n",
    "            )\n",
    "            agg = lambda x: pd.Series({\n",
    "                'pred': x['pred'].sum(min_count=1),\n",
    "                'true': x['true'].sum(min_count=1),\n",
    "                'mse_pct': ((1*x['pred'] - x['true'])**2).mean()*100,\n",
    "                'f1_inv_pct': (1-f1_score(x.dropna()['true'], x.dropna()['pred'], zero_division=np.nan))*100,\n",
    "            })\n",
    "            summary = details.groupby([*self.mlt_grp,'train_term','sim']).apply(agg).join(self.mlt).rename_axis(index={'term_code':'pred_term'})\n",
    "            for x in ['pred','true']:\n",
    "                summary[x] = summary[x] * summary['mlt']\n",
    "            summary.insert(2, 'err', summary['pred'] - summary['true'])\n",
    "            summary.insert(3, 'err_pct', (summary['err'] / summary['true']).clip(-1, 1) * 100)\n",
    "            S[k] = {'details':details.copy(), 'summary':summary.copy().drop(columns='mlt').prep(), 'model':model}\n",
    "        return S\n",
    "\n",
    "    def analyze(self):\n",
    "        def pivot(df, val):\n",
    "            Y = (\n",
    "                df\n",
    "                .reset_index()\n",
    "                .pivot_table(columns='train_term', index=['crse','styp_code','pred_term'], values=val, aggfunc=[pctl(0),pctl(25),pctl(50),pctl(75),pctl(100)])\n",
    "                .rename_axis(columns=[val,'train_term'])\n",
    "                .stack(0)\n",
    "                .assign(abs_mean = lambda x: x.abs().mean(axis=1))\n",
    "            )\n",
    "            return Y\n",
    "\n",
    "        for k, v in self.pred.items():\n",
    "            print(k)\n",
    "            df = v['summary']\n",
    "            mask = df.eval(f\"pred_term!={self.infer_term}\")\n",
    "            v['rslt'] = {stat: pivot(df[mask], stat) for stat in [\"pred\",\"err\",\"err_pct\",\"mse_pct\",\"f1_inv_pct\"]} | {'pred': pivot(df[~mask], \"pred\")}\n",
    "            v['rslt']['err_pct'].query(\"err_pct==' 50%'\").disp(200)\n",
    "        return self.dump()\n",
    "\n",
    "\n",
    "    def main(self, params_list):\n",
    "        g = lambda Y: Y | {k: pd.concat([y[k] for y in Y.values() if k in y.keys()]).sort_index() for k in ['details','summary']}\n",
    "        for params in params_list:\n",
    "            M = dict()\n",
    "            print(stringify(params))\n",
    "            for crse in self.crse:\n",
    "                C = dict()\n",
    "                for train_term in self.term_codes:\n",
    "                    T = dict()\n",
    "                    for styp_code in ['n']:#,'r','t']:\n",
    "                        S = self.predict(params, crse, train_term, styp_code)\n",
    "                        T[styp_code] = g(S)\n",
    "                    C[train_term] = g(T)\n",
    "                M[crse] = g(C)\n",
    "            self.pred[stringify(params)] = g(M) | {'params':params}\n",
    "            self.dump()\n",
    "        return self.analyze()\n",
    "\n",
    "\n",
    "code_desc = lambda x: [x+'_code', x+'_desc']\n",
    "kwargs = {\n",
    "    'term_codes': np.arange(2020,2025)*100+8,\n",
    "    'infer_term': 202408,\n",
    "    'show': {\n",
    "        # 'reg':True,\n",
    "        # 'adm':True,\n",
    "    },\n",
    "    'attr': [\n",
    "        # 'index',\n",
    "        'pidm',\n",
    "        *code_desc('term'),\n",
    "        *code_desc('apdc'),\n",
    "        *code_desc('levl'),\n",
    "        *code_desc('styp'),\n",
    "        *code_desc('admt'),\n",
    "        *code_desc('camp'),\n",
    "        *code_desc('coll'),\n",
    "        *code_desc('dept'),\n",
    "        *code_desc('majr'),\n",
    "        *code_desc('cnty'),\n",
    "        *code_desc('stat'),\n",
    "        *code_desc('natn'),\n",
    "        *code_desc('resd'),\n",
    "        *code_desc('lgcy'),\n",
    "        'international',\n",
    "        'gender',\n",
    "        *[f'race_{r}' for r in ['american_indian','asian','black','pacific','white','hispanic']],\n",
    "        'waiver',\n",
    "        'birth_day',\n",
    "        'distance',\n",
    "        'hs_qrtl',\n",
    "    ],\n",
    "    'feat': [\n",
    "        # {'term_code: {}}',\n",
    "        {'appl_day': {}},\n",
    "        {'apdc_day': {}},\n",
    "        {'birth_day': {'fill': ['median',['term_code','styp_code']]}},\n",
    "        # {'levl_code': {}},\n",
    "        # {'styp_code': {}},\n",
    "        # {'admt_code': {}},\n",
    "        # {'camp_code': {}},\n",
    "        {'remote', {'fill': False}},\n",
    "        {'coll_code': {}},\n",
    "        {'international': {'fill': False}},\n",
    "        *[{f'race_{r}': {'fill': False}} for r in ['american_indian','asian','black','pacific','white','hispanic']],\n",
    "        {'gender': {}},\n",
    "        {'lgcy': {'fill': False}},\n",
    "        {'resd': {'fill': False}},\n",
    "        {'waiver': {'fill': False}},\n",
    "        # {'fafsa_app': {'fill': False}},\n",
    "        {'schlship_app': {'fill': False}},\n",
    "        # {'finaid_accepted': {'fill': False}},\n",
    "        {'ssb': {'fill': False}},\n",
    "        {'math': {'fill': False}},\n",
    "        {'reading': {'fill': False}},\n",
    "        {'writing': {'fill': False}},\n",
    "        {'gap_score': {'fill': 0}},\n",
    "        {'oriented': {'fill': 'n'}},\n",
    "        {'distance': {}},\n",
    "        {'hs_qrtl': {}},\n",
    "        {'act_equiv': {}},\n",
    "    ],\n",
    "    'trf': {\n",
    "        'birth_day': make_pipeline(StandardScaler(), PowerTransformer()),\n",
    "        'distance': make_pipeline(StandardScaler(), PowerTransformer()),\n",
    "    },\n",
    "    # 'cycle_day': (TERM(term_code=202408).cycle_date-pd.Timestamp.now()).days+1,\n",
    "    'cycle_day': 187,\n",
    "    'crse': [\n",
    "        # 'engl1301',\n",
    "        # 'biol1406',\n",
    "        # 'biol2401',\n",
    "        # 'math1314',\n",
    "        # 'math2412',\n",
    "        # 'agri1419',\n",
    "        # 'psyc2301',\n",
    "        # 'ansc1319',\n",
    "        # 'comm1311',\n",
    "        # 'hist1301',\n",
    "        # 'govt2306',\n",
    "        # 'math1324',\n",
    "        # 'chem1411',\n",
    "        # 'univ0301',\n",
    "        # 'univ0204',\n",
    "        # 'univ0304',\n",
    "        # 'agri1100',\n",
    "        # 'comm1315',\n",
    "        # 'agec2317',\n",
    "        # 'govt2305',\n",
    "        # 'busi1301',\n",
    "        # 'arts1301',\n",
    "        # 'math1342',\n",
    "        # 'math2413',\n",
    "        ],\n",
    "    'overwrite': {\n",
    "        # 'reg':True,\n",
    "        # 'adm':True,\n",
    "        # 'flg':True,\n",
    "        # 'raw':True,\n",
    "        # 'term': True,\n",
    "        'raw_df': True,\n",
    "        'reg_df': True,\n",
    "        'X': True,\n",
    "        'Y': True,\n",
    "        'Z': True,\n",
    "        # 'imputed':True,\n",
    "        'pred': True,\n",
    "    },\n",
    "}\n",
    "# FLAGS().run()\n",
    "self = AMP(**kwargs)\n",
    "self = self.preprocess()\n",
    "self.term_codes.remove(self.infer_term)\n",
    "\n",
    "def mmc(k):\n",
    "    m = mean_match_default.copy()\n",
    "    m.set_mean_match_candidates(k)\n",
    "    return m\n",
    "MMC = [mmc(k) for k in range(0,10,2)]\n",
    "\n",
    "params_grid = {\n",
    "    'enc': {\n",
    "        OneHotEncoder: {\n",
    "            'min_frequency': [0.1],\n",
    "        },\n",
    "    },\n",
    "    'imp': {\n",
    "        ImputationKernel: {\n",
    "            'datasets': 5,\n",
    "            # 'save_all_iterations': False,\n",
    "            # 'mean_match_candidates': [29],\n",
    "            # 'mean_match_function': mean_match_kdtree_classification,\n",
    "            'mean_match_scheme': [mmc(25)],\n",
    "            'iterations':4,\n",
    "        },\n",
    "    },\n",
    "    'clf': {\n",
    "        # RandomForestClassifier: {\n",
    "        #     'max_depth': 3,\n",
    "        # },\n",
    "        HistGradientBoostingClassifier: {\n",
    "            'max_depth': 3,\n",
    "        },\n",
    "        # ImputationKernel: {\n",
    "        #     'save_all_iterations': False,\n",
    "        #     # 'mean_match_candidates': [29],\n",
    "        #     # 'mean_match_function': mean_match_kdtree_classification,\n",
    "        #     'iterations':3,\n",
    "        # }\n",
    "    },\n",
    "}\n",
    "params_list = cartesian({step: [[alg, values] for alg, grid in D.items() for values in cartesian(grid)] for step, D in params_grid.items()})\n",
    "def stringify(P):\n",
    "    def f(x):\n",
    "        try:\n",
    "            return x.__name__\n",
    "        except:\n",
    "            return x\n",
    "    return str({f(step): [f(D[0]), {f(k): f(v) for k, v in D[1].items()}] for step, D in P.items()})\n",
    "\n",
    "# self.main(params_list);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.X.join(self.Y).missing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self = imp\n",
    "targ = [self._get_var_name_from_scalar(int(i)) for i in np.sort(self.imputation_order)]\n",
    "feat = [self._get_var_name_from_scalar(int(i)) for i in np.sort(self.predictor_vars)]\n",
    "I = pd.DataFrame(self.get_feature_importance(dataset=0), index=targ, columns=feat).T\n",
    "I = I / I.sum() * 100# if normalize else I\n",
    "I.sort_values('__act_equiv', ascending=False)\n",
    "# (dataset=0, annot=True,cmap=\"YlGnBu\",vmin=0, vmax=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mmc(k):\n",
    "    m = mean_match_default.copy()\n",
    "    m.set_mean_match_candidates(k)\n",
    "    return m\n",
    "MMC = [mmc(k) for k in range(10)]\n",
    "# mmc = []\n",
    "# for k in range(10):\n",
    "#     m = mean_match_default.copy()\n",
    "#     m.set_mean_match_candidates(k)\n",
    "#     mmc.append(m)\n",
    "# mmc[2].mean_match_candidates\n",
    "    \n",
    "    \n",
    "\n",
    "# scheme_mmc_0.set_mean_match_candidates(0)\n",
    "# scheme_mmc_5.set_mean_match_candidates(5)\n",
    "# scheme_mmc_5.mean_match_candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# self.X['__coll_code'].replace('00',pd.NA)\n",
    "self.X.vc('__coll_code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import miceforest as mf\n",
    "from sklearn.datasets import load_iris\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load data and introduce missing values\n",
    "iris = pd.concat(load_iris(as_frame=True,return_X_y=True),axis=1)\n",
    "iris.rename({\"target\": \"species\"}, inplace=True, axis=1)\n",
    "iris['species'] = iris['species'].astype('category')\n",
    "iris_amp = mf.ampute_data(iris,perc=0.25,random_state=1991)\n",
    "\n",
    "# Create kernel. \n",
    "kernel = mf.ImputationKernel(\n",
    "  iris_amp,\n",
    "  datasets=4,\n",
    "  save_all_iterations=True,\n",
    "  random_state=1\n",
    ")\n",
    "\n",
    "# Run the MICE algorithm for 2 iterations on each of the datasets\n",
    "kernel.mice(2)\n",
    "\n",
    "# Printing the kernel will show you some high level information.\n",
    "print(kernel)\n",
    "\n",
    "# Using the first ImputationKernel in kernel to tune parameters\n",
    "# with the default settings.\n",
    "optimal_parameters, losses = kernel.tune_parameters(\n",
    "  dataset=0,\n",
    "  optimization_step=5,\n",
    "  hi=6\n",
    ")\n",
    "\n",
    "# Run mice with our newly tuned parameters.\n",
    "kernel.mice(1, variable_parameters=optimal_parameters)\n",
    "\n",
    "# The optimal parameters are kept in ImputationKernel.optimal_parameters:\n",
    "print(optimal_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel.tune_parameters??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using a complicated setup:\n",
    "optimal_parameters, losses = kernel.tune_parameters(\n",
    "  dataset=0,\n",
    "  variables = ['sepal width (cm)','species','petal width (cm)'],\n",
    "  variable_parameters = {\n",
    "    'sepal width (cm)': {'bagging_fraction': 0.5},\n",
    "    'species': {'bagging_freq': (5,10)}\n",
    "  },\n",
    "  optimization_steps=5,\n",
    "  extra_trees = [True, False]\n",
    ")\n",
    "\n",
    "kernel.mice(1, variable_parameters=optimal_parameters)\n",
    "print(optimal_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import miceforest as mf\n",
    "from sklearn.datasets import load_iris\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load data and introduce missing values\n",
    "iris = pd.concat(load_iris(as_frame=True,return_X_y=True),axis=1)\n",
    "iris.rename({\"target\": \"species\"}, inplace=True, axis=1)\n",
    "iris['species'] = iris['species'].astype('category')\n",
    "iris_amp = mf.ampute_data(iris,perc=0.25,random_state=1991)\n",
    "\n",
    "# Create kernel. \n",
    "kds = mf.ImputationKernel(\n",
    "  iris_amp,\n",
    "  save_all_iterations=True,\n",
    "  random_state=1991\n",
    ")\n",
    "\n",
    "# Run the MICE algorithm for 2 iterations\n",
    "kds.mice(2)\n",
    "\n",
    "# Return the completed dataset.\n",
    "iris_complete = kds.complete_data(0)\n",
    "\n",
    "# Create kernel. \n",
    "kernel = mf.ImputationKernel(\n",
    "  iris_amp,\n",
    "  datasets=4,\n",
    "  save_all_iterations=True,\n",
    "  random_state=1\n",
    ")\n",
    "\n",
    "# Run the MICE algorithm for 2 iterations on each of the datasets\n",
    "kernel.mice(2)\n",
    "\n",
    "# Printing the kernel will show you some high level information.\n",
    "print(kernel)\n",
    "\n",
    "completed_dataset = kernel.complete_data(dataset=2)\n",
    "print(completed_dataset.isnull().sum(0))\n",
    "\n",
    "# Run the MICE algorithm for 1 more iteration on the kernel with new parameters\n",
    "kernel.mice(iterations=1,n_estimators=50)\n",
    "\n",
    "# Run the MICE algorithm for 2 more iterations on the kernel \n",
    "kernel.mice(\n",
    "  iterations=1,\n",
    "  variable_parameters={'species': {'n_estimators': 25}},\n",
    "  n_estimators=50\n",
    ")\n",
    "\n",
    "# Let's get the actual models for these variables:\n",
    "species_model = kernel.get_model(dataset=0,variable=\"species\")\n",
    "sepalwidth_model = kernel.get_model(dataset=0,variable=\"sepal width (cm)\")\n",
    "\n",
    "print(\n",
    "f\"\"\"Species used {str(species_model.params[\"num_iterations\"])} iterations\n",
    "Sepal Width used {str(sepalwidth_model.params[\"num_iterations\"])} iterations\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# Create kernel. \n",
    "cust_kernel = mf.ImputationKernel(\n",
    "  iris_amp,\n",
    "  datasets=1,\n",
    "  random_state=1\n",
    ")\n",
    "\n",
    "cust_kernel.mice(\n",
    "  iterations=1, \n",
    "  variable_parameters={'sepal width (cm)': {'objective': 'poisson'}},\n",
    "  boosting = 'gbdt',\n",
    "  min_sum_hessian_in_leaf=0.01\n",
    ")\n",
    "\n",
    "# Our 'new data' is just the first 15 rows of iris_amp\n",
    "from datetime import datetime\n",
    "\n",
    "# Define our new data as the first 15 rows\n",
    "new_data = iris_amp.iloc[range(15)]\n",
    "\n",
    "# Imputing new data can often be made faster by \n",
    "# first compiling candidate predictions\n",
    "kernel.compile_candidate_preds()\n",
    "\n",
    "start_t = datetime.now()\n",
    "new_data_imputed = kernel.impute_new_data(new_data=new_data)\n",
    "print(f\"New Data imputed in {(datetime.now() - start_t).total_seconds()} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!pip install git+https://github.com/AnotherSamWilson/miceforest.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.X['__birth_day'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in self.X.select_dtypes('number').items():\n",
    "    v.plot(kind='kde', title=k)\n",
    "    plt.show()\n",
    "    # print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trf = PowerTransformer()\n",
    "self.X['__birth_day'] = trf.fit_transform(self.X[['__birth_day']]).squeeze()\n",
    "self.X['__birth_day'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S[0].keys()\n",
    "g = lambda Y: Y | {k: pd.concat([y[k] for y in Y.values() if k in y.keys()]).sort_index() for k in ['details','summary']}\n",
    "g(S)\n",
    "list(S.values())[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_importance_df(self, dataset=0, normalize=True, iteration=None):\n",
    "    predictor_var_names = [self._get_variable_name(i) for i in sorted(self.predictor_vars)]\n",
    "    imputed_var_names   = [self._get_variable_name(i) for i in sorted(self.imputation_order)]\n",
    "    I = pd.DataFrame(self.get_feature_importance(dataset, iteration), index=imputed_var_names, columns=predictor_var_names).T\n",
    "    return I / I.sum() * 100 if normalize else I\n",
    "ImputationKernel.feature_importance_df = feature_importance_df\n",
    "for k, v in self.pred.items():\n",
    "    pass\n",
    "m = v['_total'][202008]['n'][0]['model']\n",
    "m.get_feature_importance(0)\n",
    "feature_importance_df(m, 0).squeeze().sort_values()\n",
    "#.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install\n",
    "pip install git+https://github.com/AnotherSamWilson/miceforest.git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crse = '_total'\n",
    "cols = uniquify(['_total_cur',crse+'_cur',crse])\n",
    "X_imputed = list(self.imputed.values())[0]['n'][:1]\n",
    "for k, X in enumerate(X_imputed):\n",
    "    Z = X.join(self.Y[cols])\n",
    "    Z.iloc[0,-1] = pd.NA\n",
    "    imp = ImputationKernel(Z, train_nonmissing=True)\n",
    "Z.disp(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ImputationKernel?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_list[0]['imp'][0].__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(self.pred.values())[0]['rslt']['err_pct'].query(\"err_pct==' 50%'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crse = '_total'\n",
    "styp_code = 'n'\n",
    "clf = RandomForestClassifier()\n",
    "\n",
    "for k, v in self.imputed.items():\n",
    "    X_imputed = v[styp_code]\n",
    "    S = dict()\n",
    "    for k, X in enumerate(X_imputed):\n",
    "        if params['imp'][0].__name__:\n",
    "            \n",
    "\n",
    "        Z = clone(enc).fit_transform(X).join(self.Y[cols])\n",
    "        X = {'test': Z, 'train': Z.query(f\"term_code==@train_term\")}\n",
    "        y = {k: x.pop(crse) for k, x in X.items()}\n",
    "        model = clone(clf).fit(X['train'], y['train'])\n",
    "        details = (\n",
    "            y['test']\n",
    "            .rename('true')\n",
    "            .to_frame()\n",
    "            .assign(pred=model.predict(X['test']), train_term=train_term, crse=crse, sim=k)\n",
    "            .set_index(['train_term','crse','sim'], append=True)\n",
    "            .binarize()\n",
    "        )\n",
    "        agg = lambda x: pd.Series({\n",
    "            'pred': x['pred'].sum(min_count=1),\n",
    "            'true': x['true'].sum(min_count=1),\n",
    "            'mse_pct': ((1*x['pred'] - x['true'])**2).mean()*100,\n",
    "            'f1_inv_pct': (1-f1_score(x.dropna()['true'], x.dropna()['pred'], zero_division=np.nan))*100,\n",
    "        })\n",
    "        summary = details.groupby([*self.mlt_grp,'train_term','sim']).apply(agg).join(self.mlt).rename_axis(index={'term_code':'pred_term'})\n",
    "        for x in ['pred','true']:\n",
    "            summary[x] = summary[x] * summary['mlt']\n",
    "        summary.insert(2, 'err', summary['pred'] - summary['true'])\n",
    "        summary.insert(3, 'err_pct', (summary['err'] / summary['true']).clip(-1, 1) * 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = dict()\n",
    "a.setdefault('b',dict()).update({'c':4})\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ImputationKernel?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = {'imp':params_list[0]['imp']}\n",
    "p\n",
    "stringify = lambda P: str({f(step): [f(D[0]), {f(k): f(v) for k, v in D[1].items()}] for step, D in P.items()})\n",
    "stringify(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in self.pred.items():\n",
    "    print(k)\n",
    "    v['rslt']['err%'].query(\"err% == 0\").disp(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(self.pred.values())[0]['rslt']['err%']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# P = params_list[0]\n",
    "# q = P['enc'].values()\n",
    "# dict(q)\n",
    "# dir(q)\n",
    "# q.mapping()\n",
    "list(P['enc'].values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_list = cartesian({step: [[alg, values] for alg, grid in D.items() for values in cartesian(grid)] for step, D in params_grid.items()})\n",
    "def f(x):\n",
    "    try:\n",
    "        return x.__name__\n",
    "    except:\n",
    "        return x\n",
    "stringify = lambda P: {f(step): [f(k): f(v)  for k, v in D.items()} for step, D in P.items()}\n",
    "# stringify(param_list[0])\n",
    "P = params_list[0]\n",
    "P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_list = cartesian({step: [{alg: values} for alg, grid in D.items() for values in cartesian(grid)] for step, D in params_grid.items()})\n",
    "def f(x):\n",
    "    try:\n",
    "        return x.__name__\n",
    "    except:\n",
    "        return x\n",
    "param_list[0]\n",
    "stringify = lambda P: {f(step): {f(alg): {f(k): f(v) for k, v in H.items()} for alg, H in D.items()} for step, D in P.items()}\n",
    "stringify(param_list[0])\n",
    "    \n",
    "# param_list\n",
    "# S = [{f(step): {f(k): f(v)  for k, v in D.items()}  for step, D in P.items()} for P in param_list]\n",
    "# S\n",
    "# str(S[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_list = cartesian({step: [{'alg':alg}|values for alg, grid in D.items() for values in cartesian(grid)] for step, D in params_grid.items()})\n",
    "def f(x):\n",
    "    try:\n",
    "        return x.__name__\n",
    "    except:\n",
    "        return x\n",
    "stringify = lambda P: {f(step): {f(k): f(v)  for k, v in D.items()} for step, D in P.items()}\n",
    "stringify(param_list[0])\n",
    "    \n",
    "# param_list\n",
    "# S = [{f(step): {f(k): f(v)  for k, v in D.items()}  for step, D in P.items()} for P in param_list]\n",
    "# S\n",
    "# str(S[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import Collection, Mapping\n",
    "\n",
    "# def recursive_map(data, func):\n",
    "#     apply = lambda x: recursive_map(x, func)\n",
    "#     if isinstance(data, Mapping):\n",
    "#         return type(data)({k: apply(v) for k, v in data.items()})\n",
    "#     elif isinstance(data, Collection):\n",
    "#         return type(data)(apply(v) for v in data)\n",
    "#     else:\n",
    "#         return func(data)\n",
    "    \n",
    "# def func(x):\n",
    "#     try:\n",
    "#         return x.__name__\n",
    "#     except:\n",
    "#         return x\n",
    "    \n",
    "# func(mean_match_kdtree_classification)\n",
    "mean_match_kdtree_classification.__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuple(sorted(params_list))\n",
    "# str(params_list[0])\n",
    "# params_list[0]\n",
    "def f(x):\n",
    "    try:\n",
    "        return x.__name__\n",
    "    except:\n",
    "        return x\n",
    "\n",
    "# [{f(k):v} for k,v in a.items() for a in params_list]\n",
    "\n",
    "# {f(a): {f(b):} for a, A in }\n",
    "# cartesian({step: [[alg, values] for alg, grid in D.items() for values in cartesian(grid)] for step, D in params_grid.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0x7f91603f5300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 'clf'\n",
    "G = cartesian({step: [{'alg':alg} | values for alg, grid in D.items() for values in cartesian(grid)] for step, D in param_grid.items()})\n",
    "G = cartesian({step: [[alg, values] for alg, grid in D.items() for values in cartesian(grid)] for step, D in param_grid.items()})\n",
    "len(G)\n",
    "G[0]\n",
    "# param_grid['clf']\n",
    "\n",
    "\n",
    "# [{step: {'alg':alg} | values} for step, D in param_grid.items() for alg, grid in D.items() for values in cartesian(grid)][2]\n",
    "# [(step, alg) for step, D in param_grid.items() for alg, grid in cartD.items() ]\n",
    "# [{'alg':k} | m for k,v in param_grid['clf'].items() for m in cartesian(v)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(param_grid['clf'].values())\n",
    "# param_grid[0]['clf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listify(X):\n",
    "    if X is None or X is np.nan:\n",
    "        return []\n",
    "    elif isinstance(X, (str,int,float,bool)) or callable(X):\n",
    "        return [X]\n",
    "    else:\n",
    "        return list(X)\n",
    "listify(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = mean_match_kdtree_classification\n",
    "print(type(x))\n",
    "# list(x)\n",
    "isinstance(x, type(sum))\n",
    "type(sum)\n",
    "x()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cartesian(dct):\n",
    "    \"\"\"Creates the Cartesian product of a dictionary with list-like values\"\"\"\n",
    "    # try:\n",
    "    D = {key: listify(val) for key, val in dct.items()}\n",
    "    return [dict(zip(D.keys(), x)) for x in it.product(*D.values())]\n",
    "    # except:\n",
    "    #     return dict()\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    'enc': cartesian({\n",
    "        'min_frequency': [0.1],\n",
    "    }),\n",
    "    'imp': cartesian({\n",
    "        'datasets': [5],\n",
    "        'save_all_iterations': [False],\n",
    "        'mean_match_candidates': 10,\n",
    "        'mean_match_function': mean_match_kdtree_classification,\n",
    "    }),\n",
    "    'clf':{\n",
    "        RandomForestClassifier: {\n",
    "            'max_depth': 3,\n",
    "            'random_state': [1,2],\n",
    "        },\n",
    "        HistGradientBoostingClassifier: {\n",
    "            'max_depth': 3,\n",
    "            'random_state': [1,2],\n",
    "        },\n",
    "    }\n",
    "}\n",
    "param_grid['imp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import clone, BaseEstimator\n",
    "# p = {'alg':RandomForestClassifier, 'max_depth':3}\n",
    "# p.pop('alg')(**p)\n",
    "p = {\n",
    "    RandomForestClassifier: {\n",
    "        'max_depth': 3,\n",
    "        'random_state': [1,2],\n",
    "    },\n",
    "    HistGradientBoostingClassifier(): {\n",
    "        'max_depth': 3,\n",
    "        'random_state': [1,2],\n",
    "    }\n",
    "}\n",
    "a = [{'alg':k} | m for k,v in p.items() for m in cartesian(v)]\n",
    "b = [{'alg':k} | m for k,v in p.items() for m in cartesian(v)]\n",
    "c = a[0]\n",
    "inst = lambda x: x if isinstance(x, BaseEstimator) else x()\n",
    "inst(c.pop('alg')).set_params(**c)\n",
    "# type(RandomForestClassifier)\n",
    "# RandomForestClassifier.__mro__\n",
    "# isinstance(RandomForestClassifier(), BaseEstimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = ColumnTransformer([('cat', OneHotEncoder(sparse_output=False, handle_unknown='infrequent_if_exist'), make_column_selector(dtype_include=['string','category']))], remainder='passthrough', verbose_feature_names_out=False)\n",
    "enc.set_params(cat__min_frequency=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.X.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from miceforest import ImputationKernel\n",
    "from miceforest.mean_matching_functions import default_mean_match, mean_match_kdtree_classification\n",
    "imp = ImputationKernel(self.X)\n",
    "# kds.mice(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp.dataset_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crse = '_total'\n",
    "cols = uniquify(['_total_cur',crse+'_cur',crse])\n",
    "self.Y[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = kds.complete_data(0)\n",
    "X.missing()\n",
    "X.dtypes\n",
    "enc = ColumnTransformer([('cat', OneHotEncoder(sparse_output=False, handle_unknown='infrequent_if_exist'), make_column_selector(dtype_include=['string','category']))], remainder='passthrough', verbose_feature_names_out=False)\n",
    "X = enc.fit_transform(X).join(self.Y[cols])\n",
    "X = {'test': X, 'train': X.query(f\"term_code==@train_term\")}\n",
    "y = {k: x.pop(crse) for k, x in X.items()}\n",
    "\n",
    "# train_term = 202208\n",
    "# X = {'test':X.copy(), 'train':X.query('term_code==@train_term')}\n",
    "# y = {'test':self.Y.copy(), 'train':self.Y.query('term_code==@train_term')}\n",
    "# clf = HistGradientBoostingClassifier()\n",
    "# clf.fit(X['train'], y['train'])\n",
    "# clf.predict(X['test'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Z = enc.fit_transform(X).join(self.Y['cur'].rename(columns=lambda x:x+'_cur')).join(self.Y['end']>0).prep()\n",
    "Y = self.Y['cur'].rename(columns=lambda x:x+'_cur').join(self.Y['end']>0).prep()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = list(self.pred.values())[0]['_total']['summary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.Z['__act_equiv'].reset_index(drop=True).disp(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import BayesianRidge, Ridge\n",
    "est = Pipeline(verbose=False, steps=[\n",
    "    ('prc', ColumnTransformer([('cat', OneHotEncoder(sparse_output=False, handle_unknown='infrequent_if_exist'), make_column_selector(dtype_include=['string','category']))], remainder='passthrough', verbose_feature_names_out=False)),\n",
    "    # ('imp', IterativeImputer(skip_complete=True, estimator=RandomForestRegressor())),\n",
    "    ('imp', IterativeImputer(skip_complete=True, estimator=BayesianRidge())),\n",
    "    \n",
    "    # ('clf', HistGradientBoostingClassifier()),\n",
    "    # ('clf', RandomForestClassifier()),\n",
    "    # ('clf', GradientBoostingClassifier()),\n",
    "])\n",
    "\n",
    "self.X.missing().disp(100)\n",
    "Q = est.fit_transform(self.X)\n",
    "Q.missing().disp(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{k:0 for k,v in est.get_params().items() if 'random_state' in k}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = list(self.pred.values())[0]['_total']\n",
    "df = a['summary']\n",
    "df.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'x'.ljust(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = list(self.pred.values())[0]['_total']['summary']\n",
    "def pivot(df, val, q=50):\n",
    "    Y = df.reset_index().pivot_table(columns='train_term', index=['crse','styp_code','pred_term'], values=val, aggfunc=[pctl(0),pctl(25),pctl(50),pctl(75),pctl(100),'max'])   # for _ in range(2):\n",
    "    #     # mr = Y.mean(axis=1)\n",
    "    #     ma = Y.abs().mean(axis=1)\n",
    "    #     Y = (Y.assign(abs_mean=ma) if Y.shape[1] > 1 else Y).T\n",
    "        # Y = (Y.assign(mean=mr, abs_mean=ma) if Y.shape[1] > 1 else Y).T\n",
    "    return Y.stack(0)#.assign(**{val:f\"{q}%\"}).set_index(val, append=True).swaplevel(0,1).round(2).prep()\n",
    "\n",
    "\n",
    "w = {stat: pivot(df.query(f\"pred_term!={self.infer_term}\"), stat) for stat in [\"err\",\"err%\",\"mse%\",\"f1_inv%\"]}\n",
    "w['err%'].sort_index()\n",
    "\n",
    "# pivot(df.query(f\"pred_term=={self.infer_term}\"), \"pred\", q=25)\n",
    "\n",
    "# list(self.pred.keys())[0]\n",
    "# val = 'err%'\n",
    "# q = 50\n",
    "# df.query(f\"pred_term!={self.infer_term}\").reset_index().pivot_table(columns='train_term', index=['crse','pred_term','styp_code'], values=val, aggfunc=pctl(q))\n",
    "# df.query(f\"pred_term=={self.infer_term}\").reset_index().pivot_table(columns='train_term', index=['crse','pred_term','styp_code'], values=val, aggfunc=pctl(q))\n",
    "# pivot(df.query(f\"pred_term!={self.infer_term}\"), 'err%')\n",
    "# {stat: pivot(df.query(f\"pred_term!={self.infer_term}\"), stat) for stat in [\"err\",\"err%\",\"mse%\",\"f1_inv%\"]}\n",
    "# pivot(df.query(f\"pred_term=={self.infer_term}\"), \"pred\")\n",
    "# pd.concat([pivot(df.query(f\"pred_term=={self.infer_term}\"), \"pred\", q) for q in [25,50,75]], axis=1)\n",
    "# for stat in [\"err\",\"err%\",\"mse%\",\"f1_inv%\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = list(self.pred.values())[0]['_total']['summary']\n",
    "df.disp(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze(self):\n",
    "    def pivot(df, val):\n",
    "        Y = (\n",
    "            df\n",
    "            .reset_index()\n",
    "            .pivot_table(columns='train_term', index=['crse','styp_code','pred_term'], values=val, aggfunc=[pctl(0),pctl(25),pctl(50),pctl(75),pctl(100)])\n",
    "            .rename_axis(columns=[val,'train_term'])\n",
    "            .stack(0)\n",
    "            .assign(abs_mean = lambda x: x.abs().mean(axis=1))\n",
    "        )\n",
    "        return Y\n",
    "\n",
    "    for k, v in self.pred.items():\n",
    "        df = v['summary']\n",
    "        mask = df.eval(f\"pred_term!={self.infer_term}\")\n",
    "        v['rslt'] = {stat: pivot(df[mask], stat) for stat in [\"pred\",\"err\",\"err%\",\"mse%\",\"f1_inv%\"]} | {'pred': pivot(df[~mask], \"pred\")}\n",
    "        # rslt['pred'] = pivot(df[~mask], \"pred\")\n",
    "# rslt = {stat: pivot(df.query(f\"pred_term!={self.infer_term}\"), stat) for stat in [\"pred\",\"err\",\"err%\",\"mse%\",\"f1_inv%\"]}\n",
    "\n",
    "# rslt['pred'] = pivot(df.query(f\"pred_term=={self.infer_term}\"), \"pred\")\n",
    "rslt['pred']\n",
    "rslt['err%']\n",
    "# pivot(df.query(f\"pred_term=={self.infer_term}\"), \"pred\", q=25)\n",
    "\n",
    "# list(self.pred.keys())[0]\n",
    "# val = 'err%'\n",
    "# q = 50\n",
    "# df.query(f\"pred_term!={self.infer_term}\").reset_index().pivot_table(columns='train_term', index=['crse','pred_term','styp_code'], values=val, aggfunc=pctl(q))\n",
    "# df.query(f\"pred_term=={self.infer_term}\").reset_index().pivot_table(columns='train_term', index=['crse','pred_term','styp_code'], values=val, aggfunc=pctl(q))\n",
    "# pivot(df.query(f\"pred_term!={self.infer_term}\"), 'err%')\n",
    "# {stat: pivot(df.query(f\"pred_term!={self.infer_term}\"), stat) for stat in [\"err\",\"err%\",\"mse%\",\"f1_inv%\"]}\n",
    "# pivot(df.query(f\"pred_term=={self.infer_term}\"), \"pred\")\n",
    "# pd.concat([pivot(df.query(f\"pred_term=={self.infer_term}\"), \"pred\", q) for q in [25,50,75]], axis=1)\n",
    "# for stat in [\"err\",\"err%\",\"mse%\",\"f1_inv%\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(k,str(v)) for k,v in est.named_steps.items()]\n",
    "# est.get_params()\n",
    "# est['imp'].__repr__()\n",
    "# est['imp']."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g(x):\n",
    "    return 3\n",
    "g.__name__ = 'hi'\n",
    "g.__str__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est = Pipeline(verbose=False, steps=[\n",
    "    # ('prc', ColumnTransformer([('cat', OneHotEncoder(), lambda X: X.apply(pd.api.types.is_string_dtype))], remainder='passthrough', verbose_feature_names_out=False)),\n",
    "    ('prc', ColumnTransformer([('cat', OneHotEncoder(), lambda X: X.select_dtypes(['string','category']).columns)], remainder='passthrough', verbose_feature_names_out=False)),\n",
    "    \n",
    "    # ('imp', IterativeImputer()),\n",
    "    # ('clf', HistGradientBoostingClassifier()),\n",
    "])\n",
    "\n",
    "p = cartesian(param_grid)[0]\n",
    "est.set_params(**p).fit_transform(self.Z).disp(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.select_dtypes(['string','category']).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.nan\n",
    "# x=None\n",
    "x in [np.nan], x == np.nan, x is np.nan\n",
    "np.fil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = cartesian(param_grid)[0]\n",
    "est.set_params(**p)\n",
    "P = est.get_params()\n",
    "P2 = clone(est).get_params()\n",
    "w = {k: (v,P2[k]) for k,v in P.items() if v!=P2[k]}\n",
    "w.keys()\n",
    "for k,v in w.items():\n",
    "    print(k)\n",
    "    print(type(v[0]))\n",
    "    print(type(v[1]))\n",
    "    print(\"-----------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = lambda X: X.apply(pd.api.types.is_string_dtype)\n",
    "g(self.Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est = Pipeline(verbose=False, steps=[\n",
    "    ('prc', ColumnTransformer([('cat', OneHotEncoder(sparse_output=False, handle_unknown='infrequent_if_exist'), lambda X: X.apply(pd.api.types.is_string_dtype))], remainder='passthrough', verbose_feature_names_out=False)),\n",
    "    # ('prc', ColumnTransformer([('cat', OneHotEncoder(sparse_output=False, handle_unknown='infrequent_if_exist'), make_column_selector(dtype_include=['string','category']))], remainder='passthrough', verbose_feature_names_out=False)),\n",
    "    # ('imp', IterativeImputer()),\n",
    "    # ('clf', HistGradientBoostingClassifier()),\n",
    "])\n",
    "\n",
    "e2 = clone(est)\n",
    "P = est.get_params()\n",
    "P2 = clone(est).get_params()\n",
    "w = {k: (v,P2[k]) for k,v in P.items() if v!=P2[k]}\n",
    "w.keys()\n",
    "w['prc__cat']\n",
    "# print(est['prc'].get_params())\n",
    "# print(clone(est)['prc'].get_params())\n",
    "# est.fit_transform(self.Z).disp(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = lambda x: pd.api.types.is_string_dtype(x)\n",
    "g(self.Z['__styp_code'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = self.X.select_dtypes(['string','category']).columns.tolist()\n",
    "est = Pipeline(verbose=False, steps=[\n",
    "    ('prc', ColumnTransformer([('cat', OneHotEncoder(sparse_output=False, handle_unknown='infrequent_if_exist'), cat)], remainder='passthrough', verbose_feature_names_out=False)),\n",
    "    ('imp', IterativeImputer()),\n",
    "    ('clf', HistGradientBoostingClassifier()),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est.named_steps['enc'].__str__()\n",
    "est.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.Y['end'].disp(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = est\n",
    "model._params = tuple([('n_sims',3)] + sorted((k,v) for k,v in model.get_params(deep=True).items() if isinstance(v, (int,float,str,bool)) and v not in [None,np.nan]))\n",
    "S = self.predict(est.set_params(enc__enc__min_frequency=0.1), '_total', 202208, 'n')\n",
    "S[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = list(self.pred.values())[0]#['_total'][202008]['n']\n",
    "A.keys()\n",
    "A['summary']\n",
    "# A['summary']\n",
    "# [202208]#['n']#.keys()#['summary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = cartesian(param_grid)[0]\n",
    "model = clone(est).set_params(**params)\n",
    "P = tuple([('n_sims',3)] + sorted((k,v) for k,v in model.get_params(deep=True).items() if isinstance(v, (int,float,str,bool)) and v not in [None,np.nan]))\n",
    "Q = list(self.pred.keys())[0]\n",
    "len(P), len(Q)\n",
    "for p,q in zip(P,Q):\n",
    "    print(p==q,p,q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(self.pred.keys())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est = Pipeline(verbose=False, steps=[\n",
    "    ('enc', ColumnTransformer([('enc', OneHotEncoder(sparse_output=False, handle_unknown='infrequent_if_exist'))], remainder='passthrough', verbose_feature_names_out=False)),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est.fit(self.X, self.Y['end']['_total'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y.disp()\n",
    "m.fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(self.pred.values())[0].keys()\n",
    "# self.pred.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted([(1,2),(2,3), (1,2,3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = ColumnTransformer([('enc', OneHotEncoder)], remainder='passthrough', verbose_feature_names_out=False)\n",
    "b = clone(a)\n",
    "# a.get_params() == \n",
    "b.get_params\n",
    "# dir(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted((k,v) for k,v in est.get_params(deep=True).items() if isinstance(v, (int,float,str,bool)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est = Pipeline(verbose=False, steps=[\n",
    "    ('enc', ColumnTransformer([('enc', OneHotEncoder(sparse_output=False, handle_unknown='infrequent_if_exist'))], remainder='passthrough', verbose_feature_names_out=False)),\n",
    "    ('imp', IterativeImputer(random_state=self.random_state)),\n",
    "    ('clf', HistGradientBoostingClassifier(random_state=self.random_state)),\n",
    "])\n",
    "est._pa\n",
    "# est2 = clone(est)\n",
    "# eq = False\n",
    "# P = est.get_params()\n",
    "# Q = est2.get_params()\n",
    "# N = [None,np.nan,pd.NA]\n",
    "\n",
    "get_params = lambda est: {k:v for k,v in est.get_params(deep=True) if isinstance(v, (int,float,str,bool))}\n",
    "\n",
    "# if P.keys() == Q.keys():\n",
    "#     eq = all(v==Q[k] or pd.isnull([v,Q[k]]).all() for k,v in P.items() if isinstance(v, (int,float,str,bool,type,type(None))))\n",
    "# eq\n",
    "# d\n",
    "    # for k, v in P.items():\n",
    "    #     if isinstance(v, (int,float,str,bool,type,type(None))):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = pd.NA\n",
    "isinstance(v, (int,float,str,bool,type,type(None),pd.NA))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(pd.NA)\n",
    "np.isnan([1])\n",
    "pd.isnull(pd.NA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.isnull([1,None]).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est = Pipeline(verbose=False, steps=[\n",
    "    ('enc', ColumnTransformer([('enc', OneHotEncoder(sparse_output=False, handle_unknown='infrequent_if_exist'))], remainder='passthrough', verbose_feature_names_out=False)),\n",
    "    ('imp', IterativeImputer(random_state=self.random_state)),\n",
    "    ('clf', HistGradientBoostingClassifier(random_state=self.random_state)),\n",
    "])\n",
    "est.get_params(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from joblib import dump, load\n",
    "import joblib \n",
    "from sklearn.base import clone\n",
    "est = Pipeline(verbose=False, steps=[\n",
    "    ('enc', ColumnTransformer([('enc', OneHotEncoder(sparse_output=False, handle_unknown='infrequent_if_exist'))], remainder='passthrough', verbose_feature_names_out=False)),\n",
    "    ('imp', IterativeImputer(random_state=self.random_state)),\n",
    "    ('clf', HistGradientBoostingClassifier(random_state=self.random_state)),\n",
    "])\n",
    "est = Pipeline(verbose=False, steps=[\n",
    "    ('enc', ColumnTransformer([('enc', OneHotEncoder(sparse_output=False, handle_unknown='infrequent_if_exist'))], remainder='passthrough', verbose_feature_names_out=False)),\n",
    "    ('imp', IterativeImputer(random_state=self.random_state)),\n",
    "    ('clf', HistGradientBoostingClassifier(random_state=self.random_state)),\n",
    "])\n",
    "\n",
    "# s = pickle.dump(est)\n",
    "# est2 = pickle.load(est)\n",
    "f = '/home/scook/institutional_data_analytics/admitted_matriculation_projection/admitted_matriculation_predictor/src/temp.joblib'\n",
    "s = joblib.dump(est, f)\n",
    "est2 = joblib.load(f)\n",
    "est == est2\n",
    "\n",
    "\n",
    "# est2 = clone(est)\n",
    "# est == est2\n",
    "# # est2\n",
    "N = est.named_steps\n",
    "E = N['enc']\n",
    "P = est.get_params(deep=True)\n",
    "for k, v in P.items():\n",
    "    if not isinstance(v, (int,float,str,bool,type,type(None))):\n",
    "        print(k, type(v))\n",
    "    # print(k, type(v), isinstance(v, (int,float,str,bool,type,type(None))))\n",
    "# N2 = est2.named_steps\n",
    "# N.keys() == N2.keys()\n",
    "# for k in N.keys():\n",
    "#     # print(k)\n",
    "#     P = N[k].get_params()\n",
    "#     P2 = N2[k].get_params()\n",
    "#     print(P.keys() == P2.keys())\n",
    "#     for l in P.keys():\n",
    "#         if P[l] != P2[l]:\n",
    "#             print(k, l)\n",
    "#             print(P[l])\n",
    "#             print(P2[l])\n",
    "\n",
    "# #         print()\n",
    "# #     print()\n",
    "# # print(N['enc'].get_params())\n",
    "# # print(N2['enc'].get_params())\n",
    "# # est.get_params()\n",
    "# # est.named_steps == est2.named_steps\n",
    "# # est."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import make_column_transformer\n",
    "ColumnTransformer(transformers=[('enc', OneHotEncoder(sparse_output=False, handle_unknown='infrequent_if_exist'), make_column_selector(dtype_include=['string','category']))], remainder='passthrough', verbose_feature_names_out=False)\n",
    "make_column_transformer((OneHotEncoder(sparse_output=False, handle_unknown='infrequent_if_exist'), make_column_selector(dtype_include=['string','category'])), remainder='passthrough', verbose_feature_names_out=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                # ('enc', ColumnTransformer([('enc', OneHotEncoder(sparse_output=False, handle_unknown='infrequent_if_exist'), make_column_selector(dtype_include=['string','category']))], remainder='passthrough', verbose_feature_names_out=False)),\n",
    "                # ('imp', IterativeImputer(random_state=self.random_state)),\n",
    "                # ('clf', HistGradientBoostingClassifier(random_state=self.random_state)),\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "G = {\n",
    "    'enc': {\n",
    "        ColumnTransformer: {\n",
    "            'tranformers':\n",
    "        OneHotEncoder: {\n",
    "            'sparse_output': False,\n",
    "            'handle_unknown': 'infrequent_if_exist',\n",
    "            'min_frequency': [0.1,0.2],\n",
    "        },\n",
    "        LabelEncoder: {\n",
    "            'sparse_output': False,\n",
    "            'handle_unknown': 'infrequent_if_exist',\n",
    "            'min_frequency': [0.1,0.2],\n",
    "        },\n",
    "    },\n",
    "    'imp': {\n",
    "        IterativeImputer: {\n",
    "            'initial_strategy': ['median','mean'],\n",
    "        },\n",
    "    }\n",
    "    # __enc__min_frequency':[0.1],\n",
    "    # # 'imp__initial_strategy': ['median'],\n",
    "    # 'clf__learning_rate': [0.1,0.2],\n",
    "    # 'n_sims': 4,\n",
    "}\n",
    "# cartesian({step: {meth: cartesian({name: listify(vals) for name, vals in M.items()}) for meth, M in S.items()} for step, S in G.items()})[0]\n",
    "# {step: {meth: cartesian({name: listify(vals) for name, vals in M.items()}) for meth, M in S.items()} for step, S in G.items()}['enc']\n",
    "# cartesian({step: {meth: {name: listify(vals) for name, vals in M.items()} for meth, M in S.items()} for step, S in G.items()})\n",
    "\n",
    "# [{meth: values for values in cartesian(M)} for meth, M in G['enc'].items()]\n",
    "step = 'enc'\n",
    "# H = {step: [(method,values) for method, M in S.items() for values in cartesian(M) ] for step, S in G.items()}\n",
    "H = {step: [[method,values] for method, M in S.items() for values in cartesian(M)] for step, S in G.items()}\n",
    "q = cartesian(H)[0]\n",
    "\n",
    "# meth = OneHotEncoder\n",
    "# {meth: cartesian(G['enc'][meth])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                # ('enc', ColumnTransformer([('enc', OneHotEncoder(sparse_output=False, handle_unknown='infrequent_if_exist'), make_column_selector(dtype_include=['string','category']))], remainder='passthrough', verbose_feature_names_out=False)),\n",
    "                # ('imp', IterativeImputer(random_state=self.random_state)),\n",
    "                # ('clf', HistGradientBoostingClassifier(random_state=self.random_state)),\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "G = {\n",
    "    'enc': {\n",
    "        OneHotEncoder: {\n",
    "            'sparse_output': False,\n",
    "            'handle_unknown': 'infrequent_if_exist',\n",
    "            'min_frequency': [0.1,0.2],\n",
    "        },\n",
    "        LabelEncoder: {\n",
    "            'sparse_output': False,\n",
    "            'handle_unknown': 'infrequent_if_exist',\n",
    "            'min_frequency': [0.1,0.2],\n",
    "        },\n",
    "    },\n",
    "    'imp': {\n",
    "        IterativeImputer: {\n",
    "            'initial_strategy': ['median','mean'],\n",
    "        },\n",
    "    }\n",
    "    # __enc__min_frequency':[0.1],\n",
    "    # # 'imp__initial_strategy': ['median'],\n",
    "    # 'clf__learning_rate': [0.1,0.2],\n",
    "    # 'n_sims': 4,\n",
    "}\n",
    "# cartesian({step: {meth: cartesian({name: listify(vals) for name, vals in M.items()}) for meth, M in S.items()} for step, S in G.items()})[0]\n",
    "# {step: {meth: cartesian({name: listify(vals) for name, vals in M.items()}) for meth, M in S.items()} for step, S in G.items()}['enc']\n",
    "# cartesian({step: {meth: {name: listify(vals) for name, vals in M.items()} for meth, M in S.items()} for step, S in G.items()})\n",
    "\n",
    "# [{meth: values for values in cartesian(M)} for meth, M in G['enc'].items()]\n",
    "step = 'enc'\n",
    "# H = {step: [(method,values) for method, M in S.items() for values in cartesian(M) ] for step, S in G.items()}\n",
    "H = {step: [[method,values] for method, M in S.items() for values in cartesian(M)] for step, S in G.items()}\n",
    "q = cartesian(H)[0]\n",
    "\n",
    "# meth = OneHotEncoder\n",
    "# {meth: cartesian(G['enc'][meth])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.pred['_total'][0]['summary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rslt = {'train': T} | {k: pd.concat([t[k] for t in T.values()]) for k in list(T.values()).pop().keys()}\n",
    "# {k: 0 for k in list(T.values()).pop().keys()}\n",
    "# T\n",
    "\n",
    "\n",
    "# rslt = {'param':param, 'train': {train_term: self.predict(crse, param.copy(), train_term) for train_term in self.term_codes}}\n",
    "# self.pred.setdefault(crse,list()).append(rslt)\n",
    "rslt['summary']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d = {'a':3}\n",
    "# # d[d.keys().pop()]\n",
    "# d.keys().__iter__()[0]\n",
    "v = list(d.values())[0]\n",
    "v\n",
    "# dir(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crse = '_total'\n",
    "w = self.pred[crse][0]\n",
    "q = w['train']\n",
    "q."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crse = '_total'\n",
    "W = self.pred[crse][0]\n",
    "# for train_term, sims in W['train'].items():\n",
    "#     for sim in sims:\n",
    "#         for rslt, df in sim.items():\n",
    "#             print(rslt)\n",
    "# W |= {rslt: pd.concat([df for df in sim[rslt]])}\n",
    "\n",
    "rslt = 'details'\n",
    "L = [sim[rslt] for sims in W['train'].values() for sim in sims]\n",
    "\n",
    "crse = '_total'\n",
    "W = self.pred[crse][0]\n",
    "W |= {rslt: pd.concat([sim[rslt] for sims in W['train'].values() for sim in sims]) for rslt in ['details','summary']}\n",
    "W.keys()\n",
    "    \n",
    "    \n",
    "    # df in sim[rslt]\n",
    "\n",
    "\n",
    "        # print(sim.keys())\n",
    "    # print(len(rslt))\n",
    "    # print(rslt.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterations = 3\n",
    "\n",
    "# opts = dict()\n",
    "# opts['random_state'] = 42\n",
    "# opts['save_all_iterations'] = False\n",
    "# opts['datasets'] = 5\n",
    "# opts['mean_match_candidates'] = 29\n",
    "# opts['mean_match_function'] = mean_match_kdtree_classification\n",
    "\n",
    "# opts['datasets'] = 2\n",
    "# opts['mean_match_candidates'] = 1\n",
    "# opts['mean_match_function'] = default_mean_match\n",
    "\n",
    "# P = self.predict(opts=opts)\n",
    "\n",
    "# R = self.train(iterations=iterations, opts=opts,\n",
    "#     styp_codes='n',\n",
    "#     # train_terms=202208,\n",
    "#     )\n",
    "# for k, v in R[False]['rslt'].items():\n",
    "#     print(k)\n",
    "#     v['err%'].disp(100)\n",
    "\n",
    "# tune = []\n",
    "# # for func in [mean_match_kdtree_classification, default_mean_match]:\n",
    "#     # opts['mean_match_function'] = func\n",
    "# for cand in range(2,41,3):\n",
    "#     opts['mean_match_candidates'] = cand\n",
    "#     print(mysort(opts))\n",
    "#     R = self.train(\n",
    "#         styp_codes='n',\n",
    "#         iterations=iterations,\n",
    "#         opts=opts)\n",
    "#     R[False]['rslt']['_total','n']['err%'].disp(100)\n",
    "#     tune.append(R)\n",
    "#     write(self.tune, tune)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import make_column_selector, ColumnTransformer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier\n",
    "\n",
    "seed = 0\n",
    "n_sims = 2\n",
    "param_grid = cartesian({\n",
    "    'enc__enc__min_frequency':[0.05,0.1],\n",
    "    'imp__initial_strategy': ['median'],\n",
    "    'clf__learning_rate': [0.1],\n",
    "})\n",
    "\n",
    "rslt = dict()\n",
    "# for crse, param, train_term in it.product(self.crse, param_grid, self.term_codes):\n",
    "for crse in self.crse:\n",
    "    for param in param_grid:\n",
    "        for train_term in self.term_codes:\n",
    "    print(crse, train_term, param)\n",
    "    rslt.setdefault(crse, list()).append({'param':param, 'train':dict()})\n",
    "    cols = uniquify([*self.feat,'_total_cur',crse+'_cur',crse])\n",
    "    X = self.Z[cols]\n",
    "    X = {'test': X, 'train': X.query(f\"term_code==@train_term\")}\n",
    "    y = {k: x.pop(crse) for k, x in X.items()}\n",
    "    for sim in range(n_sims):\n",
    "        est = Pipeline(verbose=False, steps=[\n",
    "            ('enc', ColumnTransformer([('enc', OneHotEncoder(sparse_output=False, handle_unknown='infrequent_if_exist'), make_column_selector(dtype_include=['string','category']))], remainder='passthrough', verbose_feature_names_out=False)),\n",
    "            ('imp', IterativeImputer(random_state=seed)),\n",
    "            ('clf', HistGradientBoostingClassifier(random_state=seed)),\n",
    "        ]).set_params(**param).fit(X['train'], y['train'])\n",
    "        details = (\n",
    "            y['test']\n",
    "            .rename('true')\n",
    "            .to_frame()\n",
    "            .assign(pred=est.predict(X['test']), train_term=train_term, crse=crse, sim=sim)\n",
    "            .set_index(['train_term','crse','sim'], append=True)\n",
    "            .binarize()\n",
    "        )\n",
    "        agg = lambda x: pd.Series({\n",
    "            'pred': x['pred'].sum(min_count=1),\n",
    "            'true': x['true'].sum(min_count=1),\n",
    "            'mse%': ((1*x['pred'] - x['true'])**2).mean()*100,\n",
    "            'f1_inv%': (1-f1_score(x.dropna()['true'], x.dropna()['pred'], zero_division=np.nan))*100,\n",
    "        })\n",
    "        summary = details.groupby([*self.mlt_grp,'train_term','sim']).apply(agg).join(self.mlt).rename_axis(index={'term_code':'pred_term'})\n",
    "        for x in ['pred','true']:\n",
    "            summary[x] = summary[x] * summary['mlt']\n",
    "        summary.insert(2, 'err', summary['pred'] - summary['true'])\n",
    "        summary.insert(3, 'err%', (summary['err'] / summary['true']).clip(-1, 1) * 100)\n",
    "        rslt[crse][-1]['train'].setdefault(train_term, list()).append({'details':details, 'summary':summary})\n",
    "        # assert 1==2\n",
    "\n",
    "    # rslt[crse][-1]['train'].setdefault(train_term, list()).append({'details':, 'summary':  })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary.disp(1)\n",
    "r = rslt['_total'][0]\n",
    "r['param']\n",
    "r['train'][202008][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cartesian(dct):\n",
    "    \"\"\"Creates the Cartesian product of a dictionary with list-like values\"\"\"\n",
    "    D = {key: listify(val) for key, val in dct.items()}\n",
    "    return [dict(zip(D.keys(), x)) for x in it.product(*D.values())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import make_column_selector, make_column_transformer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier\n",
    "\n",
    "seed = 0\n",
    "train_term = 202208\n",
    "crse = '_total'\n",
    "cols = list({'_total',crse})\n",
    "\n",
    "for sim in range(1):\n",
    "    X = (\n",
    "        self.X\n",
    "        .join(self.Y['cur'][cols].rename(columns=lambda x:x+'_cur'))\n",
    "        .join(self.Y['end'][crse]>0)\n",
    "        .assign(train_term=train_term, crse=crse, sim=sim)\n",
    "        .prep()\n",
    "        .set_index(['train_term','crse','sim'], append=True)\n",
    "    )\n",
    "    X = {'test': X.copy(), 'train': X.query(f\"term_code==@train_term\")}\n",
    "    y = {k: x.pop(crse) for k, x in X.items()}\n",
    "\n",
    "    enc = ColumnTransformer(remainder='passthrough', verbose_feature_names_out=False, transformers=[\n",
    "        ('enc', OneHotEncoder(sparse_output=False, handle_unknown='infrequent_if_exist'), make_column_selector(dtype_include=['string','category'])),\n",
    "    ])\n",
    "\n",
    "    est = Pipeline(verbose=False, steps=[\n",
    "        ('enc', enc),\n",
    "        ('imp', IterativeImputer(random_state=seed)),\n",
    "        ('clf', HistGradientBoostingClassifier(random_state=seed)),\n",
    "    ])\n",
    "    param_grid = {\n",
    "        'enc__enc__min_frequency':[0.05],\n",
    "        'imp__initial_strategy': ['median'],\n",
    "        'clf__learning_rate': [0.2],\n",
    "    }\n",
    "    grid = GridSearchCV(estimator=est, param_grid=param_grid)\n",
    "    grid.fit(X['train'], y['train'])\n",
    "    model = grid.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = y['test'].rename('true').to_frame().assign(pred=model.predict(X['test'])).binarize()\n",
    "P.disp(1)\n",
    "agg = lambda x: pd.Series({\n",
    "    'pred': x['pred'].sum(min_count=1),\n",
    "    'true': x['true'].sum(min_count=1),\n",
    "    'mse%': ((1*x['pred'] - x['true'])**2).mean()*100,\n",
    "    'f1_inv%': (1-f1_score(x.dropna()['true'], x.dropna()['pred'], zero_division=np.nan))*100,\n",
    "})\n",
    "S = P.groupby([*self.mlt_grp,'train_term','sim']).apply(agg).join(self.mlt).rename_axis(index={'term_code':'pred_term'})\n",
    "for x in ['pred','true']:\n",
    "    S[x] = S[x] * S['mlt']\n",
    "# # # S.insert(2, 'err', S['pred'] - S['true'])\n",
    "# # # S.insert(3, 'err%', (S['err'] / S['true']).clip(-1, 1) * 100)\n",
    "S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(check_params_exist(pipe, 'imputer'))\n",
    "model.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = pipe['enc']\n",
    "vars(e)\n",
    "# e.onehotencoder\n",
    "e.named_transformers_['onehotencoder']\n",
    "e.transformers_[0]\n",
    "# e.onehotencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit_transform(X['train']).disp(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pick imp & clf hyperparams\n",
    "imp m datasets\n",
    "draw n shuffled datasets\n",
    "train on one year\n",
    "predict on others\n",
    "apply multiplier\n",
    "compute error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.X.query('idx==0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crse = '_total'\n",
    "train_term = 202208\n",
    "\n",
    "iterations = 3\n",
    "opts = dict()\n",
    "opts['random_state'] = 42\n",
    "opts['save_all_iterations'] = False\n",
    "opts['datasets'] = 2\n",
    "opts['mean_match_candidates'] = 29\n",
    "cols = list({'_total',crse})\n",
    "\n",
    "imp = ImputationKernel(self.X, **opts)\n",
    "imp.mice(iterations)\n",
    "self.Z = (\n",
    "    pd.concat([imp.complete_data(k).assign(sim=k) for k in range(imp.dataset_count())])\n",
    "    .pipe(pd.get_dummies)\n",
    "    .join(self.Y['cur'][cols].rename(columns=lambda x:x+'_cur'))\n",
    "    .join(self.Y['end'][crse]>0)\n",
    "    .prep()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.Z.query('\"index\"==17404')\n",
    "\n",
    "# .query('pidm==2710').disp(1000)#.query('index==0')#.shape#.sort_index().disp(10)\n",
    "# self.Z.groupby('pidm').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.Z = (\n",
    "    pd.concat([\n",
    "        imp.complete_data(k)\n",
    "        .assign(sim=k)\n",
    "        \n",
    "    for k in range(imp.dataset_count())])\n",
    "    .pipe(pd.get_dummies)\n",
    "    .join(self.Y['cur'][cols].rename(columns=lambda x:x+'_cur'))\n",
    "    .join(self.Y['end'][crse]>0)\n",
    "    .prep()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# self.Z.groupby(self.attr).ngroup().sort_index().disp(10)\n",
    "# self.Z.groupby('index')\n",
    "self.X = self.X.reset_index().reset_index().set_index(['index',*self.attr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GroupShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier\n",
    "train_term = 202208\n",
    "# Z = self.Z.reset_index().sample(frac=1).drop_duplicates(self.attr).assign(train_term=train_term, crse=crse).set_index([*self.attr,'train_term','crse','sim'])\n",
    "Z = (\n",
    "    self.Z\n",
    "    .sample(frac=1)\n",
    "    .reset_index()\n",
    "    .drop_duplicates(self.attr)\n",
    "    .assign(train_term=train_term, crse=crse)\n",
    "    .set_index([*self.attr,'train_term','crse','sim'])\n",
    ")\n",
    "X = {'test':Z, 'train':Z.query(f'term_code=={train_term}')}\n",
    "y = {k: v.pop(crse) for k, v in X.items()}\n",
    "clf = HistGradientBoostingClassifier()\n",
    "clf.fit(X['train'], y['train'])\n",
    "P = y['test'].rename('true').to_frame().assign(pred=clf.predict(X['test'])).binarize()\n",
    "\n",
    "agg = lambda x: pd.Series({\n",
    "    'pred': x['pred'].sum(min_count=1),\n",
    "    'true': x['true'].sum(min_count=1),\n",
    "    'mse%': ((1*x['pred'] - x['true'])**2).mean()*100,\n",
    "    'f1_inv%': (1-f1_score(x.dropna()['true'], x.dropna()['pred'], zero_division=np.nan))*100,\n",
    "})\n",
    "S = P.groupby([*self.mlt_grp,'train_term']).apply(agg).join(self.mlt).rename_axis(index={'term_code':'pred_term'})\n",
    "for x in ['pred','true']:\n",
    "    S[x] = S[x] * S['mlt']\n",
    "# S.insert(2, 'err', S['pred'] - S['true'])\n",
    "# S.insert(3, 'err%', (S['err'] / S['true']).clip(-1, 1) * 100)\n",
    "S\n",
    "\n",
    "# P.groupby([*self.mlt_grp,'train_term','crse','sim']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "where = lambda x: x.query(\"levl_code == 'ug' and styp_code in ('n','r','t')\").copy()\n",
    "agg = lambda y: where(y).groupby(self.mlt_grp)['credit_hr'].agg(lambda x: (x>0).sum())\n",
    "A = agg(self.reg_df['end'])\n",
    "self.Y = {k: self.X[[]].join(y.set_index(['pidm','term_code','crse'])['credit_hr']) for k, y in self.reg_df.items()}\n",
    "B = agg(self.Y['end'])\n",
    "M = (A / B).replace(np.inf, pd.NA).rename('mlt').reset_index().query(f\"term_code != {self.infer_term}\").prep()\n",
    "N = M.assign(term_code=self.infer_term)\n",
    "self.mlt = pd.concat([M, N], axis=0).set_index(self.mlt_grp)\n",
    "# self.Y = Y\n",
    "self.Y = {k: y.squeeze().unstack().dropna(how='all', axis=1).fillna(0) for k, y in self.Y.items()}\n",
    "self.Y['end'].disp(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# self.Y = {k: y.squeeze().unstack().dropna(how='all', axis=1).fillna(0) for k, y in Y.items()}\n",
    "self.Y['end'].groupby(self.mlt_grp).agg(lambda x:(x>0).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.mlt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.mlt_grp = ['levl_code', 'styp_code', 'term_code','crse']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "where = lambda x: x.query(\"levl_code == 'ug' and styp_code in ('n','r','t')\").copy()\n",
    "with warnings.catch_warnings(action='ignore'):\n",
    "    Y = {k: pd.concat([term.reg[k].query(\"crse in @self.crse\") for term in self.term.values()]) for k in ['cur','end']}\n",
    "agg = lambda y: where(y).groupby(self.mlt_grp)['credit_hr'].agg(lambda x: (x>0).sum())\n",
    "A = agg(Y['end'])\n",
    "# A.disp(10)\n",
    "Y = {k: self.X[[]].join(y.set_index(['pidm','term_code','crse'])['credit_hr']) for k, y in Y.items()}\n",
    "B = agg(Y['end'])\n",
    "M = (A / B).replace(np.inf, pd.NA).rename('mlt').reset_index().query(f\"term_code != {self.infer_term}\").prep()\n",
    "N = M.assign(term_code=self.infer_term)\n",
    "self.mlt = pd.concat([M, N], axis=0).set_index(self.mlt_grp)\n",
    "self.Y = {k: y.squeeze().unstack().dropna(how='all', axis=1).fillna(0) for k, y in Y.items()}\n",
    "\n",
    "fat = lambda y: y.squeeze().unstack('crse').dropna(how='all', axis=1).fillna(0)\n",
    "# fat(self.mlt)\n",
    "self.mlt.squeeze()#unstack('crse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "where = lambda x: x.query(\"levl_code == 'ug' and styp_code in ('n','r','t')\").copy()\n",
    "with warnings.catch_warnings(action='ignore'):\n",
    "    Y = {k: pd.concat([term.reg[k].query(\"crse in @self.crse\") for term in self.term.values()]) for k in ['cur','end']}\n",
    "agg = lambda y: where(y).groupby(self.mlt_grp)['credit_hr'].agg(lambda x: (x>0).sum())\n",
    "A = agg(Y['end'])\n",
    "# A.disp(10)\n",
    "Y = {k: self.X[[]].join(y.set_index(['pidm','term_code','crse'])['credit_hr']) for k, y in Y.items()}\n",
    "B = agg(Y['end'])\n",
    "M = (A / B).replace(np.inf, pd.NA).rename('mlt').reset_index().query(f\"term_code != {self.infer_term}\").prep()\n",
    "N = M.assign(term_code=self.infer_term)\n",
    "self.mlt = pd.concat([M, N], axis=0).set_index(self.mlt_grp)\n",
    "self.mlt\n",
    "\n",
    "# self.Y = Y\n",
    "# fat = lambda y: y.squeeze().unstack('crse').dropna(how='all', axis=1).fillna(0)\n",
    "# self.Y = {k: fat(y) for k, y in Y.items()}\n",
    "# self.mlt\n",
    "# fat(self.mlt)\n",
    "# self.Y = {k: y.squeeze().unstack('crse').dropna(how='all', axis=1).fillna(0) for k, y in Y.items()}\n",
    "# self.Y['end'].disp(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = self.mlt_grp\n",
    "w.pop(0)\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "where = lambda x: x.query(\"levl_code == 'ug' and styp_code in ('n','r','t')\").copy()\n",
    "\n",
    "with warnings.catch_warnings(action='ignore'):\n",
    "    Y = {k:\n",
    "         pd.concat([term.reg[k].query(\"crse in @self.crse\") for term in self.term.values()])\n",
    "         .set_index(['pidm',*w,'crse'])['credit_hr'].unstack()\n",
    "          \n",
    "          \n",
    "           for k in ['cur','end']}\n",
    "agg = lambda y: where(y).groupby(w).agg(lambda x: (x>0).sum()).prep()\n",
    "A = agg(Y['end'])\n",
    "# A.disp(100)\n",
    "Y = {k: self.X[[]].join(y.droplevel(['levl_code','styp_code'])) for k, y in Y.items()}\n",
    "# B = agg(Y['end'])\n",
    "# M = (A / B).replace(np.inf, pd.NA).rename('mlt').reset_index().query(f\"term_code != {self.infer_term}\").prep()\n",
    "\n",
    "# N = M.assign(term_code=self.infer_term)\n",
    "# self.mlt = pd.concat([M, N], axis=0).set_index(self.mlt_grp)\n",
    "# self.Y = {k: y.squeeze().unstack().dropna(how='all', axis=1).fillna(0) for k, y in Y.items()}\n",
    "Y['end'].droplevel('levl_code')\n",
    "\n",
    "# Z = (\n",
    "#     self.X\n",
    "#     .pipe(pd.get_dummies)\n",
    "#     .join(self.Y['cur'][cols].rename(columns=lambda x:x+'_cur'))\n",
    "#     .join(self.Y['end'][crse]>0)\n",
    "#     .prep()\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.Y['cur'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(self.Z.query('sim==0')[crse]>0).groupby(['styp_code','term_code']).sum()\n",
    "1777*1.27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.Z.query('sim==0')[crse].groupby(['styp_code','term_code']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(self.Y['end']>0).groupby(w).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = self.mlt_grp\n",
    "w.pop(0)\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.Z['_total'].groupby(w).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.mlt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crse = '_total'\n",
    "Q = (self.Y['end'][[crse]]>0).assign(crse=crse).groupby(self.mlt_grp).sum().join(self.mlt)\n",
    "Q[crse] *= Q['mlt']\n",
    "Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier\n",
    "train_term = 202208\n",
    "# Z = self.Z.reset_index().sample(frac=1).drop_duplicates(self.attr).assign(train_term=train_term, crse=crse).set_index([*self.attr,'train_term','crse','sim'])\n",
    "Z = (\n",
    "    self.Z\n",
    "    .sample(frac=1)\n",
    "    .reset_index()\n",
    "    .drop_duplicates(self.attr)\n",
    "    .assign(train_term=train_term, crse=crse)\n",
    "    .set_index([*self.attr,'train_term','crse','sim'])\n",
    ")\n",
    "X = {'train': Z.query(f'term_code=={train_term}'), 'test': Z}\n",
    "y = {k: v.pop(crse) for k, v in X.items()}\n",
    "clf = HistGradientBoostingClassifier()\n",
    "clf.fit(X['train'], y['train'])\n",
    "P = y['test'].rename('true').to_frame().assign(pred=clf.predict(X['test'])).binarize()\n",
    "\n",
    "agg = lambda x: pd.Series({\n",
    "    'pred': x['pred'].sum(min_count=1),\n",
    "    'true': x['true'].sum(min_count=1),\n",
    "    'mse%': ((1*x['pred'] - x['true'])**2).mean()*100,\n",
    "    'f1_inv%': (1-f1_score(x.dropna()['true'], x.dropna()['pred'], zero_division=np.nan))*100,\n",
    "})\n",
    "S = P.groupby([*self.mlt_grp,'train_term']).apply(agg).join(self.mlt).rename_axis(index={'term_code':'pred_term'})\n",
    "for x in ['pred','true']:\n",
    "    S[x] = S[x] * S['mlt']\n",
    "# S.insert(2, 'err', S['pred'] - S['true'])\n",
    "# S.insert(3, 'err%', (S['err'] / S['true']).clip(-1, 1) * 100)\n",
    "S\n",
    "\n",
    "# P.groupby([*self.mlt_grp,'train_term','crse','sim']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P.disp(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.term[202208].reg['cur'].empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from LiveAMP import *\n",
    "attr = [\n",
    "        'term_code',\n",
    "        'pidm',\n",
    "        'hs_qrtl',\n",
    "    ]\n",
    "feat= [\n",
    "        'term_code',\n",
    "        ['birth_day','median',['term_code','styp_code']],\n",
    "        ['international',False],\n",
    "]\n",
    "\n",
    "def f(A):\n",
    "    A = [listify(x) for x in A]\n",
    "    return {x.pop(0): x if x else [np.nan] for x in A}\n",
    "L = [f(attr), f(feat)]\n",
    "cols = L[0] | L[1]\n",
    "attr, feat = [list(x.keys()) for x in L]\n",
    "\n",
    "# attr = f(attr)\n",
    "# feat = f(feat)\n",
    "# cols = attr | feat\n",
    "# attr = list(attr.keys())\n",
    "# feat = list(feat.keys())\n",
    "\n",
    "# attr\n",
    "# c0\n",
    "\n",
    "# for A in [attr, feat]:\n",
    "#     A = [listify(x) for x in A]\n",
    "#     cols |= {x.pop(0): x if x else [np.nan] for x in A}\n",
    "#     A = list(A.keys())\n",
    "\n",
    "# attr = [listify(x) for x in attr]\n",
    "# feat = [listify(x) for x in feat]\n",
    "# feat = {x.pop(0): x if x else [np.nan] for x in feat}\n",
    "# attr = {x.pop(0): x if x else [np.nan] for x in attr}\n",
    "# cols = attr | feat\n",
    "# attr = list(attr.keys())\n",
    "# feat = list(feat.keys())\n",
    "# # cols = {x.pop(0): x if x else [np.nan] for x in attr+feat}\n",
    "# # print(cols)\n",
    "# where = lambda x: x.query(\"levl_code == 'ug' and styp_code in ('n','r','t')\").copy()\n",
    "# X = where(self.X)\n",
    "# for k, v in cols.items():\n",
    "#     X[k] = X.impute(k, *v)\n",
    "# X.missing()\n",
    "# X = X.prep().binarize().categorize().set_index(attr, drop=False)[feat].rename(columns=lambda col:'_'+col)\n",
    "# X\n",
    "cols\n",
    "attr\n",
    "# feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr = {x: [np.nan] for x in self.attr}\n",
    "feat = {x[0][1:]:listify(x[1]) for x in self.feat}\n",
    "cols = attr | feat\n",
    "# cols = {k:listify(v) for k,v in cols.items()}\n",
    "cols\n",
    "# where = lambda x: x.query(\"levl_code == 'ug' and styp_code in ('n','r','t')\").copy()\n",
    "# X = where(self.X)\n",
    "# X.missing().disp(1000)\n",
    "# cols\n",
    "# for k, v in cols.items():\n",
    "#     X[k] = X.impute(k, *listify(v))\n",
    "# X = X.prep().binarize().categorize()\n",
    "# X.missing().disp(1000)\n",
    "# Z = pd.concat([X.impute(*x) for x in self.feat], axis=1).prep().binarize().categorize()\n",
    "\n",
    "# col = uniquify(self.attr + feat)\n",
    "\n",
    "\n",
    "\n",
    "# .set_index(self.attr, drop=False).rename(columns=lambda col:'_'+col)\n",
    "# # X = where(self.X).reset_index().set_index(self.attr, drop=False).rename(columns=lambda col:'_'+col)\n",
    "# self.Z = pd.concat([X.impute(*x) for x in self.feat], axis=1).prep().binarize().categorize()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.Y['cur'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = y['test'].rename('true').to_frame().assign(train_term=train_term, crse=crse, pred=clf.predict(X['test'])).binarize()\n",
    "P.groupby([*self.mlt_grp,'train_term','crse']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['train'].disp(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.Z.shape, Z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([imp.complete_data(k).assign(sim=k) for k in range(imp.dataset_count())]).transform(pd.get_dummies).disp(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.Y['cur'].disp(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = (\n",
    "    pd.concat([imp.complete_data(k).assign(sim=k) for k in range(imp.dataset_count())])\n",
    "    .pipe(pd.get_dummies)\n",
    "    .join(self.Y['cur'][cols].rename(columns=lambda x:x+'_cur'))\n",
    "    .join(self.Y['end'][crse])\n",
    "    .reset_index()\n",
    "    .sample(frac=1)\n",
    "    .drop_duplicates(self.attr)\n",
    "    .set_index([*self.attr,'sim'])\n",
    ")\n",
    "Z\n",
    "# pd.get_dummies(Z)\n",
    "\n",
    "# X = Z.reset_index().sample(frac=1).drop_duplicates(self.attr).set_index([*self.attr,'sim'])\n",
    "# train_term = 202208\n",
    "# qry = {'infer': f'term_code=={self.infer_term}', 'train': f'term_code=={train_term}'}\n",
    "# qry['test'] = f\"~({join(qry.values(),'|')})\"\n",
    "# X = {k: X.query(v) for k, v in qry.items()}\n",
    "# X = {'train': Z.query(f'term_code=={train_term}'), 'test': Z.query(f'term_code!={train_term}')}\n",
    "\n",
    "\n",
    "\n",
    "# X['test'].vc('term_code')\n",
    "# X = {\"infer\":X.query(f\"term_code=={self.infer_term}\"), \"train\"}\n",
    "\n",
    "# Z#.index#['index']\n",
    "# Z.loc['pidm']\n",
    "# Z.index.names\n",
    "# Z.groupby('index').size()\n",
    "# Z = Z.sample(frac=1)\n",
    "# Z\n",
    "# Z.reset_index?\n",
    "\n",
    "# ('index', drop=False)#.drop_duplicates('index')#.vc('sim')\n",
    "# sample(n=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier\n",
    "qry = {\"infer\": f\"term_code=={self.infer_term}\", \"train\": f\"term_code==train_term\"}\n",
    "qry['test'] = \"~(\"+join(qry.values() ,' | ')+\")\"\n",
    "X = dict()\n",
    "y = dict()\n",
    "for k, v in qry.items():\n",
    "    X[k] = Z.query(v)\n",
    "    y[k] = X[k].pop(crse)\n",
    "# clf = RandomForestClassifier(categorical_features=self.Z.select_dtypes('category').columns.tolist())\n",
    "from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier\n",
    "clf = HistGradientBoostingClassifier()#categorical_features=X['train'].select_dtypes('category').columns.tolist())\n",
    "# clf.categorical_features\n",
    "# X['train'].dtypes\n",
    "clf.fit(X['train'], y['train'])\n",
    "y['pred'] = clf.predict(X['test'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y['pred'] = clf.predict(X['test']).astype('boolean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "details = y['test'].rename('true').to_frame().assign(pred=y['pred']).binarize().assign(crse=crse)\n",
    "details.groupby([*self.mlt_grp,'train_term','sim']).sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.get_dummies(Z).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.Z.query('coll_code==00')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Z.columns)\n",
    "W = pd.get_dummies(self.Z)\n",
    "print(W.columns)\n",
    "W[W['_coll_code_00']].disp(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = pd.concat([imp.complete_data(k).join(self.Y['end'][crse]>0).assign(train_term=train_term, sim=k).set_index(['train_term','sim'], append=True) for k in range(imp.dataset_count())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.Z.select_dtypes('category').columns.tolist()\n",
    "# pd.api.types.is_string_dtype(x) for x in self.Z)\n",
    "# A = self.Z.dtypes.to_frame()\n",
    "# A.query(\"0=='index\")\n",
    "# A[A=='category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier\n",
    "qry = {\"infer\": f\"term_code=={self.infer_term}\", \"train\": f\"term_code==train_term\"}\n",
    "qry['test'] = \"~(\"+join(qry.values() ,' | ')+\")\"\n",
    "X = dict()\n",
    "y = dict()\n",
    "Z = pd.get_dummies(Z)\n",
    "for k, v in qry.items():\n",
    "    X[k] = pd.get_dummies(Z).query(v)\n",
    "    y[k] = X[k].pop(crse)\n",
    "# clf = RandomForestClassifier(categorical_features=self.Z.select_dtypes('category').columns.tolist())\n",
    "clf = HistGradientBoostingClassifier()#categorical_features=X['train'].select_dtypes('category').columns.tolist())\n",
    "# clf.categorical_features\n",
    "# X['train'].dtypes\n",
    "clf.fit(X['train'], y['train'])\n",
    "# y['pred'] = clf.predict(X['test'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['infer'].vc('term_code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I[0].join(self.Y['end'][crse]>0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self = P['model']\n",
    "predictor_var_names = [self._get_variable_name(i) for i in np.sort(self.predictor_vars)]\n",
    "imputed_var_names   = [self._get_variable_name(i) for i in self.imputation_order]\n",
    "I = pd.DataFrame(self.get_feature_importance(0), index=imputed_var_names, columns=predictor_var_names).T\n",
    "I\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.Z.missing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = P['model']\n",
    "# [model._get_variable_name(i) for i in model.imputation_order]\n",
    "# model.imputation_order\n",
    "# [model._get_variable_name(int(i)) for i in np.sort(model.imputation_order)]\n",
    "model.vars_with_any_missing\n",
    "self.Z.columns[27]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 0\n",
    "model = self.pred[k]['model']\n",
    "# [model._get_variable_name(int(i)) for i in np.sort(model.imputation_order)]\n",
    "# model.imputed_variable_count\n",
    "# model.column_names\n",
    "# print(self.pred[k]['meta'])\n",
    "model.feature_importance_df().sort_values('_total_end', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.Z.shape\n",
    "self.term[202208].adm['zip']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ImputationKernel?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.X.binarize()['international'].dtype\n",
    "self.X['international'].dropna().dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "where = lambda x: x.query(\"levl_code == 'ug' and styp_code in ('n','r','t')\").copy()\n",
    "X = where(self.X.binarize()).set_index(self.attr, drop=False).rename(columns=lambda col:'_'+col)\n",
    "# X['_international'].disp(1)\n",
    "X.impute('_international', False)\n",
    "# X.dtypes.disp(1000)\n",
    "# self.X.dtypes.disp(100)\n",
    "# pd.concat([X.impute(*x) for x in self.feat], axis=1).prep().binarize().categorize()\n",
    "# X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binarize(ser):\n",
    "    assert isinstance(ser, pd.Series)\n",
    "    s = set(ser.dropna())\n",
    "    if s:\n",
    "        try:\n",
    "            ser = ser.str.lower()\n",
    "            if s.issubset({'y','n'}):\n",
    "                ser = (ser=='y').astype('boolean').fillna(False)\n",
    "            if s.issubset({'true','false'}):\n",
    "                ser = (ser=='true').astype('boolean').fillna(False)\n",
    "        except:\n",
    "            if s.issubset({0,1}):\n",
    "                ser = ser.astype('boolean').fillna(False)\n",
    "    return ser\n",
    "A = R['international'].str.lower()\n",
    "(A=='true').astype('boolean').fillna(False)\n",
    "# binarize(A).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# self.term[202408].adm['international'].dtypes\n",
    "# self.reg\n",
    "R = pd.concat([term.raw for term in self.term.values()], ignore_index=True).dropna(axis=1, how='all').prep()\n",
    "R['international'].value_counts()#.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.term[202208].adm['international'].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R['international'].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.X.columns\n",
    "self.X['international'].value_counts()\n",
    "# self.X['natn_code'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qry = f\"\"\"select distinct B.gorvisa_natn_code_issue, B.gorvisa_vtyp_code from gorvisa B\"\"\"\n",
    "db.execute(qry).disp(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from LiveAMP import *\n",
    "t = TERM(term_code=202008, overwrite={'adm':True}, show={'adm':True})\n",
    "A = t.get_adm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A['zip']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.head('gorvisa')\n",
    "# db.value_counts('gorvisa','gorvisa_natn_code_issue','gorvisa_pidm')\n",
    "# qry = \"select * from (select gorvisa_pidm, count(gorvisa_natn_code_issue) as ct from gorvisa group by gorvisa_pidm) where ct>1\"\n",
    "# qry = \"select gorvisa_pidm, count(distinct gorvisa_natn_code_issue) as ct from gorvisa group by gorvisa_pidm order by ct desc\"\n",
    "# db.head(qry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qry = f\"\"\"\n",
    "select A.* from (\n",
    "    select \n",
    "        A.*,\n",
    "        case\n",
    "            when max(case when A.cycle_day >= 0 then A.cycle_date end) over (partition by A.pidm, A.appl_no) = A.cycle_date then 1\n",
    "            end as r1,\n",
    "        case\n",
    "            when sum(case when A.cycle_day <  0 then 1 end) over (partition by A.pidm, A.appl_no) >= 0/2 then 1\n",
    "            when sysdate - trunc(to_date('11-Sep-20')) < 5 then 1\n",
    "            end as r2\n",
    "    from (\n",
    "        select \n",
    "            trunc(to_date('11-Sep-20')) - trunc(A.current_date) as cycle_day,\n",
    "            trunc(A.current_date) as cycle_date,\n",
    "            min(trunc(A.current_date)) over (partition by A.pidm, A.appl_no) as appl_date,\n",
    "            min(case when A.apst_code = 'D' and A.apdc_code in (select stvapdc_code from stvapdc where stvapdc_inst_acc_ind is not null) then trunc(A.current_date) end) over (partition by A.pidm, A.appl_no) as apdc_date,\n",
    "            A.pidm,\n",
    "            A.id,\n",
    "            A.term_code as term_code_entry,\n",
    "            A.levl_code,\n",
    "            A.styp_code,\n",
    "            A.admt_code,\n",
    "            A.appl_no,\n",
    "            A.apst_code,\n",
    "            A.apdc_code,\n",
    "            A.camp_code,\n",
    "            A.saradap_resd_code as resd_code,\n",
    "            A.coll_code_1 as coll_code,\n",
    "            A.majr_code_1 as majr_code,\n",
    "            A.dept_code,\n",
    "            A.hs_percentile as hs_pctl\n",
    "        from opeir.admissions_fall2020 A\n",
    "    ) A where cycle_day between 0 and 0 + 14 and A.apst_code = 'D' and A.apdc_code in (select stvapdc_code from stvapdc where stvapdc_inst_acc_ind is not null)\n",
    ") A where A.r1 = 1 and A.r2 = 1\n",
    "\"\"\"\n",
    "db.head(qry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qry = f\"\"\"\n",
    "select A.* from (\n",
    "--select A.r2, count(*) from (\n",
    "    select \n",
    "        A.*,\n",
    "        case\n",
    "            when max(case when A.cycle_day >= 0 then A.cycle_date end) over (partition by A.pidm, A.appl_no) = A.cycle_date then 1\n",
    "            end as r1,\n",
    "        case\n",
    "            when sum(case when A.cycle_day <  0 then 1 else 0 end) over (partition by A.pidm, A.appl_no) >= 0/2 then 1\n",
    "            when sysdate - trunc(to_date('11-Sep-23')) < 5 then 1\n",
    "            end as r2\n",
    "    from (\n",
    "        select \n",
    "            trunc(to_date('11-Sep-23')) - trunc(A.current_date) as cycle_day,\n",
    "            trunc(A.current_date) as cycle_date,\n",
    "            min(trunc(A.current_date)) over (partition by A.pidm, A.appl_no) as appl_date,\n",
    "            min(case when A.apst_code = 'D' and A.apdc_code in (select stvapdc_code from stvapdc where stvapdc_inst_acc_ind is not null) then trunc(A.current_date) end) over (partition by A.pidm, A.appl_no) as apdc_date,\n",
    "            A.pidm,\n",
    "            A.id,\n",
    "            A.term_code as term_code_entry,\n",
    "            A.levl_code,\n",
    "            A.styp_code,\n",
    "            A.admt_code,\n",
    "            A.appl_no,\n",
    "            A.apst_code,\n",
    "            A.apdc_code,\n",
    "            A.camp_code,\n",
    "            A.saradap_resd_code as resd_code,\n",
    "            A.coll_code_1 as coll_code,\n",
    "            A.majr_code_1 as majr_code,\n",
    "            A.dept_code,\n",
    "            A.hs_percentile as hs_pctl\n",
    "        from opeir.admissions_fall2023 A\n",
    "    ) A where cycle_day between 0 and 0 + 14 and A.apst_code = 'D' and A.apdc_code in (select stvapdc_code from stvapdc where stvapdc_inst_acc_ind is not null)\n",
    ") A where A.r1 = 1 and A.r2 = 1\n",
    "\"\"\"\n",
    "A = db.head(qry)\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A.query('r1.notnull()')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = self.pred[-1]['model']\n",
    "model.mean_match_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.pred[0]['summary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R[False]['summary'].disp(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for g, d in R[False]['rslt'].items():\n",
    "    print(g)\n",
    "    d['err%'].disp(100)\n",
    "    # print(df.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in R[False]['rslt'].keys():\n",
    "    for b, v in R.items():\n",
    "        print(k, b)\n",
    "        v['rslt'][k]['err%'].disp(100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R = self.train(iterations=iterations, opts=opts,\n",
    "    styp_codes='n',\n",
    "    # train_terms=202208,\n",
    "    )\n",
    "\n",
    "def pivot(df, val, q=50):\n",
    "    Y = df.reset_index().pivot_table(columns='train_term', index='pred_term', values=val, aggfunc=pctl(q))\n",
    "    for _ in range(2):\n",
    "        mr = Y.mean(axis=1)\n",
    "        ma = Y.abs().mean(axis=1)\n",
    "        Y = (Y.assign(mean=mr, abs_mean=ma) if Y.shape[1] > 1 else Y).T\n",
    "    return Y.assign(**{val:f\"{q}%\"}).set_index(val, append=True).swaplevel(0,1).round(2).prep().T\n",
    "\n",
    "def analyze(df):\n",
    "    r = {stat: pivot(df.query(f\"pred_term!={self.infer}\"), stat) for stat in [\"err\",\"err%\",\"mse%\",\"f1_inv%\"]}\n",
    "    r['proj'] = pd.concat([pivot(df.query(f\"pred_term=={self.infer}\"), \"pred\", q) for q in [25,50,75]], axis=1)\n",
    "    return r\n",
    "\n",
    "for b, L in R.items():\n",
    "    v = {k: pd.concat([Y[k] for Y in L]) for k in ['detail', 'summary']}\n",
    "    # v['opts'] = opts.copy()\n",
    "    # for g, df in v['summary'].groupby(['crse', 'styp_code']):\n",
    "    #     v[g] = analyze(df)\n",
    "    v['rslt'] = {g: analyze(df) for g, df in v['summary'].groupby(['crse', 'styp_code'])}\n",
    "    # R[b] = v\n",
    "# v[False]\n",
    "# R[False]\n",
    "# R[False][0].keys()\n",
    "# v['summary'].keys()\n",
    "# v['rslt']\n",
    "v.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from LiveAMP import *\n",
    "yr = 23\n",
    "qry = f\"\"\"\n",
    "select\n",
    "    sfrstcr_term_code as term_code,\n",
    "    sfrstcr_pidm as pidm,\n",
    "    (select sgbstdn_levl_code from sgbstdn where sgbstdn_pidm = sfrstcr_pidm and sgbstdn_term_code_eff <= sfrstcr_term_code order by sgbstdn_term_code_eff desc fetch first 1 rows only) as levl_code,\n",
    "    (select sgbstdn_styp_code from sgbstdn where sgbstdn_pidm = sfrstcr_pidm and sgbstdn_term_code_eff <= sfrstcr_term_code order by sgbstdn_term_code_eff desc fetch first 1 rows only) as styp_code\n",
    "from sfrstcr\n",
    "where\n",
    "    sfrstcr_term_code = 20{yr}08\n",
    "    and  trunc(sfrstcr_add_date) <= trunc(to_date('10-Sep-{yr}')) -- added before cycle_day\n",
    "    and (trunc(sfrstcr_rsts_date) > trunc(to_date('10-Sep-{yr}')) or sfrstcr_rsts_code in ('DC','DL','RD','RE','RW','WD','WF')) -- dropped after cycle_day or still enrolled\n",
    "    and sfrstcr_ptrm_code not in ('28','R3')\n",
    "group by sfrstcr_term_code, sfrstcr_pidm\n",
    "\"\"\"\n",
    "\n",
    "qry = f\"\"\"\n",
    "select\n",
    "    term_code, levl_code, styp_code, count(distinct pidm)\n",
    "from {subqry(qry)}\n",
    "where levl_code='UG' and styp_code in ('N','R','T')\n",
    "group by term_code, levl_code, styp_code\n",
    "order by term_code, levl_code, styp_code\n",
    "\"\"\"\n",
    "db.head(qry, 100, show=True)\n",
    "# )\n",
    "# select A.* from A\n",
    "# union all\n",
    "# select\n",
    "#     A.cycle_day, A.term_code, A.pidm, A.levl_code, A.styp_code,\n",
    "#     '_total' as crse,\n",
    "#     sum(A.credit_hr) as credit_hr\n",
    "# from A\n",
    "# group by A.cycle_day, A.term_code, A.pidm, A.levl_code, A.styp_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(2143-2273)/2273"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from LiveAMP import *\n",
    "yr = 21\n",
    "qry = f\"\"\"\n",
    "select\n",
    "    A.sfrstcr_term_code as term_code,\n",
    "    A.sfrstcr_pidm as pidm,\n",
    "    (select C.sgbstdn_levl_code from sgbstdn C where C.sgbstdn_pidm = A.sfrstcr_pidm and C.sgbstdn_term_code_eff <= A.sfrstcr_term_code order by C.sgbstdn_term_code_eff desc fetch first 1 rows only) as levl_code,\n",
    "    (select C.sgbstdn_styp_code from sgbstdn C where C.sgbstdn_pidm = A.sfrstcr_pidm and C.sgbstdn_term_code_eff <= A.sfrstcr_term_code order by C.sgbstdn_term_code_eff desc fetch first 1 rows only) as styp_code,\n",
    "    --lower(B.ssbsect_subj_code) || B.ssbsect_crse_numb as crse,\n",
    "    sum(B.ssbsect_credit_hrs) as credit_hr\n",
    "from sfrstcr A, ssbsect B\n",
    "where\n",
    "    A.sfrstcr_term_code = B.ssbsect_term_code\n",
    "    and A.sfrstcr_crn = B.ssbsect_crn\n",
    "    and A.sfrstcr_term_code = 20{yr}08\n",
    "    and A.sfrstcr_ptrm_code not in ('28','R3')\n",
    "    and  trunc(A.sfrstcr_add_date) <= trunc(to_date('10-Sep-{yr}')) -- added before cycle_day\n",
    "    and (trunc(A.sfrstcr_rsts_date) > trunc(to_date('10-Sep-{yr}')) or A.sfrstcr_rsts_code in ('DC','DL','RD','RE','RW','WD','WF')) -- dropped after cycle_day or still enrolled\n",
    "    and B.ssbsect_subj_code <> 'INST'\n",
    "group by A.sfrstcr_term_code, A.sfrstcr_pidm--, B.ssbsect_subj_code, B.ssbsect_crse_numb\n",
    "\"\"\"\n",
    "\n",
    "qry = f\"\"\"\n",
    "select\n",
    "    term_code, levl_code, styp_code, count(distinct pidm)\n",
    "from {subqry(qry)}\n",
    "where credit_hr > 0 and levl_code='UG' and styp_code in ('N','R','T')\n",
    "group by term_code, levl_code, styp_code\n",
    "\"\"\"\n",
    "db.head(qry, 100)\n",
    "# )\n",
    "# select A.* from A\n",
    "# union all\n",
    "# select\n",
    "#     A.cycle_day, A.term_code, A.pidm, A.levl_code, A.styp_code,\n",
    "#     '_total' as crse,\n",
    "#     sum(A.credit_hr) as credit_hr\n",
    "# from A\n",
    "# group by A.cycle_day, A.term_code, A.pidm, A.levl_code, A.styp_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "where = lambda x: x.query(\"levl_code == 'ug' and styp_code in ('n','r','t')\").copy()\n",
    "where(self.term[202208].reg['end']).query(\"styp_code=='n'\")['pidm'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.mlt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.pred[0]['rslt']['summary'].disp(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.pred[0]['rslt']['model'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_term = 202208\n",
    "styp_code = 'n'\n",
    "crse = '_total'\n",
    "model = self.pred[0]['rslt']['model']\n",
    "\n",
    "\n",
    "targ = crse+'_end'\n",
    "cols = uniquify(['_total_cur', crse+'_cur', targ])\n",
    "T = self.Z.join(self.Y[cols])\n",
    "T[targ] = T[targ] > 0\n",
    "if styp_code != \"all\":\n",
    "    T = T.query(\"styp_code==@styp_code\")\n",
    "\n",
    "\n",
    "qry = f\"term_code!={train_term}\"\n",
    "g = lambda df, nm=None: df.filter(like='_end').rename(columns=lambda x:x[:-4]).melt(ignore_index=False, var_name='crse', value_name=nm).set_index('crse', append=True)\n",
    "P = pd.concat([model.complete_data(k).assign(sim=k).set_index('sim', append=True) for k in range(model.dataset_count())])\n",
    "Y = g(P,'pred').join(g(T,'true')).assign(train_term=train_term).query(qry).prep()\n",
    "agg = lambda x: pd.Series({\n",
    "    'pred': x['pred'].sum(min_count=1),\n",
    "    'true': x['true'].sum(min_count=1),\n",
    "    'mse%': ((1*x['pred'] - x['true'])**2).mean()*100,\n",
    "    'f1_inv%': (1-f1_score(x.dropna()['true'], x.dropna()['pred'], zero_division=np.nan))*100,\n",
    "})\n",
    "S = Y.groupby(['crse','levl_code','styp_code','term_code','train_term','sim']).apply(agg).join(self.mlt).rename_axis(index={'term_code':'pred_term'})\n",
    "for x in ['pred','true']:\n",
    "    S[x] = S[x] * S['mlt']\n",
    "S.insert(2, 'err', S['pred'] - S['true'])\n",
    "S.insert(3, 'err%', (S['err'] / S['true']).clip(-1, 1) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = Y.groupby(['crse','levl_code','styp_code','term_code','train_term','sim']).apply(agg).join(self.mlt).rename_axis(index={'term_code':'pred_term'})\n",
    "for x in ['pred','true']:\n",
    "    S[x] = S[x] * S['mlt']\n",
    "S.insert(2, 'err', S['pred'] - S['true'])\n",
    "S.insert(3, 'err%', (S['err'] / S['true']).clip(-1, 1) * 100)\n",
    "S.disp(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(self.Y.shape, self.Z.shape)\n",
    "self.mlt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.Y.isnull().sum()\n",
    "# self.crse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.term[202208].reg['cur'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crse = '_total'\n",
    "d = {'cur':{crse}, 'end':{crse}}\n",
    "d['cur'].add('_total')\n",
    "T = self.Z.copy()\n",
    "for k, v in d.items():\n",
    "    T = T.join(self.Y[k][listify(v)].rename(columns=lambda x:x+'_'+k))\n",
    "T.disp(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.Y['end']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.Y['cur']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = self.X.query('distance.isnull()')\n",
    "\n",
    "Y[['pidm','distance','stat_code','natn_code','zip','international','resd_code']].disp(100)\n",
    "# self.X.query(\"pidm in [1121725,1060603,1063123,1071878]\").disp(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = self.X.query(\"distance.notnull() & ((natn_code.notnull() & natn_code!='us') | stat_code in ['hi','ak','pr','ae','ap'])\")\n",
    "Y[['distance','stat_code','natn_code']].disp(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.X.query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qry = f\"\"\"\n",
    "select\n",
    "    B.spraddr_cnty_code as cnty_code,\n",
    "    B.spraddr_seqno as s,\n",
    "    case\n",
    "        when B.spraddr_atyp_code = 'PA' then 6\n",
    "        when B.spraddr_atyp_code = 'PR' then 5\n",
    "        when B.spraddr_atyp_code = 'MA' then 4\n",
    "        when B.spraddr_atyp_code = 'BU' then 3\n",
    "        when B.spraddr_atyp_code = 'BI' then 2\n",
    "        when B.spraddr_atyp_code = 'P1' then 1\n",
    "        when B.spraddr_atyp_code = 'P2' then 0\n",
    "        end as r\n",
    "from spraddr B\n",
    "where B.spraddr_pidm = 1087120\n",
    "\"\"\"\n",
    "\n",
    "qry = f\"\"\"\n",
    "select\n",
    "    B.cnty_code\n",
    "from (\n",
    "    select\n",
    "        B.spraddr_cnty_code as cnty_code,\n",
    "        B.spraddr_seqno as s,\n",
    "        case\n",
    "            when B.spraddr_atyp_code = 'PA' then 6\n",
    "            when B.spraddr_atyp_code = 'PR' then 5\n",
    "            when B.spraddr_atyp_code = 'MA' then 4\n",
    "            when B.spraddr_atyp_code = 'BU' then 3\n",
    "            when B.spraddr_atyp_code = 'BI' then 2\n",
    "            when B.spraddr_atyp_code = 'P1' then 1\n",
    "            when B.spraddr_atyp_code = 'P2' then 0\n",
    "            end as r\n",
    "    from spraddr B\n",
    "    where B.spraddr_pidm = 1087120\n",
    ") B\n",
    "where\n",
    "    B.cnty_code is not null\n",
    "    and B.r is not null\n",
    "order by\n",
    "    B.r desc,\n",
    "    B.s desc\n",
    "\"\"\"\n",
    "db.execute(qry).disp(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.X.missing()\n",
    "Y = self.X.query(\"distance.isnull() & (natn_code.isnull() | natn_code=='us') & stat_code not in ['hi','ak','pr','ae','ap']\")\n",
    "Y.sort_values('pidm').disp(1000)\n",
    "uniquify(Y['pidm'].tolist())\n",
    "# [['stat_code','natn_code']].disp(100)\n",
    "# self.X.iloc[[17412,30806]].disp(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qry = f\"select distinct spraddr_stat_code from spraddr\"\n",
    "A = db.execute(qry)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.value_counts('spbpers','spbpers_gndr_code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.head('spbpers').columns.sort_values()\n",
    "db.head('opeir.admissions_fall2022').columns.sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "join(['AL','AR','AZ','CA','CO','CT','DC','DE','FL','GA','IA','ID','IL','IN','KS','KY','LA','MA','MD','ME','MI','MN','MO','MS','MT','NC','ND','NE','NH','NJ','NM','NV','NY','OH','OK','OR','PA','RI','SC','SD','TN','TX','UT','VA','VT','WA','WI','WV','WY')\n",
    "# len(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(sort(A.squeeze().str.upper().unique().tolist()))\n",
    "# sort(A.squeeze().str.upper).unique().tolist())\n",
    "print(A.squeeze().str.upper().value_counts().sort_index().index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qry = f\"select A.* from spraddr A where A.spraddr_pidm = 1029274\"db.execute(qry).disp(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.head('opeir.admissions_fall2023',1).disp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.head('szrsstd',1).disp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.execute('stvatyp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = self.X.query(\"distance.isnull()\")['pidm'].tolist()\n",
    "qry = f\"select A.* from spraddr A where A.spraddr_pidm in ({join(L)}) and spraddr_stat_code='TX' order by spraddr_pidm\"\n",
    "# qry = f\"select A.* from sarappd A where A.sarappd_pidm in ({join(L)})\"\n",
    "# qry = f\"select A.*, (select sarappd_apdc_code from sarappd where sarappd_pidm = saradap_pidm and sarappd_appl_no = saradap_appl_no order by sarappd_seq_no desc fetch first 1 row only) as sarappd_apdc_code from saradap A where saradap_pidm in ({join(L)})\"\n",
    "A = db.execute(qry, show=True)\n",
    "A.disp(2000)\n",
    "# A['saradap_resd_code']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "w = pathlib.Path('/home/scook/institutional_data_analytics/admitted_matriculation_projection/resources/flags/raw/old/201608_admitted_flags_report_031616.xlsx')\n",
    "w.w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pd_ext(func):\n",
    "    def wrapper(X, *args, **kwargs):\n",
    "        try:\n",
    "            Y = func(X, *args, **kwargs)\n",
    "            print(1)\n",
    "        except:\n",
    "            Y = pd.DataFrame(X)\n",
    "            try:\n",
    "                Y = func(Y, *args, **kwargs)\n",
    "                print(2)\n",
    "            except:\n",
    "                Y = Y.apply(func, *args, **kwargs)\n",
    "                print(3)\n",
    "        if isinstance(X, pd.Series):\n",
    "            try:\n",
    "                Y = Y.squeeze()\n",
    "            except:\n",
    "                pass\n",
    "        return Y\n",
    "    wrapper.__name__ = func.__name__\n",
    "    return wrapper\n",
    "\n",
    "@pd_ext\n",
    "def binarize(ser):\n",
    "    assert isinstance(ser, pd.Series)\n",
    "    s = set(ser.dropna())\n",
    "    if s:\n",
    "        if s.issubset({'y','Y'}):\n",
    "            ser = ser.notnull().astype('boolean')\n",
    "        elif s.issubset({0,1}):\n",
    "            ser = ser.astype('boolean')\n",
    "    return ser\n",
    "\n",
    "for func in [disp, to_numeric, prep, categorize, binarize, rnd, vc, missing, impute, unmelt]:\n",
    "    for cls in [pd.DataFrame, pd.Series]:\n",
    "        setattr(cls, func.__name__, func)\n",
    "\n",
    "# self.X['schlship_app'].value_counts()\n",
    "# self.X['fafsa_app'].value_counts()\n",
    "# self.X['schlship_app'].dtype\n",
    "# A = self.X.binarize()['schlship_app']\n",
    "# A['schlship_app']\n",
    "binarize(self.X)['schlship_app']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.X['schlship_app'].groupby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.X.columns.sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.Y['end']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "where = lambda x: x.query(\"levl_code == 'ug' and styp_code in ('n','r','t')\").copy()\n",
    "X = where(self.X).set_index(self.attr, drop=False).rename(columns=lambda col:'_'+col)\n",
    "self.Z = pd.concat([X.impute(*x) for x in self.feat], axis=1).prep().binarize().categorize()\n",
    "\n",
    "g = ['levl_code','styp_code','term_code','crse']\n",
    "with warnings.catch_warnings(action='ignore'):\n",
    "    Y = {k: pd.concat([term.reg[k].query(\"crse in @self.crse\")[['pidm',*g,'credit_hr']].assign(credit_hr=lambda x: x['credit_hr'].fillna(0)>0) for term in self.term.values()]) for k in ['end','cur']}\n",
    "agg = lambda y, g: where(y).groupby(g)[['credit_hr']].sum()\n",
    "A = agg(Y['end'], g)\n",
    "Y = {k: self.Z[[]].join(y.set_index(['pidm','term_code','crse'])['credit_hr'], how='inner') for k, y in Y.items()}\n",
    "B = agg(Y['end'], g)\n",
    "self.mlt = A / B\n",
    "\n",
    "self.Y = {k: y.squeeze().unstack().fillna(False).rename(columns=lambda x:f'{x}_{k}') for k, y in Y.items()}\n",
    "Y['end']\n",
    "# M = (end / cur).query(\"term_code != @self.infer\")\n",
    "# N = M.reset_index().assign(term_code=self.infer).set_index(M.index.names)\n",
    "# self.mlt = pd.concat([M, N], axis=0).replace(np.inf, pd.NA).squeeze().rename('mlt').prep()\n",
    "\n",
    "\n",
    "# end\n",
    "self.Y['end']\n",
    "# Y = [self.Y[0].query(\"crse in @self.crse\").set_index('crse', append=True).unstack().droplevel(0,1).rename(columns=lambda x:f\"_{x}_end\")\n",
    "# # Y.droplevel?\n",
    "# Y.disp(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y['end']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def impute(df, col, val=None, grp=None):\n",
    "#     val = val if val is not None else 'median' if pd.api.types.is_numeric_dtype(df[col]) else 'mode'\n",
    "#     if val in ['median']:\n",
    "#         func = lambda x: x.median()\n",
    "#     elif val in ['mean','ave','avg','average']:\n",
    "#         func = lambda x: x.mean()\n",
    "#     elif val in ['mode','most_frequent']:\n",
    "#         func = lambda x: x.mode()[0]\n",
    "#     else:\n",
    "#         func = lambda x: val\n",
    "#     df[col] = (df if grp is None else df.groupby(grp))[col].transform(lambda x: x.fillna(func(x)))\n",
    "#     return df\n",
    "# pd.DataFrame.impute = impute\n",
    "\n",
    "self.Z.reset_index(drop=True)\n",
    "A = self.Z.copy()\n",
    "c = '_birth_day'\n",
    "mask = A[c].isnull()\n",
    "# A.impute('_birth_day', val='median', grp=['term_code','styp_code'])\n",
    "# A.impute('_birth_day', val=np.nan, grp=['term_code','styp_code'])\n",
    "A.impute('_birth_day', val=np.nan, grp=['term_code','styp_code'])\n",
    "A.loc[mask,c].disp(5)\n",
    "# A.groupby(['term_code','styp_code'])['_birth_day'].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trf = ColumnTransformer(self.feat, remainder='drop',verbose_feature_names_out = False)\n",
    "# where = lambda x: x.query(\"levl_code == 'ug' and styp_code in ('n','r','t')\").copy()\n",
    "# with warnings.catch_warnings(action='ignore'):\n",
    "#     self.Y = [pd.concat([term.reg[k] for term in self.term.values()]).assign(credit_hr=lambda x:x['credit_hr'].fillna(0)>0) for k in [0,1]]\n",
    "#     self.Z = trf.fit_transform(where(self.X).set_index(self.attr, drop=False)).rename(columns=lambda x:'_'+x)\n",
    "\n",
    "kwargs = {\n",
    "    'feat': [\n",
    "        ['_gender',np.nan],\n",
    "        ['_appl_day',np.nan],\n",
    "        ['_apdc_day',np.nan],\n",
    "        ['_hs_qrtl',np.nan],\n",
    "        ['_act_equiv',np.nan],\n",
    "        ['_remote',False],\n",
    "        ['_resd',False],\n",
    "        ['_legacy',False],\n",
    "        *[[f'_race_{r}',False] for r in ['american_indian','asian','black','pacific','white','hispanic']],\n",
    "        ['_waiver',False],\n",
    "        # ['_fafsa_app',False],\n",
    "        ['_schlship_app',False],\n",
    "        # ['_finaid_accepted',False],\n",
    "        ['_ssb',False],\n",
    "        ['_math',False],\n",
    "        ['_reading',False],\n",
    "        ['_writing',False],\n",
    "        ['_gap_score',0],\n",
    "        ['_oriented','n'],\n",
    "        ['_distance','max'],\n",
    "        ['_birth_day','median',['term_code','styp_code']],\n",
    "    ],\n",
    "}\n",
    "where = lambda x: x.query(\"levl_code == 'ug' and styp_code in ('n','r','t')\").copy()\n",
    "g = lambda col:'_'+col\n",
    "# cols = [x[0] for x in kwargs['feat']]\n",
    "# where(self.X).rename(columns=g)[cols].isnull().sum().disp(1000)\n",
    "X = where(self.X).set_index(self.attr, drop=False).rename(columns=g)\n",
    "Z = pd.concat([X.impute(*x) for x in self.feat], axis=1).prep().binarize().categorize()\n",
    "Z.isnull().sum().disp(1000)\n",
    "Z.dtypes\n",
    "# L = [Z.impute(col, *val)]\n",
    "# L = [[col, *listify(val)] for C, val in kwargs['feat'] for col in listify(C)]\n",
    "# L = [Z.impute(g(col), *listify(val)) for C, val in kwargs['feat'] for col in listify(C)]\n",
    "# L\n",
    "#).rename(columns=lambda x:'_'+x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.get_dummies(Z).disp(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = read('/home/scook/institutional_data_analytics/admitted_matriculation_projection/LiveAMP/flags/parq/flg_202308.parq', columns=['gender'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.head('spbpers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qry = \"select spbpers_sex, count(*) from spbpers group by spbpers_sex\"\n",
    "db.execute(qry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from LiveAMP import *\n",
    "from miceforest import ImputationKernel\n",
    "from miceforest.mean_matching_functions import default_mean_match, mean_match_kdtree_classification\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler, PowerTransformer, QuantileTransformer, FunctionTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn import set_config\n",
    "set_config(transform_output=\"pandas\")\n",
    "\n",
    "def feature_importance_df(self, dataset=0, normalize=True, iteration=None):\n",
    "    imputed_var_names = [self._get_variable_name(int(i)) for i in np.sort(self.imputation_order)]\n",
    "    predictor_var_names = [self._get_variable_name(int(i)) for i in np.sort(self.predictor_vars)]\n",
    "    I = pd.DataFrame(self.get_feature_importance(dataset, iteration), index=imputed_var_names, columns=predictor_var_names).T\n",
    "    return I / I.sum() * 100 if normalize else I\n",
    "ImputationKernel.feature_importance_df = feature_importance_df\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class AMP(MyBaseClass):\n",
    "    cycle_day: int\n",
    "    term_codes: typing.List\n",
    "    infer: int\n",
    "    crse: typing.List\n",
    "    feat: typing.Dict\n",
    "    attr: typing.List\n",
    "    sch: bool = True\n",
    "    overwrite: typing.Dict = None\n",
    "    show: typing.Dict = None\n",
    "\n",
    "    def dump(self):\n",
    "        return write(self.rslt, self, overwrite=True)\n",
    "\n",
    "    def __post_init__(self):\n",
    "        # check feat lists are disjoint\n",
    "        L = [x for f in self.feat for x in f[-1]]\n",
    "        assert len(L) == len(set(L))\n",
    "\n",
    "        self.term_codes = listify(self.term_codes)\n",
    "        D = {'adm':False, 'reg':False, 'flg':False, 'raw':False, 'term':False, 'X':False, 'Y':False, 'Z':False, 'pred':False}\n",
    "        for x in ['overwrite','show']:\n",
    "            self[x] = D.copy() if self[x] is None else D.copy() | self[x]\n",
    "        self.overwrite['Z'] |= self.overwrite['X'] | self.overwrite['Y']\n",
    "        self.overwrite['raw'] |= self.overwrite['reg'] | self.overwrite['adm'] | self.overwrite['flg']\n",
    "        self.overwrite['term'] |= self.overwrite['raw']\n",
    "        self.path = root_path / f\"rslt/{rjust(self.cycle_day,3,0)}\"\n",
    "        self.rslt = self.path / f\"rslt.pkl\"\n",
    "        self.tune = self.path / f\"tune.pkl\"\n",
    "        try:\n",
    "            self.__dict__ = read(self.rslt).__dict__ | self.__dict__\n",
    "        except:\n",
    "            pass\n",
    "        for k, v in self.overwrite.items():\n",
    "            if v and k in self:\n",
    "                del self[k]\n",
    "        for k in ['pred']:\n",
    "            self[k] = self[k] if k in self else list()\n",
    "        for k in ['term']:\n",
    "            self[k] = self[k] if k in self else dict()\n",
    "\n",
    "        opts = {x:self[x] for x in ['cycle_day','overwrite','show']}\n",
    "        for nm in self.term_codes:\n",
    "            if nm not in self.term:\n",
    "                print(f'get {nm}')\n",
    "                self.term[nm] = TERM(term_code=nm, **opts).get_raw()\n",
    "        return self.dump()\n",
    "\n",
    "    def get_X(self):\n",
    "        nm = 'X'\n",
    "        if nm in self:\n",
    "            return self\n",
    "        print(f'get {nm}')\n",
    "        R = pd.concat([term.raw for term in self.term.values()], ignore_index=True).dropna(axis=1, how='all').prep()\n",
    "        repl = {'ae':0, 'n1':1, 'n2':2, 'n3':3, 'n4':4, 'r1':1, 'r2':2, 'r3':3, 'r4':4}\n",
    "        R['hs_qrtl'] = pd.cut(R['hs_pctl'], bins=[-1,25,50,75,90,101], labels=[4,3,2,1,0], right=False).combine_first(R['apdc_code'].map(repl))\n",
    "        R['camp_main'] = R['camp_code'] == 's'\n",
    "        R['distance'] = R['distance'].fillna(R['distance'].max())\n",
    "        R['majr_code'] = R['majr_code'].replace({'0000':'und', 'eled':'eted', 'agri':'unda'})\n",
    "        R['coll_code'] = R['coll_code'].replace({'ae':'an', 'eh':'ed', 'hs':'hl', 'st':'sm'})\n",
    "        R['coll_desc'] = R['coll_desc'].replace({\n",
    "            'ag & environmental sciences':'ag & natural resources',\n",
    "            'education & human development':'education',\n",
    "            'health science & human service':'health sciences',\n",
    "            'science & technology':'science & mathematics'})\n",
    "        majr = ['majr_desc','dept_code','dept_desc','coll_code','coll_desc']\n",
    "        S = R.sort_values('cycle_date').drop_duplicates(subset='majr_code', keep='last')[['majr_code',*majr]]\n",
    "        R = R.drop(columns=majr).merge(S, on='majr_code', how='left')\n",
    "\n",
    "        checks = {\n",
    "            'cycle_day': R['cycle_day']>=0,\n",
    "            'apdc_day' : R['apdc_day' ]>=R['cycle_day'],\n",
    "            'appl_day' : R['appl_day' ]>=R['apdc_day' ],\n",
    "            'birth_day':(R['birth_day']>=R['appl_day' ]) & (R['birth_day']>=5000),\n",
    "            'distance': R['distance']>=0,\n",
    "            'hs_pctl': (R['hs_pctl']>=0) & (R['hs_pctl']<=100),\n",
    "            'act_equiv': (R['act_equiv']>=1) & (R['act_equiv']<=36),\n",
    "            'gap_score': (R['gap_score']>=0) & (R['gap_score']<=100),\n",
    "        }\n",
    "        for k, mask in checks.items():\n",
    "            if (~mask).any():\n",
    "                R[~mask].disp(10)\n",
    "                raise Exception(f'check failed - {k}')\n",
    "        self[nm] = R\n",
    "        return self.dump()\n",
    "\n",
    "    def preprocess(self):\n",
    "        nm = 'Z'\n",
    "        if nm in self:\n",
    "            return self\n",
    "        self.get_X()\n",
    "        print(f'get {nm}')\n",
    "\n",
    "        trf = ColumnTransformer(self.feat, remainder='drop',verbose_feature_names_out = False)\n",
    "        where = lambda x: x.query(\"levl_code == 'ug' and styp_code in ('n','r','t')\").copy()\n",
    "        with warnings.catch_warnings(action='ignore'):\n",
    "            self.Y = [pd.concat([term.reg[k] for term in self.term.values()]).assign(credit_hr=lambda x:x['credit_hr'].fillna(0)>0) for k in [0,1]]\n",
    "            self.Z = trf.fit_transform(where(self.X).set_index(self.attr, drop=False)).rename(columns=lambda x:'_'+x)\n",
    "            self.Z.missing().disp(100)\n",
    "            for c in ['_hs_qrtl', '_act_equiv']:\n",
    "                self.Z[c+'_missing'] = self.Z[c].isnull()\n",
    "            self.Z = self.Z.prep().binarize().categorize()\n",
    "        agg = lambda y, g: where(y).groupby(g)[['credit_hr']].sum()\n",
    "        grp = ['levl_code','styp_code','term_code','crse']\n",
    "        end = agg(self.Y[0], grp)\n",
    "        \n",
    "        self.Y = [self.Z[[]].join(y.set_index(['pidm','term_code'])[['crse','credit_hr']], how='inner') for y in self.Y]\n",
    "        cur = agg(self.Y[0], grp)\n",
    "\n",
    "        M = (end / cur).query(\"term_code != @self.infer\")\n",
    "        N = M.reset_index().assign(term_code=self.infer).set_index(M.index.names)\n",
    "        self.mlt = pd.concat([M, N], axis=0).replace(np.inf, pd.NA).squeeze().rename('mlt').prep()\n",
    "        return self.dump()\n",
    "\n",
    "\n",
    "    def predict(self, crse='_total', styp_code='all', train_term=202208, iterations=3, opts=dict()):\n",
    "        print(crse,train_term,styp_code, end=': ')\n",
    "        prediction = {'meta': {'crse':crse, 'train_term':train_term, 'styp_code':styp_code, 'iterations':iterations, 'opts':opts.copy()}}\n",
    "        for P in self.pred:\n",
    "            if P['meta'] == prediction['meta']:\n",
    "                print('reusing')\n",
    "                return P\n",
    "        print(f'creating')\n",
    "        # d = {'_total_cur':1, crse+'_cur':1, crse+'_end':0}\n",
    "        d = {crse+'_cur':1, crse+'_end':0,}\n",
    "        end = {c:c[:-4] for c, i in d.items() if i==0}\n",
    "        Y = pd.concat([self.Y[i].query(\"crse == @crse\").rename(columns={'credit_hr':c})[c] for c, i in d.items()], axis=1, join='outer')\n",
    "        T = self.Z.join(Y, how='left').fillna({c:False for c in d.keys()})\n",
    "        if styp_code != \"all\":\n",
    "            T = T.query(\"styp_code==@styp_code\")\n",
    "        T.loc[T.eval(\"term_code==@self.infer\"), end.keys()] = pd.NA\n",
    "        X = T.copy()\n",
    "        # qry = \"term_code!=@train_term\"\n",
    "        qry = \"term_code==@train_term\"\n",
    "        X.loc[X.eval(qry), end.keys()] = pd.NA\n",
    "        model = ImputationKernel(X, **opts)\n",
    "        model.mice(iterations)\n",
    "        # with warnings.catch_warnings(action='ignore'):\n",
    "        #     imp.plot_imputed_distributions(wspace=0.2,hspace=0.4)\n",
    "            # assert 1==2\n",
    "        #     imp.plot_mean_convergence()#wspace=0.3, hspace=0.4)\n",
    "        #     # imp.plot_correlations()\n",
    "\n",
    "        g = lambda df, nm=None: df[end.keys()].rename(columns=end).melt(ignore_index=False, var_name='crse', value_name=nm).set_index('crse', append=True)\n",
    "        P = pd.concat([model.complete_data(k).assign(sim=k).set_index('sim', append=True) for k in range(model.dataset_count())])\n",
    "        Y = g(P,'pred').join(g(T,'true')).assign(train_term=train_term).query(qry).prep()\n",
    "        grp = ['crse','styp_code','term_code','train_term','sim']\n",
    "        agg = lambda x: pd.Series({\n",
    "            'pred': x['pred'].sum(min_count=1),\n",
    "            'true': x['true'].sum(min_count=1),\n",
    "            'mse%': ((1*x['pred'] - x['true'])**2).mean()*100,\n",
    "            'f1_inv%': (1-f1_score(x.dropna()['true'], x.dropna()['pred'], zero_division=np.nan))*100,\n",
    "        })\n",
    "        S = Y.groupby(grp).apply(agg).join(self.mlt).rename_axis(index={'term_code':'pred_term'})\n",
    "        for x in ['pred','true']:\n",
    "            S[x] = S[x] * S['mlt']\n",
    "        S.insert(2, 'err', S['pred'] - S['true'])\n",
    "        S.insert(3, 'err%', (S['err'] / S['true']).clip(-1, 1) * 100)\n",
    "        prediction['rslt'] = {'X':X,'T':T,'P':P,'model':model, 'full':Y, 'summary': S.drop(columns='mlt').prep()}\n",
    "        self.pred.append(prediction)\n",
    "        self.dump()\n",
    "        return prediction\n",
    "\n",
    "\n",
    "    def train(self, styp_codes=('n','r','t'), train_terms=None, iterations=3, opts=dict()):\n",
    "        train_terms = self.term_codes if train_terms is None else train_terms\n",
    "        def pivot(df, val, q=50):\n",
    "            Y = df.reset_index().pivot_table(columns='train_term', index='pred_term', values=val, aggfunc=pctl(q))\n",
    "            for _ in range(2):\n",
    "                mr = Y.mean(axis=1)\n",
    "                ma = Y.abs().mean(axis=1)\n",
    "                Y = (Y.assign(mean=mr, abs_mean=ma) if Y.shape[1] > 1 else Y).T\n",
    "            return Y.assign(**{val:f\"{q}%\"}).set_index(val, append=True).swaplevel(0,1).round(2).prep().T\n",
    "        \n",
    "        def analyze(df):\n",
    "            r = {stat: pivot(df.query(f\"pred_term!={self.infer}\"), stat) for stat in [\"err\",\"err%\",\"mse%\",\"f1_inv%\"]}\n",
    "            r['proj'] = pd.concat([pivot(df.query(f\"pred_term=={self.infer}\"), \"pred\", q) for q in [25,50,75]], axis=1)\n",
    "            return r\n",
    "\n",
    "        P = {(crse, styp_code, train_term): self.predict(crse, styp_code, train_term, iterations, opts) for crse in self.crse for styp_code in listify(styp_codes) for train_term in listify(train_terms)}\n",
    "        R = dict()\n",
    "        for k,v in P.items():\n",
    "            R.setdefault(k[1]=='all', []).append(v)\n",
    "\n",
    "        for b, L in R.items():\n",
    "            v = {k: pd.concat([Y['rslt'][k] for Y in L]) for k in ['full','summary']}\n",
    "            v['opts'] = opts.copy()\n",
    "            v['rslt'] = {g: analyze(df) for g, df in v['summary'].groupby(['crse', 'styp_code'])}\n",
    "            R[b] = v\n",
    "        return R\n",
    "\n",
    "\n",
    "code_desc = lambda x: [x+'_code', x+'_desc']\n",
    "simpimp = lambda fill: SimpleImputer(strategy='constant', fill_value=fill, missing_values=pd.NA)\n",
    "kwargs = {\n",
    "    'attr': [\n",
    "        'pidm',\n",
    "        *code_desc('term'),\n",
    "        *code_desc('apdc'),\n",
    "        *code_desc('levl'),\n",
    "        *code_desc('styp'),\n",
    "        *code_desc('admt'),\n",
    "        *code_desc('camp'),\n",
    "        *code_desc('coll'),\n",
    "        *code_desc('dept'),\n",
    "        *code_desc('majr'),\n",
    "        *code_desc('cnty'),\n",
    "        *code_desc('stat'),\n",
    "        *code_desc('natn'),\n",
    "        'resd',\n",
    "        'legacy',\n",
    "        'gender',\n",
    "        *[f'race_{r}' for r in ['american_indian','asian','black','pacific','white','hispanic']],\n",
    "        'waiver',\n",
    "        'birth_day',\n",
    "        'distance',\n",
    "        'hs_pctl',\n",
    "    ],\n",
    "    'feat': [\n",
    "        ('scl', make_pipeline(StandardScaler(), PowerTransformer()), [\n",
    "            'distance',\n",
    "            'birth_day',\n",
    "            # 'gap_score',\n",
    "            # 'hs_pctl',\n",
    "            'act_equiv',\n",
    "        ]),\n",
    "        ('pass', 'passthrough', [\n",
    "            'gender',\n",
    "            # 'styp_code',\n",
    "            # 'camp_code',\n",
    "            # 'coll_code',\n",
    "            # 'verified',\n",
    "            # 'term_code',\n",
    "            'appl_day',\n",
    "            'apdc_day',\n",
    "            'hs_qrtl',\n",
    "        ]),\n",
    "        ('false', simpimp(False), [\n",
    "            'camp_main',\n",
    "            'resd',\n",
    "            'legacy',\n",
    "            *[f'race_{r}' for r in ['american_indian','asian','black','pacific','white','hispanic']],\n",
    "            'waiver',\n",
    "            # 'fafsa_app',\n",
    "            'schlship_app',\n",
    "            # 'finaid_accepted',\n",
    "            'ssb',\n",
    "            'math',\n",
    "            'reading',\n",
    "            'writing',\n",
    "        ]),\n",
    "        ('0', simpimp(0), [\n",
    "            'gap_score',\n",
    "        ]),\n",
    "        ('n', simpimp('n'), [\n",
    "            'oriented',\n",
    "        ]),\n",
    "    ],\n",
    "    'infer': 202408,\n",
    "    'cycle_day': (TERM(term_code=202408).cycle_date-pd.Timestamp.now()).days+1,\n",
    "    # 'cycle_day': 197,\n",
    "    'term_codes': np.arange(2020,2025)*100+8,\n",
    "    'crse': [\n",
    "        '_total',\n",
    "        # 'engl1301',\n",
    "        # 'biol1406',\n",
    "        # 'biol2401',\n",
    "        # 'math1314',\n",
    "        # 'math2412',\n",
    "        # 'agri1419',\n",
    "        # 'psyc2301',\n",
    "        # 'ansc1319',\n",
    "        # 'comm1311',\n",
    "        # 'hist1301',\n",
    "        # 'govt2306',\n",
    "        # 'math1324',\n",
    "        # 'chem1411',\n",
    "        # 'univ0301',\n",
    "        # 'univ0204',\n",
    "        # 'univ0304',\n",
    "        # 'agri1100',\n",
    "        # 'comm1315',\n",
    "        # 'agec2317',\n",
    "        # 'govt2305',\n",
    "        # 'busi1301',\n",
    "        # 'arts1301',\n",
    "        # 'math1342',\n",
    "        # 'math2413',\n",
    "        ],\n",
    "    'overwrite': {\n",
    "        # 'reg':True,\n",
    "        # 'adm':True,\n",
    "        # 'flg':True,\n",
    "        # 'raw':True,\n",
    "        # 'term': True,\n",
    "        # 'X': True,\n",
    "        # 'Y': True,\n",
    "        # 'Z': True,\n",
    "        'pred': True,\n",
    "    },\n",
    "    'show': {\n",
    "        # 'reg':True,\n",
    "        # 'adm':True,\n",
    "    },\n",
    "    # 'sch': False,\n",
    "}\n",
    "# FLAGS().run()\n",
    "self = AMP(**kwargs)\n",
    "self = self.preprocess()\n",
    "self.term_codes.remove(self.infer)\n",
    "iterations = 3\n",
    "\n",
    "opts = dict()\n",
    "opts['random_state'] = 42\n",
    "opts['save_all_iterations'] = False\n",
    "opts['datasets'] = 5\n",
    "opts['mean_match_candidates'] = 10\n",
    "opts['mean_match_function'] = mean_match_kdtree_classification\n",
    "\n",
    "# # opts['datasets'] = 2\n",
    "# # opts['mean_match_candidates'] = 1\n",
    "# # opts['mean_match_function'] = default_mean_match\n",
    "\n",
    "# P = self.predict(opts=opts)\n",
    "\n",
    "R = self.train(iterations=iterations, opts=opts,\n",
    "    styp_codes='n',\n",
    "    # train_terms=202208,\n",
    "    )\n",
    "# for k in R[False]['rslt'].keys():\n",
    "#     for b, v in R.items():\n",
    "#         print(k, b)\n",
    "#         v['rslt'][k]['err%'].disp(100)\n",
    "\n",
    "# tune = []\n",
    "# for func in [mean_match_kdtree_classification, default_mean_match]:\n",
    "#     opts['mean_match_function'] = func\n",
    "#     for cand in range(2,41,3):\n",
    "#         opts['mean_match_candidates'] = cand\n",
    "#         print(sort(opts))\n",
    "#         R = self.train(\n",
    "#             styp_codes='n',\n",
    "#             iterations=iterations,\n",
    "#             opts=opts)\n",
    "#         R[False]['rslt']['_total','n']['err%'].disp(100)\n",
    "#         tune.append(R)\n",
    "#         write(self.tune, tune)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = None\n",
    "match x:\n",
    "    case 2:\n",
    "        print(2)\n",
    "    case 10:\n",
    "        print(11)\n",
    "    case None:\n",
    "        print('hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.X['styp_code'].mode()\n",
    "# self.Z['_birth_day']['median']()\n",
    "df = pd.DataFrame()\n",
    "df['a'] = [1,1,2,2]\n",
    "df['b'] = ['a','a','a','a',]\n",
    "df.mode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute(df, col, val=None, grp=None):\n",
    "    val = val if val is not None else 'median' if pd.api.types.is_numeric_dtype(df[col]) else 'mode'\n",
    "    if val in ['median']:\n",
    "        func = lambda x: x.median()\n",
    "    elif val in ['mean','ave','avg','average']:\n",
    "        func = lambda x: x.mean()\n",
    "    elif val in ['mode','most_frequent']:\n",
    "        func = lambda x: x.mode()[0]\n",
    "    else:\n",
    "        func = lambda x: val\n",
    "    df[col] = (df if grp is None else df.groupby(grp))[col].transform(lambda x: x.fillna(func(x)))\n",
    "    return df\n",
    "pd.DataFrame.impute = impute\n",
    "\n",
    "self.Z.reset_index(drop=True)\n",
    "A = self.Z.copy()\n",
    "c = '_birth_day'\n",
    "mask = A[c].isnull()\n",
    "# A.impute('_birth_day', val='median', grp=['term_code','styp_code'])\n",
    "A.impute('_birth_day', val=np.nan, grp=['term_code','styp_code'])\n",
    "A.loc[mask,c].disp(5)\n",
    "# A.groupby(['term_code','styp_code'])['_birth_day'].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ".102924"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = self.pred[0]\n",
    "R = P['rslt']\n",
    "self.Z.dtypes\n",
    "# R['P'].dtypes#.values.astype(float)\n",
    "# model = self.pred[0]['rslt']['model']\n",
    "# model.feature_importance_df()\n",
    "# model.plot_correlations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.pred[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write(self.path / 'predictions.csv', R[False]['summary'])\n",
    "write(self.path / 'predictions.parq', R[False]['summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R = pd.concat([term.raw for term in self.term.values()]).dropna(axis=1, how='all').reset_index(drop=True).prep()\n",
    "repl = {'ae':0, 'n1':1, 'n2':2, 'n3':3, 'n4':4, 'r1':1, 'r2':2, 'r3':3, 'r4':4}\n",
    "R['hs_qrtl'] = pd.cut(R['hs_pctl'], bins=[-1,25,50,75,90,101], labels=[4,3,2,1,0], right=False).combine_first(R['apdc_code'].map(repl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R = pd.concat([term.raw for term in self.term.values()]).dropna(axis=1, how='all').reset_index(drop=True).prep()\n",
    "repl = {'ae':0, 'n1':1, 'n2':2, 'n3':3, 'n4':4, 'r1':1, 'r2':2, 'r3':3, 'r4':4}\n",
    "# R['hs_qrtl'] = \n",
    "R['A'] = pd.cut(R['hs_pctl'], bins=[-1,25,50,75,90,101], labels=[4,3,2,1,0], right=False)\n",
    "R['B'] = R['apdc_code'].map(repl)\n",
    "A\n",
    "# R['hs_qrtl'] = R['A'].combine_first(R['B'])\n",
    "# pd.concat([A,B],axis=1)\n",
    "R\n",
    "# A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.head('stvapdc', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "where = lambda x: x.query(\"levl_code == 'ug' and styp_code in ('n','r','t')\").copy()\n",
    "idx = ['pidm','styp_code','apdc_code','apdc_desc']\n",
    "# P = self.X.set_index(idx)[['hs_pctl']]\n",
    "P = where(self.X).filter([*idx, 'hs_pctl'])\n",
    "repl = {\n",
    "    # 'a2':pd.NA,\n",
    "    # 'aa':pd.NA,\n",
    "    # 'ac':pd.NA,\n",
    "    # 'ad':pd.NA,\n",
    "    'ag':pd.NA,\n",
    "    'ai':pd.NA,\n",
    "    'at':pd.NA,\n",
    "    'ae':0,\n",
    "    'n1':1,\n",
    "    'n2':2,\n",
    "    'n3':3,\n",
    "    'n4':4,\n",
    "    'r1':1,\n",
    "    'r2':2,\n",
    "    'r3':3,\n",
    "    'r4':4,\n",
    "}\n",
    "repl = {'ae':0, 'n1':1, 'n2':2, 'n3':3, 'n4':4, 'r1':1, 'r2':2, 'r3':3, 'r4':4}\n",
    "\n",
    "# bins = [100,89.9,74.9,49.9,24.9,0]\n",
    "# bool, default False\n",
    "repl = {'ae':0, 'n1':1, 'n2':2, 'n3':3, 'n4':4, 'r1':1, 'r2':2, 'r3':3, 'r4':4}\n",
    "P['hs_qrtl'] = pd.cut(P['hs_pctl'], right=False, bins=[-1,25,50,75,90,101], labels=[4,3,2,1,0]).combine_first(P['apdc_code'].map(repl))\n",
    "# P.query('hs_qrtl==2')\n",
    "# P.query(\"apdc_code=='n2'\")\n",
    "# P.vc(['apdc_desc','hs_qrtl']).disp(200)\n",
    "# Q = P.query(\"hs_qrtl.isnull()\")\n",
    "# P.groupby(['apdc_code','hs_qrtl']).size()\n",
    "P.groupby(['apdc_code','apdc_desc'])['hs_qrtl'].value_counts(normalize=True, dropna=False).round(2).sort_index().to_frame().disp(200)\n",
    "# Q.vc(['styp_code','apdc_code','apdc_desc']).disp(200)\n",
    "# P['hs_qrtl'].isnull().sum()\n",
    "# P.query(\"hs_qrtl.isnull()\").vc('apdc_desc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = [100,89.9,74.9,49.9,24.9,0]\n",
    "np.arange(4,-1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# P.query(\"apdc_code=='n2' & hs_pctl.notnull()\" ).disp(2000)\n",
    "P.query(\"apdc_code=='n2'\" ).disp(2000)\n",
    "# P.query(\"apdc_code=='n2'\").vc('hs_qrtl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P.query(\"apdc_desc=='admitted (nr1)' & hs_qrtl==2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P.query('hs_pctl.isnull()').vc('apdc_desc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repl = {\n",
    "    'a2':pd.NA,\n",
    "    'aa':pd.NA,\n",
    "    'ac':pd.NA,\n",
    "    'ad':pd.NA,\n",
    "    'ae':5,\n",
    "    'ag':pd.NA,\n",
    "    'ai':pd.NA,\n",
    "    'at':pd.NA,\n",
    "    'n1':1,\n",
    "    'n2':3,\n",
    "    'n3':4,\n",
    "    'n4':4,\n",
    "    'r1':1,\n",
    "    'r2':2,\n",
    "    'r3':3,\n",
    "    'r4':4,\n",
    "}\n",
    "P['q'] = P['']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P.vc(['apdc_code','apdc_desc'])\n",
    "# {'n1':1}\n",
    "# set(P.reset_index()['apdc_code'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# self.Z.filter(like='_hs_pctl').query('_hs_pctl.isnull()').vc('apdc_desc')\n",
    "# # self.X.groupby('apdc_desc')['hs_pctl'].describe()\n",
    "# P = self.X[['apdc_desc','hs_pctl']]\n",
    "# pd.cut(self.X['hs_pctl'],4)\n",
    "# P = pd.cut(self.X.set_index(['pidm','apdc_desc'])['hs_pctl'], bins=[-1,25,50,75,100], labels=[1,2,3,4])\n",
    "P.vc(['apdc_desc','hs_qrtl']).disp(200)\n",
    "# P.groupby('apdc_desc').describe()\n",
    "# (P==2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R[False]['rslt'][('_total', 'n')]['proj']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = A['summary'].query(\"pred_term!=202408 & styp_code=='n' & pred_term!=train_term\")#['err%']\n",
    "import seaborn as sns\n",
    "sns.boxplot(M, hue='train_term', y='err%', x='pred_term',\n",
    "    # fill=False,\n",
    "    whis=(0, 100),\n",
    "    dodge = True,\n",
    "    palette='tab10',\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.pred[0]['rslt']['Pmodel'].feature_importance_df().sort_values('_total_end', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R[False]['rslt']['_total','n']['err%']\n",
    "R[False]['rslt']['_total','n'].keys()#['model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P['rslt']['model'].feature_importance_df().sort_values('_total_end', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_importance_df(self, dataset, normalize=True, iteration=None):\n",
    "    imputed_var_names = [self._get_variable_name(int(i)) for i in np.sort(self.imputation_order)]\n",
    "    predictor_var_names = [self._get_variable_name(int(i)) for i in np.sort(self.predictor_vars)]\n",
    "    I = pd.DataFrame(self.get_feature_importance(datset, iteration), index=imputed_var_names, columns=predictor_var_names).T\n",
    "    return I / I.sum() if normalize else I\n",
    "ImputationKernel.feature_importance_df = feature_importance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.plot_feature_importance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = self.pred[0]['rslt']['model']\n",
    "# model.plot_feature_importance??\n",
    "imputed_var_names = [model._get_variable_name(int(i)) for i in np.sort(model.imputation_order)]\n",
    "predictor_var_names = [model._get_variable_name(int(i)) for i in np.sort(model.predictor_vars)]\n",
    "# model.\n",
    "c = '_total_end'\n",
    "I = pd.DataFrame(model.get_feature_importance(0), index=imputed_var_names, columns=predictor_var_names).T\n",
    "I *= 100 / I.sum()\n",
    "I[c].sort_values(ascending=False)\n",
    "# I.T['_total_end']\n",
    "# (0).shape\n",
    "#(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R[False]['rslt']['_total','n'].keys()\n",
    "\n",
    "# ['rslt']['_total','n'].keys()\n",
    "# model = R[False]['rslt']['_total','n']\n",
    "#['model']\n",
    "# model.plot_feature_importance?\n",
    "# (dataset=0, annot=True,cmap=\"YlGnBu\",vmin=0, vmax=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum((len(f[-1]) for f in self.feat))\n",
    "L = [x for f in self.feat for x in f[-1]]\n",
    "len(L), len(set(L))\n",
    "# {x for f in self.feat for x in f[-1]}\n",
    "# {*self.feat[0][-1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F = read('/home/scook/institutional_data_analytics/admitted_matriculation_projection/LiveAMP/flags/parq/flg_202308.parq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F.columns\n",
    "F['styp_code']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# self.Z.isnull().sum().sort_index().disp(1000)\n",
    "# self.Z.dtypes\n",
    "# .vc('oriented')\n",
    "# hs_pctlact_equiv\n",
    "mask = self.Z['birth_day'].isnull()\n",
    "self.Z[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qry = \"select * from spbpers where spbpers_pidm=1115874\"\n",
    "db.execute(qry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.Z.select_dtypes('string')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.Z.isnull().sum().sort_values(ascending=False).to_frame('missing').query('missing>0')\n",
    "# self.Z.vc('writing')\n",
    "# self.Z.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "feat = [\n",
    "    ('scl', make_pipeline(StandardScaler(), PowerTransformer()), [\n",
    "        'distance',\n",
    "        'birth_day',\n",
    "    ]),\n",
    "    # ('nom', FunctionTransformer(lambda x: x.astype('category')), [\n",
    "    ('nom', 'passthrough', [\n",
    "        'gender',\n",
    "        'oriented',\n",
    "        'styp_code',\n",
    "        # 'camp_code',\n",
    "        'coll_code',\n",
    "        # 'verified',\n",
    "    ]),\n",
    "    ('pass', 'passthrough', [\n",
    "        'term_code',\n",
    "        'math',\n",
    "        'reading',\n",
    "        'writing',\n",
    "        'hs_pctl',\n",
    "        'appl_day',\n",
    "        'apdc_day',\n",
    "        'act_equiv',\n",
    "    ]),\n",
    "    ('false', SimpleImputer(strategy='constant', fill_value=False), [\n",
    "        'camp_main',\n",
    "        'resd',\n",
    "        'legacy',\n",
    "        *[f'race_{r}' for r in ['american_indian','asian','black','pacific','white','hispanic']],\n",
    "        'waiver',\n",
    "        # 'fafsa_app',\n",
    "        'schlship_app',\n",
    "        # 'finaid_accepted',\n",
    "        'ssb',\n",
    "    ]),\n",
    "    ('0', SimpleImputer(strategy='constant', fill_value=0), [\n",
    "        'gap_score',\n",
    "    ]),\n",
    "    # ('n', SimpleImputer(strategy='constant', fill_value='n'), [\n",
    "    #     'oriented',\n",
    "    # ]),\n",
    "\n",
    "]\n",
    "\n",
    "# trf = make_pipeline(ColumnTransformer(feat,remainder='drop',verbose_feature_names_out = False), ft)\n",
    "# trf = ColumnTransformer(feat,remainder='drop',verbose_feature_names_out = False)\n",
    "# Z = trf.fit_transform(self.X).binarize()\n",
    "# # Z = Z.apply(f)\n",
    "# # Z.isnull().sum()\n",
    "# Z.dtypes\n",
    "self.X.fillna({c:'' for c in self.X.select_dtypes('string').columns}, inplace=True)\n",
    "self.X.select_dtypes('string').isnull().sum().disp(300)\n",
    "# self.X.select_dtypes('string').fillna('')\n",
    "# self.X.select_dtypes('string').isnull().sum()\n",
    "\n",
    "# .fillna('')\n",
    "# Z\n",
    "# pd.api.types.is_string_dtype(Z['gender'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.Z.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.head('opeir.admissions_fall2022',2).T.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.Z.waiver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.X.query(\"waiver.isnull()\").vc(['cycle_day'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.X.isnull().sum().disp(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R[False]['rslt']['_total','n']['err%'].disp(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in R[False]['rslt'].keys():\n",
    "    for b, v in R.items():\n",
    "        print(k, b)\n",
    "        v['rslt'][k]['err%'].disp(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {k:v for k,v in R.items() if k[1]!='all'}.keys()\n",
    "R = {True:[], False:[]}\n",
    "for k,v in P.items():\n",
    "    R[k[1]=='all'].append(v)\n",
    "# q[True][0]['rslt'].keys()\n",
    "# for b,L in R.items():\n",
    "    # print(type(v))\n",
    "    # print(v[0]['rslt'].keys())\n",
    "\n",
    "S = {b: {k: pd.concat([Y['rslt'][k] for Y in L]) for k in ['full','summary']} for b,L in R.items()}\n",
    "S[False]['summary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # def predict(self, crse='_total', train_term=202208, iterations=3, opts=dict()):\n",
    "    #     for styp_code in [\"n\",\"r\",\"t\",\"all\"]:\n",
    "    #         print(crse,train_term,styp_code, end=\": \")\n",
    "    #         prediction = {'meta': {'crse':crse, 'train_term':train_term, 'styp_code':styp_code, 'iterations':iterations, 'opts':opts.copy()}}\n",
    "    #         for P in self.pred:\n",
    "    #             if P['meta'] == prediction['meta']:\n",
    "    #                 print('reusing')\n",
    "    #                 return P\n",
    "    #         print(f'creating')\n",
    "\n",
    "    #         d = {'_total_cur':1, crse+'_cur':1, crse+'_end':0}\n",
    "    #         end = {c:c[:-4] for c, i in d.items() if i==0}\n",
    "    #         Y = pd.concat([self.Y[i].query(\"crse == @crse\").rename(columns={'credit_hr':c})[c] for c, i in d.items()], axis=1, join='outer')\n",
    "    #         T = self.Z.join(Y, how='left').fillna({c:False for c in d.keys()})\n",
    "    #         if styp_code != \"all\":\n",
    "    #             T = T.query(\"styp_code==@styp_code\")\n",
    "    #         X = T.copy()\n",
    "    #         X.loc[X.eval(\"term_code!=@train_term or term_code==@self.infer\"), end.keys()] = pd.NA\n",
    "    #         imp = ImputationKernel(X, **opts)\n",
    "    #         imp.mice(iterations)\n",
    "    #         # with warnings.catch_warnings(action='ignore'):\n",
    "    #         #     imp.plot_imputed_distributions(wspace=0.2,hspace=0.4)\n",
    "    #         #     imp.plot_mean_convergence()#wspace=0.3, hspace=0.4)\n",
    "    #         #     # imp.plot_correlations()\n",
    "\n",
    "    #         g = lambda df, nm=None: df[end.keys()].rename(columns=end).melt(ignore_index=False, var_name='crse', value_name=nm).set_index('crse', append=True)\n",
    "    #         P = pd.concat([imp.complete_data(k).assign(sim=k).set_index('sim', append=True) for k in range(imp.dataset_count())])\n",
    "    #         Y = g(P,'pred').join(g(T,'true')).assign(train_term=train_term).query('term_code != train_term').prep()\n",
    "    #         grp = ['crse','styp_code','term_code','train_term','sim']\n",
    "    #         agg = lambda x: pd.Series({\n",
    "    #             'pred': x['pred'].sum(min_count=1),\n",
    "    #             'true': x['true'].sum(min_count=1),\n",
    "    #             'mse%': ((1*x['pred'] - x['true'])**2).mean()*100,\n",
    "    #             'f1_inv%': (1-f1_score(x.dropna()['true'], x.dropna()['pred'], zero_division=np.nan))*100,\n",
    "    #         })\n",
    "    #         S = Y.groupby(grp).apply(agg).join(self.mlt).rename_axis(index={'term_code':'pred_term'})\n",
    "    #         for x in ['pred','true']:\n",
    "    #             S[x] = S[x] * S['mlt']\n",
    "    #         S.insert(2, 'err', S['pred'] - S['true'])\n",
    "    #         S.insert(3, 'err%', (S['err'] / S['true']).clip(-1, 1) * 100)\n",
    "    #         prediction['rslt'] = {'full':Y, 'summary': S.drop(columns='mlt').prep()}\n",
    "    #         self.pred.append(prediction)\n",
    "    #         self.dump()\n",
    "    #     return prediction\n",
    "\n",
    "# class MM():\n",
    "#     def __init__(self, func, candidates):\n",
    "#         assert func in [mean_match_kdtree_classification, default_mean_match]\n",
    "#         self.func = func\n",
    "#         self.candidates = candidates\n",
    "#     def __call__(self, *args, **kwargs):\n",
    "#         return self.func(*args, **kwargs)\n",
    "#     def __str__(self):\n",
    "#         return join([x for x in ['kdtree','default'] if x in self.func.__name__]+[self.candidates], \"_\")\n",
    "\n",
    "# class kdtree():\n",
    "#     def __call__(self, *args, **kwargs):\n",
    "#         return mean_match_kdtree_classification(*args, **kwargs)\n",
    "#     def __str__(self):\n",
    "#         return 'kdtree__mean_match'\n",
    "\n",
    "# class default():\n",
    "#     def __call__(self, *args, **kwargs):\n",
    "#         return default_mean_match(*args, **kwargs)\n",
    "#     def __str__(self):\n",
    "#         return 'default_mean_match'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = it.product(self.crse, ['n','r','t','all'])\n",
    "[[crse, styp_code, train_term] for crse, styp_code in Z for train_term in self.term_codes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = it.product(self.crse, ['n','r','t','all'])\n",
    "list(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {\n",
    "    ('a','b'):7,\n",
    "    ('a','c'):71}\n",
    "d['a','c']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qry = f\"\"\"\n",
    "select\n",
    "        A.sfrstcr_term_code,\n",
    "        A.sfrstcr_pidm,\n",
    "        B.ssbsect_subj_code,\n",
    "        B.ssbsect_crse_numb,\n",
    "        B.ssbsect_credit_hrs,\n",
    "        A.sfrstcr_credit_hr\n",
    "from sfrstcr A, ssbsect B\n",
    "where\n",
    "        A.sfrstcr_term_code = B.ssbsect_term_code\n",
    "        and A.sfrstcr_crn = B.ssbsect_crn\n",
    "        and A.sfrstcr_term_code = 202308\n",
    "        and A.sfrstcr_ptrm_code not in ('28','R3')\n",
    "        and  trunc(to_date('18-Sep-23')) - trunc(A.sfrstcr_add_date) >= 197  -- added before cycle_day\n",
    "        and (trunc(to_date('18-Sep-23')) - trunc(A.sfrstcr_rsts_date) < 197 or A.sfrstcr_rsts_code in ('DC','DL','RD','RE','RW','WD','WF')) -- dropped after cycle_day or still enrolled\n",
    "        and B.ssbsect_subj_code <> 'INST'\n",
    "        and A.sfrstcr_credit_hr <> B.ssbsect_credit_hrs\n",
    "\"\"\"\n",
    "db.head(qry, show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qry = f\"styp_code=='n' & pred_term!={self.infer}\"\n",
    "val = \"err%\"\n",
    "q=50\n",
    "P = A['summary'].reset_index().query(qry).pivot_table(columns='train_term', index='pred_term', values=val, aggfunc=pctl(q))\n",
    "for _ in range(2):\n",
    "    P = (P.assign(mean=lambda x:x.mean(axis=1)) if P.shape[1] > 1 else P).T\n",
    "P.assign(**{val:f\"{q}%\"}).set_index(val, append=True).swaplevel(0,1).round(0).prep().T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kdtree = mean_match_kdtree_classification\n",
    "kdtree.__name__ = 'a'\n",
    "setattr(kdtree,'__str__','a')\n",
    "setattr(kdtree,'__repr__','a')\n",
    "\n",
    "print(kdtree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from LiveAMP import *\n",
    "from miceforest.mean_matching_functions import default_mean_match, mean_match_kdtree_classification\n",
    "class MM():\n",
    "    def __init__(self, func, candidates):\n",
    "        self.func = func\n",
    "        self.candidates = candidates\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return self.func(*args, **kwargs)\n",
    "    def __str__(self):\n",
    "        return join([x for x in ['kdtree','deafult'] if x in self.func.__name__]+[self.candidates], \"_\")\n",
    "\n",
    "mm = MM(mean_match_kdtree_classification, 3)\n",
    "print(mm)\n",
    "# type(mean_match_kdtree_classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_match_kdtree_classification.__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = default_mean_match\n",
    "x.__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "            # A[styp_code] = {\n",
    "            #     'proj': pd.concat([pivot(f\"styp_code=='{styp_code}' & pred_term=={self.infer}\", \"pred\", q) for q in [25,50,75]], axis=1),\n",
    "            #     **{stat: pivot(f\"styp_code=='{styp_code}' & pred_term!={self.infer}\", stat) for stat in [\"err\",\"err%\",\"mse%\",\"f1_inv%\"]}\n",
    "\n",
    "\n",
    "\n",
    "    # R = {styp_code: {\n",
    "    #         'proj': pd.concat([pivot(f\"styp_code=='{styp_code}' & pred_term=={self.infer}\", \"pred\", q) for q in [25,50,75]], axis=1),\n",
    "    #         **{stat: pivot(f\"styp_code=='{styp_code}' & pred_term!={self.infer}\", stat) for stat in [\"err\",\"err%\",\"mse%\",\"f1_inv%\"]}\n",
    "    #     } for styp_code in [\"n\"]}\n",
    "\n",
    "        # R['n']['proj'].disp(100)\n",
    "        # R['n']['err%'].disp(100)\n",
    "# B = (\n",
    "#     A['summary']\n",
    "#     .grpby(['crse','styp_code','train_term','pred_term'])\n",
    "#     # .grpby(['crse','styp_code','pred_term'])\n",
    "#     [['pred','err%','mse%','f1_inv%']]\n",
    "#     .agg(summary)\n",
    "#     .stack(0, sort=False)\n",
    "#     .rename_axis(index={None:'kind'})\n",
    "#     .query(f\"(pred_term == {self.infer} and kind == 'pred') or (pred_term != {self.infer} and kind == 'err%')\")\n",
    "#     .reset_index()\n",
    "#     # .sort_values(['crse','styp_code','pred_term','train_term'],ascending=[True,True,False,False])\n",
    "#     .prep()\n",
    "# )\n",
    "# M = A['summary'].query(\"pred_term != @self.infer & styp_code=='n'\").pivot_table(index='train_term', columns='pred_term', values='err%', margins=True)\n",
    "# M.disp(10)\n",
    "# B.disp(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(self.pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R['n']['err%'].disp(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A['summary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = {k: pd.concat([p['rslt'][k] for p in P]) for k in ['full','summary']}\n",
    "def pivot(qry, val, q=50):\n",
    "    P = A['summary'].reset_index().query(qry).pivot_table(columns='train_term', index='pred_term', values=val, aggfunc=pctl(q), margins=True, margins_name='mean')\n",
    "    for _ in range(2):\n",
    "        P = (P.head(1) if P.shape[0] == 2 else P).T\n",
    "    return P.assign(**{val:f\"{q}%\"}).set_index(val, append=True).swaplevel(0,1).round(0).prep().T\n",
    "\n",
    "R = {styp_code: {\n",
    "    'proj': pd.concat([pivot(f\"styp_code=='{styp_code}' & pred_term=={self.infer}\", \"pred\", q) for q in [25,50,75]], axis=1),\n",
    "    **{stat: pivot(f\"styp_code=='{styp_code}' & pred_term!={self.infer}\", stat) for stat in [\"err\",\"err%\",\"mse%\",\"f1_inv%\"]}\n",
    "} for styp_code in [\"n\"]}\n",
    "\n",
    "R['n']['proj'].disp(100)\n",
    "R['n']['err%'].disp(100)\n",
    "# }}\n",
    "# projections = pd.concat([piv(\"pred_term == @self.infer & styp_code=='n'\", 'pred', q) for q in [25,50,75]], axis=1)\n",
    "# errors = piv(\"pred_term != @self.infer & styp_code=='n'\", 'err%', 50)\n",
    "# Q\n",
    "# M\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g(p):\n",
    "    f = lambda x: x.quantile(p/100)\n",
    "    f.__name__ = f'{p}%'\n",
    "    f.__str__ = f'{p}%'\n",
    "    f.__repr__ = f'{p}%'\n",
    "    return f\n",
    "print(f\"{g(25)}\")\n",
    "display(f)\n",
    "str(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = pctl(50)\n",
    "f.__repr__ = 'a'\n",
    "f.__str__ = 'a'\n",
    "f'{f}'\n",
    "# print(f)\n",
    "# f.__qualname__\n",
    "# print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = pctl(50)\n",
    "hasattr(w, '__name__')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 'hi'\n",
    "# x.__name__ = x\n",
    "hasattr(x, '__name__')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A['summary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def piv(qry, val, q=50):\n",
    "    P = A['summary'].reset_index().query(qry).pivot_table(columns='train_term', index='pred_term', values=val, aggfunc=pctl(q), margins=True, margins_name='mean')\n",
    "    for _ in range(2):\n",
    "        P = (P.head(1) if P.shape[0] == 2 else P).T\n",
    "    return P.assign(**{val:f\"{q}%\"}).set_index(val, append=True).swaplevel(0,1).round(0).prep().T\n",
    "A = {k: pd.concat([p['rslt'][k] for p in P]) for k in ['full','summary']}\n",
    "Q = pd.concat([piv(\"pred_term == @self.infer & styp_code=='n'\", 'pred', q) for q in [25,50,75]], axis=1)\n",
    "M = piv(\"pred_term != @self.infer & styp_code=='n'\", 'err%', 50)\n",
    "Q\n",
    "M\n",
    "# q = Q[0]\n",
    "# q\n",
    "# Q[0]\n",
    "# piv(\"styp_code=='n'\", 'err%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q.T.assign(a=50).set_index('a', append=True).swaplevel(0,1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = Q[0]\n",
    "A.rename('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = (\n",
    "    A['summary']\n",
    "    .grpby(['crse','styp_code','train_term','pred_term'])\n",
    "    [['pred','err%','mse%','f1_inv%']]\n",
    "    .agg(summary)\n",
    "    .stack(0, sort=False)\n",
    "    .rename_axis(index={None:'kind'})\n",
    "    .query(f\"(pred_term == {self.infer} and kind == 'pred') or (pred_term != {self.infer} and kind == 'err%')\")\n",
    "    .reset_index()\n",
    "    # .sort_values(['crse','styp_code','pred_term','train_term'],ascending=[True,True,False,False])\n",
    "    .prep()\n",
    ")\n",
    "\n",
    "B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# M = A['summary'].query(\"pred_term!=202408 & styp_code=='n'\")['err%'].groupby(['train_term','pred_term']).mean().reset_index()#.unstack()\n",
    "# M.pivot_table(index='train_term',columns='pred_term', margins=True)\n",
    "\n",
    "# M\n",
    "A['summary'].reset_index().query(\"pred_term!=202408 & styp_code=='n'\").pivot_table(index='train_term', columns='pred_term', values='err%', margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M.disp(10)\n",
    "B.disp(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A['summary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = self.Z.vc('term_code')\n",
    "v = t.values\n",
    "pd.DataFrame((v / v.T - 1) * 100, index=t.index, columns=t.index).round().prep(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B.sort_values(['train_term','pred_term'], ascending=False).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M.disp(10)\n",
    "B.disp(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = A['summary'].query(\"pred_term!=202408 & styp_code=='n' & pred_term!=train_term\")['err%'].groupby(['train_term','pred_term']).mean().unstack()\n",
    "M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A['summary'].disp(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "where = lambda x: x.query(\"levl_code == 'ug' and styp_code in ('n','r','t')\").copy()\n",
    "\n",
    "with warnings.catch_warnings(action='ignore'):\n",
    "    self.Y = [pd.concat([term.reg[k] for term in self.term.values()]).assign(credit_hr=lambda x:x['credit_hr'].fillna(0)>0) for k in [0,1]]\n",
    "    # self.Z = trf.fit_transform(where(self.X).set_index(self.attr, drop=False))\n",
    "# agg = lambda y, g: y.groupby(g)[['credit_hr']].sum()\n",
    "# grp = ['styp_code','term_code','crse']\n",
    "# end = agg(where(self.Y[0]), grp)\n",
    "# self.Y = [self.Z[[]].join(y.set_index(['pidm','term_code'])[['crse','credit_hr']], how='inner') for y in self.Y]\n",
    "# cur = agg(self.Y[0], grp)\n",
    "# M = (end / cur).query(\"term_code != @self.infer\")\n",
    "# N = M.reset_index().assign(term_code=self.infer).set_index(M.index.names)\n",
    "# self.mlt = pd.concat([M, N], axis=0).replace(np.inf, pd.NA).squeeze().rename('mlt').prep()\n",
    "# return self.dump()\n",
    "\n",
    "agg = lambda y, g: where(y).groupby(g)[['credit_hr']].sum()\n",
    "grp = ['levl_code','styp_code','term_code','crse']\n",
    "end = agg(self.Y[0], grp)\n",
    "self.Y = [self.Z[[]].join(y.set_index(['pidm','term_code'])[['crse','credit_hr']], how='inner') for y in self.Y]\n",
    "cur = agg(self.Y[0], grp)\n",
    "M = (end / cur).query(\"term_code != @self.infer\")\n",
    "M\n",
    "# agg(self.Y[0], grp).disp(500)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
